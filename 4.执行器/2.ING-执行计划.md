# 1. 导语
如何快速准确地找到需要的数据，是每个数据库需要考虑的核心问题。   

参考《数据库系统概念》书中的描述，查询处理的基本步骤一般包括：语法分析与翻译，优化器，查询计划和执行引擎。    
因此，本文将对照上述流程分析 MongoDB 中请求执行模块的实现。

# 2. 词法解析
由于 MongoDB 采用 BSON 格式表达命令，因此词法解析非常容易。不像传统关系型数据库 SQL需要进行字符串匹配，还要考虑大小写、空格等问题。    

具体来说，一个 BSON 格式的查询命令会通过 [parseFromFindCommand](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/query/query_request.cpp#L133) 解析成标准的 [QueryRequest](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/query/query_request.h#L53)。在解析的过程中还会进行合法性判断，比如 filter 必须是 Object 类型，limit 必须是数字等。    
QueryRequest 中包含了 nss、filter、skip、limit、batchSize、project、sort、hint 等参数。    

# 3. 语法解析
语法解析的核心是将 filter 解析成 [MatchExpression](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/matcher/expression.h#L57) 组成的语法树。MatchExpression 有 AndMatchExpression等派生类表示 "$and" 等逻辑运算，也有 GTEMatchExpression 等派生类表示 "$gte" 等比较运算。通过 VSCode 可以看到类的继承关系如下：    

<p align="center">
  <img src="https://github.com/user-attachments/assets/fc7992b6-0890-4b13-9c7c-eea4543d0081" width=400>
</p>

以下面的查询命令为例：    
```
{ find: "coll1", filter: { $and: [ { a: { $gte: 1 } }, { a: { $lte: 100000 } } ] }, skip: 5, limit: 10, sort: { c: 1 }, projection: { b: 1, c: 1 }}
```

首先会通过词法解析生成 QueryRequest。然后会调用 [parse](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/matcher/expression_parser.cpp#L244) 方法将 filter 解析成语法树，生成 [CanonicalQuery](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/query/canonical_query.h#L46)：     

<p align="center">
  <img src="https://github.com/user-attachments/assets/ae18e594-f3f0-497f-ae48-f35ecdc15c15" width=500>
</p>

# 4. 执行计划

## 4.1 执行流程
### 4.1.1 预处理阶段
[prepareExecution](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/query/get_executor.cpp#L371) 核心逻辑：    
1. 调用 [fillOutPlanerParams](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/query/get_executor.cpp#L252) 填充参数，用于指导后续执行计划的生成。完成的工作包括遍历索引、使用索引过滤器（相关用法可以参考 [planCacheSetFilter](https://www.mongodb.com/zh-cn/docs/v5.0/reference/command/planCacheSetFilter/) 命令）、是否支持 noTableScan、对于分片集群则判断是否需要 ShardFilter 过滤孤儿文档、是否需要 INDEX_INTERSECTION 和 GENERATE_COVERED_IXSCANS、是否需要 SPLIT_LIMITED_SORT、是否需要 OPLOG_SCAN_WAIT_FOR_VISIBLE。
2. 调用 [IDHackStage::supportsQuery](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/exec/idhack.cpp#L166) 判断能否直接用 _id 索引进行 IDHACK。如果能，则快速生成执行计划并返回。
3. 调用 [shouldCacheQuery](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/query/plan_cache.cpp#L136) 检查能否直接从 planCache 中获取执行计划。如果能从 planCache 中获取，则生成执行计划并快速返回。
4. 调用 [QueryPlanner::plan](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/query/query_planner.cpp#L539) 生成执行计划。具体来说会根据查询条件进行一轮规则匹配，比如 tailable cursor 会选择全表扫描 + 过滤的方式。
5. 如果是 count 查询，调用 [turnIxscanIntoCount](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/query/get_executor.cpp#L1143) 判断能否将 indexScan 或者 fetch with indexScan 转换为 countScan 进行加速。
6. 如果只有一个 querySolution，则直接生成 planStage 并返回给后续的流程执行。如果有多个 querySolution，则生成 MultiPlanStage 给后续流程去选择。

### 4.1.2 生成最优执行计划
[PlanExecutorImpl::make](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/query/plan_executor_impl.cpp#L189) 会生成真正的执行计划，如果在预处理阶段生成的是 STAGE_MULTI_PLAN、STAGE_SUBPLAN、 STAGE_CACHED_PLAN、 STAGE_TRIAL，则需要选择最优的执行计划。    

下面主要介绍 MultiPlanStage::pickBestPlan 的选择流程。

>如何选择最优的执行计划，是数据库优化器需要考虑的问题: RBO、CBO 还是其他？    
>RBO(Rule-Based Optimizer) 优化器相对简单，根据预先设定的规则，选择最优的执行计划。这种优化器对于数据并不敏感，可能会选择一些代价较大的执行计划。这种策略一般应用于早期的数据库系统。    
>CBO(Cost-Based Optimizer) 优化器相对复杂，需要结合实际的运行数据，计算出每个执行计划的代价，然后选择代价最小的执行计划。数据库需要有专门的流程（analyze/vacuum）去更新数据的统计信息（直方图），否则优化器无法正确选择代价最小的执行计划。这种策略在当前的关系型数据库中广泛使用。    
>MongoDB 作为 NoSQL 数据库，并没有照搬 RBO 和 CBO 的优化器，个人认为的原因：
>1. MongoDB 作为 "schema-free" 的数据库，没有固定的列定义，而且每一列的数据类型也是不固定的，因此本身没有像 SQL 数据库那样的统计信息，因此无法照搬 CBO 优化器。
>2. RBO 的缺点非常明显，显然也不是一个很好的选择。
>
>因此，MongoDB 选择了另外一种策略，即根据不同执行计划实际去运行采样，选择代价最小的执行计划。另外结合自身的 planCache 缓存机制，可以减少不必要的代价计算。    
相比传统的 CBO 优化器，MongoDB 的执行计划选择可能需要更多的 IO 和运行时间（因为要实际去执行一次），但是减少了数据统计信息的维护开销。 

MongoDB 选择最优执行计划的核心思路是：**将备选计划都执行一遍并打分，然后选择分数最高的去执行。**     
显然，将备选计划完整的执行一遍需要消耗很多资源，特别是全表扫描以及大数据量排序的场景。因此，MongoDB 对备选计划的执行做了一些限制，包括：    
1. 每个备选计划最多的调用次数（numWorks, 可以简单理解为扫描的行数）。对于小表来说默认为 [1 万](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/query/query_knobs.idl#L40)，对于大表来说为表的记录总数的 [30%](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/query/query_knobs.idl#L49).
2. 有备选计划获取了足够的结果数。默认为 [101 条](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/query/query_knobs.idl#L59)，或者有备选计划 EOF。

---

在“采样”执行结束后，会对备选计划打分。具体参考 [PlanRanker::pickBestPlan](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/query/plan_ranker.cpp#L68) 以及 [PlanRanker::scoreTree](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/query/plan_ranker.cpp#L191) 的流程。简单来说，每个执行计划的得分来源是：    
1. **baseScore** 基础分，固定为 **1.0** 。主要的目的是防止 0 分出现，因为 0 分对应的是 "no plan selected"。
2. **productivity** 生产力评分，取值为 [0,1]。计算公式如下：
    ```
    static_cast<double>(stats->common.advanced) / static_cast<double>(workUnits)

    // workUnits 是调用执行计划的次数
    // advanced 是每次调用产生有效结果的次数

    // 这个公式能够表示执行计划是否高效，如果接近 1 ，说明几乎每次执行都能产生有效结果
    ``` 
3. **noFetchBonus** 奖励分，取值为 std::min(1.0 / static_cast<double>(10 * workUnits), 1e-4)。如果有“covered projections” 则加上这个奖励分，因为不需要额外的 fetch 阶段来进行 projection，效果会更优。
4. **noSortBonus** 奖励分，取值为 std::min(1.0 / static_cast<double>(10 * workUnits), 1e-4)。如果不需要单独的"blocking sort stage" 就能完成排序则加上这个奖励分。一个比较典型的场景是直接利用索引完成排序，显然效果更优。
5. **noIxisectBonus** 奖励分，取值为 std::min(1.0 / static_cast<double>(10 * workUnits), 1e-4)。MongoDB 优化器认为同等情况下 "single index" 优于 "index intersection"。
6. **eofBonus** 奖励分，取值为 1.0 。如果有执行计划达到了 EOF， 则加上这个奖励分。
7. 如果设置了 [internalQueryForceIntersectionPlans](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/query/query_knobs.idl#L68) 参数（默认 false），则对包含 index intersection 的备选计划加 3.0 分。

### 4.1.3 执行阶段

**火山模型**    

MongoDB 的查询执行计划是按照“火山模型”来设计的，具有以下特点：
1. 每个操作抽象为独立的 Stage，最终构成一个树形结构。
2. 自顶向下调用。从根节点开始调用其子节点的 work() 接口, 逐级向下调用，并向上返回结果。是一个 pull-based 模型。
3. 在 MongoDB 中这些任务都是由单线程来执行的。

以一个常见的查询：`find({$or:[{a:100},{b:200}]})` 为例，假设已经分别对字段 a 和 b 建立索引，那么最终生成的执行计划如下图所示：

<p align="center">
  <img src="https://github.com/user-attachments/assets/eff80a94-7d5d-432b-a850-84e350ac3b75" width=300>
</p>

这种模型的优势有：
1. 灵活性；
2. 扩展性；
3. 逻辑清晰，易于理解和维护。

但是其缺点也非常明显：
1. 虚函数调用开销大；
2. 执行效率低，特别是全表扫描以及大数据量排序的场景。

和很多 AP 数据库具有的多线程调度、pipleline 机制、向量化执行等特性相比，MongoDB 的执行引擎并不具备优势。

**Yield机制**

Yield 机制可用于释放锁、快照等资源。不同的请求类型可以设置不同的 yield 策略。常见的 yield 策略可以按照特性和使用场景进行如下分类：

|[Yield 策略](https://github.com/mongodb/mongo/blob/r4.2.5/src/mongo/db/query/plan_executor.h#L103)|[释放锁](https://github.com/mongodb/mongo/blob/r4.2.5/src/mongo/db/query/plan_yield_policy.h#L95)|[释放快照](https://github.com/mongodb/mongo/blob/r4.2.5/src/mongo/db/query/plan_yield_policy.cpp#L84)|[检查是否主动终止*](https://github.com/mongodb/mongo/blob/r4.2.5/src/mongo/db/query/plan_yield_policy.cpp#L84)|[遵循 auto yield 策略](https://github.com/mongodb/mongo/blob/r4.2.5/src/mongo/db/query/plan_yield_policy.h#L117)|典型使用场景|
|:--|:--|:--|:--|:--|:--|
|YIELD_AUTO|**Y**|**Y**|**Y**|**Y**|普通的 find|
|WRITE_CONFLICT_RETRY_ONLY|N|**Y**|**Y**|**Y**|foreground indexBuilding|
|YIELD_MANUAL|**Y**|**Y**|**Y**|N| range deletion|
|NO_YIELD|N|N|N|N|findOne、dbHash、dbSize、listCollection|
|INTERRUPT_ONLY|N|N|**Y**|N|multiDocument-Transaction|

>_说明：checkForInterruptNoAssert 函数会检查是否需要主动终止。除了 yieldOrInterrupt，在很多流程中都会调用（比如加锁的时候）。_

PlanExecutorImpl 每轮迭代会[调用 shouldYieldOrInterrupt](https://github.com/mongodb/mongo/blob/r4.2.5/src/mongo/db/query/plan_executor_impl.cpp#L516) 检查是否需要 yield，如果没有设置强制 yield 标志（比如出现 WriteConflictException 时强制 yield），则会采用自定义的 yield “合并”策略。该策略会采取一些合并机制避免过于频繁的 yield 导致性能受损，具体来说有 2 个参数控制：
1. 距离上一次 yield，到现在调用 shouldYield 的次数internalQueryExecYieldIterations([默认 128](https://github.com/mongodb/mongo/blob/r4.2.5/src/mongo/db/query/query_knobs.idl#L215)).
2. 距离上一次 yield，到现在的时间 internalQueryExecYieldPeriodMS（[默认10ms](https://github.com/mongodb/mongo/blob/r4.2.5/src/mongo/db/query/query_knobs.idl#L222)）.    

满足上述条件之一，才会 yield。

有了 Yield 机制后，一条耗时很长的扫描操作会定期释放锁，因此不会一直阻塞其他流程的删表、建索引、甚至停服务等操作。另外 yield 机制定期释放快照的操作，也会减轻 WT 引擎维护 MVCC 的压力。

Yield 机制带来的警示是：**普通的扫描操作可能读取的是涉及多个快照的非一致性数据**。在一些需要一致性保证的场景中，需要特别注意。    
比如，在使用 mongodump 进行备份时，使用的就是普通的全表扫描操作。如果要执行一致性备份，需要[配合 oplog 的备份](https://www.mongodb.com/docs/database-tools/mongodump/#std-option-mongodump.--oplog) 来完成。


## 4.2 核心数据结构
### 4.2.1 QuerySolution
QuerySolution 定义了“抽象化”的查询解决方案，内部由不同类型的 QuerySolutionNode 连接成“树型结构”。   

每个 QuerySolutionNode 定义了对应的操作类型。比如 IndexScanNode 表示了索引扫描操作，里面会包含索引信息，但是不包含具体执行的 startKey 和 endKey。

QuerySolution 会使用在 PlanCache 中。

### 4.2.2 PlanCache

**存储结构**

**生命周期管理**

**如何感知外部状态变化**

**用户接口和干预手段**

### 4.2.3 PlanStage
[PlanStage](https://github.com/mongodb/mongo/blob/r4.2.5/src/mongo/db/exec/plan_stage.h#L106) 是执行计划的基本组成单元。Stage 可以执行数据访问，比如从表和索引读数据，也可以执行数据转换生成数据流，比如 AND,OR,SORT 等操作。

每个 Stage 有 0 个或者多个输入流，但是只有一个输出流。数据访问的 stage 是整个执行计划的“叶子节点”，数据转换的 stage 是“非叶子节点”。多种类型的 stage 按照规则连接在一起，组成“执行树”来完成查询任务。

外部通过调用 work() 接口来驱动 stage 执行任务，根节点会调用其子节点的 work() 接口来逐级驱动。每个 stage 的 work() 流程会执行特定的任务并返回结果。有些阻塞式的 stage，比如 AND、SORT 等，可能需要多次被调用 work() 累积到足够的输入数据之后才能向更上层节点输出结果。在不能立即输出结果时，会向上层返回 [NEED_TIME](https://github.com/mongodb/mongo/blob/r4.2.5/src/mongo/db/exec/plan_stage.h#L140)，告诉上层节点需要继续调用 work().

### 4.2.4 WorkingSet
每个查询的执行器会分配 1 个 WorkingSet 内存结构用于存储中间结果，并在自己的多个 PlanStage 中共享。

以一个常见的查询： db.coll.find({a:{$gt:100, $lt:200}}, {_id:0 , b:1, c:1}).skip(5).limit(5) 为例，会生成下图左侧的执行计划，并分配下图右侧的 WorkingSet 结构。

<p align="center">
  <img src="https://github.com/user-attachments/assets/6be9a763-81ed-46d0-9f42-3624dee5d736" width=800>
</p>

按照自顶向下的视角，[WorkingSet](https://github.com/mongodb/mongo/blob/r4.2.5/src/mongo/db/exec/working_set.h#L52) 中包含：    
1. _data: 存储中间结果数据，本质上是一个 vector, vector 中的每个槽位是一个 MemberHolder.
2. _freeList: 类型为 WorkingSetID(本质上是一个 size_t)，存储了 _data 中空闲的 MemberHolder。可以看做这是一个空闲链表的头部。
3. _yieldSensitiveIds：直译过来就是“对 yield 操作敏感的 WorkingSetID 数组”。具体来说，对应的是 indexScan 获取的中间结果。这些中间结果，可能会由于 yield 操作，导致的 snapshot 更新后，fetch 出来的文档已经不能匹配 indexScan 的查询条件。

[MemberHolder](https://github.com/mongodb/mongo/blob/r4.2.5/src/mongo/db/exec/working_set.h#L118) 中包含：    
1. nextFreeOrSelf: 类型为 WorkingSetID。如果WorkingSetMember中存储的是有效值，则 nextFreeOrSelf 存储自己在 WorkingSet::_data数组中的偏移。否则，nextFreeOrSelf 存储下一个空闲 Holder 的偏移，如果自己就是最后一个空闲 Holder，则设置为 -1.
2. [WorkingSetMember](https://github.com/mongodb/mongo/blob/r4.2.5/src/mongo/db/exec/working_set.h#L237): 存储具体的中间结果。包含的内容：    
	2.1 recordId: 数据在表中的 RecordId.    
	2.2 obj: 带 snapshot 版本信息的 BSONObj。    
	2.3 keyData: 类型为 vector<IndexKeyDatum>， 每个 IndexKeyDatum 包含了索引信息，对应的索引 Key 信息。    
	2.4 _state: 当前的状态，包含 INVALID、RID_AND_IDX（从索引获取的数据）、RID_AND_OBJ（从表获取的数据）、OWNED_OBJ（运算后的 BSONObj）    
	2.5 _computed: 存储运算后的中间结果，比如 sortKey、geo point、text score 等。    
	2.6 _isSuspicious: 直译为“可疑的”，和前面的 _yieldSensitiveIds 对应。一般用于 indexScan 的中间结果，在 yield 之前，会先将这个标志置位 true. 在 yield 重新调度回来时，会检查这个标记，将表中 fetch 的结果和索引 key 进行比对。    

对于 WorkingSet::_yieldSensitiveIds 和 WorkingSetMember::_isSuspicious，下面说明为啥这样设计。    
假设系统中有 2 个请求，依次执行如下流程：        
1. 请求 A 根据条件 {a:1} 去查询文档，在 indexScan 阶段得到中间结果 {a:1} -> recordId:1, 并将这个中间结果存储到 WorkingSet 中。然后请求 A 就 yield 出去了，并释放了 snapshot.    
2. 请求 B 是一个 update 操作，将 recordId:1 这条记录从 {a:1} 改成了 {a:2}.    
3. 请求 A yield 回来，获取了新的 snapshot，然后根据 recordId 去查表，读到 {a:2} 这条文档。    

可以看到，如果使用默认的“非一致性快照”读，可能会由于 yield 操作，导致 indexScan 获取的结果并不符合预期。    

因此，需要将 indexScan（idHack, distinct 等其他使用索引的 stage 类似） 得到的中间结果[记录在 WorkingSet::_yieldSensitiveIds](https://github.com/mongodb/mongo/blob/r4.2.5/src/mongo/db/exec/working_set.cpp#L96) 中，在执行器被 yield 出去之前，会遍历 WorkingSet::_yieldSensitiveIds 并将其中的 WorkingSetMember::_isSuspicious [设置为 true](https://github.com/mongodb/mongo/blob/r4.2.5/src/mongo/db/exec/working_set_common.cpp#L43).
在 yield 调度回来之后，在从表里面 fetch 数据后，如果设置了 isSuspicious 标记，会[再次检查](https://github.com/mongodb/mongo/blob/r4.2.5/src/mongo/db/exec/working_set_common.cpp#L76)读出来的文档是否和索引条件匹配。


# 5. 分片集群的执行计划

接下来讨论分片集群中，mongos 侧是如何执行的。

## 5.1 写请求

MongoDB 提供了功能强大的批量 insert/update/delete 接口，并提供了非常丰富的响应信息用于错误处理。    
下面结合 4.2.25 版本的内核代码，介绍mongos 是如何调度写请求的。

在介绍具体流程之前，先了解一下关键的数据结构：
1. [BatchedCommandRequest](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/s/write_ops/batched_command_request.h#L45): 存储了原始的写请求，并归一化成内核代码可以直接访问的数据结构。
2. [BatchWriteOp](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/s/write_ops/batch_write_op.h#L122): 按原始写请求的数组拆分成多个子请求（item），后续再根据每个子请求要发往的 shard 数量进行进一步拆分。经过这 2 次拆分后的子请求，能够明确表示要发往哪个 shard，去执行哪个命令，可以直接用于后续调度。MongoDB 使用了“数组下标引用”的设计，避免原始请求的内存拷贝（对于一个很大的插入请求，每次对原始请求的内存拷贝都是消耗很大的操作）。比如一个批量更新请求中，第 3 个请求要发往 shard1 和 shard2, 则发往 shard1 的子请求会记录为 {3,0}+endpoint(包含shardId和 shardversion 信息)，发往 shard2 的子请求会记录为 {3,1}+endpoint，其中第 1 个下标表示了在原始请求数组中的下标，而第 2 个下标则是区分不同的 shard .
3. [childBatches](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/s/write_ops/batch_write_exec.cpp#L147): 一个 std::map 临时结构，表示每一轮要转发的子请求。包含了要进行转发的 shardId，以及子请求的命令，shardVersion 等信息。


BatchWrite 的整体流程为（参考 [BatchWriteExec::executeBatch](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/s/write_ops/batch_write_exec.cpp#L102)）：
1. 根据原始请求 BatchedCommandRequest，生成对应的 BatchWriteOp 管理请求状态的生命周期。生成 BatchWriteOp 的过程中避免了内存拷贝，而是通过“数组下标引用”这种巧妙的方式，来建立和原始请求中每个子请求的对应关系。
2. 通过 BatchWriteOp::targetBatch 生成可以并行转发给 shard 的子请求 （childBatches）.
3. 通过网络模块给各个 shard 转发子请求，并在子请求中携带了 mongos 维护的 shardVersion 信息，用于路由版本校验。
4. 收集每个 shard 的响应，并进行必要的错误处理。比如在出现 StaleConfig路由版本较低的错误时，需要进行必要的路由刷新。
5. 跳转到第 2 步，重试或者生成下一轮要执行的 childBatches，然后继续发送流程。直到 BatchWriteOp 中的请求全部执行完毕，或者执行遇到不可重试的错误，或者连续 [5轮](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/s/write_ops/batch_write_exec.cpp#L98)没有实质性进展，则返回执行结果或者报错。

mongos 侧执行写请求的难点，主要集中在 `ordered` 顺序，以及子请求的路由处理。下面分开进行讨论。

**Ordered**    

`ordered` 参数指定了子请求是否严格按照顺序执行，默认为 true.    

假设现在有一个分片表 coll1, 分片建为 sk. 表中有几条文档，它们的 shardKey 和所属分片的对应关系为：
```
sk:3  --> shard1
sk:2  --> shard2
sk:1  --> shard2
```

假设现在有如下 `{ordered: true}` 的 Update 命令：
```
db.runCommand({update: "coll1",
	updates:[
		{q:{sk: 3}, u: {$set: {a:1}}, multi: true},
		{q:{sk: 2}, u: {$set: {a:2}}, multi: true},
		{q:{sk: 1}, u: {$set: {a:1}}, multi: true},
		{q:{}, u: {$set: {d:0}}, multi:true},
		{q:{sk:3}, u: {$set:{b:1}}, multi: true}
	],
	ordered: true})
```

其对应的转发流程会拆分为 4 轮，如下图所示：

<TODO  示意图>

如果将请求设置为  `{ordered: false}`, 即：
```
db.runCommand({update: "coll1",
	updates:[
		{q:{sk: 3}, u: {$set: {a:1}}, multi: true},
		{q:{sk: 2}, u: {$set: {a:2}}, multi: true},
		{q:{sk: 1}, u: {$set: {a:1}}, multi: true},
		{q:{}, u: {$set: {d:0}}, multi:true},
		{q:{sk:3}, u: {$set:{b:1}}, multi: true}
	],
	ordered: false})
```

则对应的转发流程会拆分为 3 轮，如下所示：

<TODO 示意图 2>

这里可能会好奇，为啥第 3 轮发往 shard1 的请求，没有和第 1 轮发往 shard1 的请求合并？    
在 mongos 中， 如果某个子请求是 `multi-shard`请求，且不处于分布式事务中，且与之前的子请求发往的 shard 有重叠，则会单独作为一个childBatch 执行（这种情况会降低整体调度的并发度）。
这么做的原因，是方便子请求的重试处理，正如[代码注释](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/s/write_ops/write_op.cpp#L98)中的描述：     
>Outside of a transaction, multiple endpoints currently imply no versioning, since we can't retry half a regular multi-write.
 
具体的做法，是将子请求对应的 [shardVersion 置为0](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/s/write_ops/write_op.cpp#L98-L103) ， 这样在 [isNewBatchRequiredUnordered](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/s/write_ops/batch_write_op.cpp#L116) 中进行判断时，发现与前面存在 shard 重叠的子请求的 shardVersion 不同，则不将这两者放在同一批次中调度。    


如果没有  `multi-shard`子请求，则可以非常方便的进行合并。比如下面这个请求：
```
db.runCommand({update: "coll1",
	updates:[
		{q:{sk: 3}, u: {$set: {a:1}}, multi: true},
		{q:{sk: 2}, u: {$set: {a:2}}, multi: true},
		{q:{sk: 1}, u: {$set: {a:1}}, multi: true},
		{q:{sk:3}, u: {$set:{b:1}}, multi: true}
	],
	ordered: false})
```
会进行合并后在 1 轮内搞定，如下所示：

<TODO 示意图3>

**子请求路由**    

以最复杂的 update 请求为例，其路由策略可以参考 [ChunkManagerTargeter::targetUpdate](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/s/write_ops/chunk_manager_targeter.cpp#L421)（同理， insert和 delete 请求的路由过程可以参考[targetInsert](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/s/write_ops/chunk_manager_targeter.cpp#L378) 和 [targetDelete](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/s/write_ops/chunk_manager_targeter.cpp#L521)），核心流程为：    
1. 根据 query 条件确定目标 shard 列表。和读请求相似，会根据 shardKey 的查询条件，根据 shardKey 索引去生成 indexBounds（mongos 上没有真正维护索引，只是模拟了一下 indexScan 生成执行计划的流程）, 然后根据路由表获取目标 shard 列表。
2. 除了上述常规的路由流程之外，对于 Update 还有一些快速路径：    
  a. 对于 {upsert: true} 类型的请求，请求中必然包含了 shardKey，因此可以提取 shardKey 之后直接去查路由表。    
  b. 对于 replacement 结构的请求，其 u 字段中包含了全量文档信息，因此必然也包含了 shardKey，因此可以用于对前面生成的目标 shard 列表进行裁剪（如果之前生成的目标 shard 数大于 1 个）。



## 5.2 读请求

# 6. 索引的使用


# 7. 总结


# 参考文档
1. 数据库内核杂谈（七）：数据库优化器（上）：https://www.infoq.cn/article/GhhQlV10HWLFQjTTxRtA
2. 数据库内核杂谈（八）：数据库优化器（下）：https://www.infoq.cn/article/JCJyMrGDQHl8osMFQ7ZR
3. 数据库内核杂谈（九）：开源优化器 ORCA：https://www.infoq.cn/article/5o16eHOZ5zk6FzPSJpT2
4. SQL 查询优化原理与 Volcano Optimizer 介绍：https://zhuanlan.zhihu.com/p/48735419
5. Cascades Optimizer：https://zhuanlan.zhihu.com/p/73545345
