# 1. 导语
事务是数据库系统中的核心功能，它确保数据可以一致地从一个状态到另一个状态。
MongoDB 从 4.0 内核版本提供了 ACID 事务，支持跨文档和集合的事务。从 4.2 版本开始，MongoDB 提供了跨分片的 2PC 事务。

本文依据 4.2 版本内核代码，自顶向下介绍 MongoDB 中分布式事务、副本集事务、单机事务的实现。

>有些内容在前面章节也有涉及，比如事务的 oplog 复制，混合时钟等。但是为了不影响本文的阅读性，会适当进行重复介绍。

# 2. 分布式事务
## 2.1 基本概念
MongoDB 采用基于混合逻辑时钟的 2PC 协议实现分布式事务。
### 混合逻辑时钟
MongoDB 通过[混合逻辑时钟](https://dl.acm.org/doi/pdf/10.1145/3299869.3314049)(Hybrid Logical Clock, HLC) 来维护每个 client 内部多个请求的顺序。    
**HLC 由 <int32: unix second> 和 <int32: counter> 共同组成**，也叫 clusterTime 在集群中传播。Primary 节点在执行写请求时推进 HLC，并将其写入到 oplog，也会跟随数据写入到存储引擎，作为 MVCC 版本控制的依据。    
Primary 节点作为 HLC 的推动者，集群中的其他节点（client, mongos, config server, shard secondary）作为 HLC 的传播者，通过 gossip 机制实现全局同步。     
下图说明 Client 如何通过 clusterTime 在 secondary 节点上完成”read own write“:

<p align="center">
  <img src="https://github.com/user-attachments/assets/fab405a1-0831-4d16-b8e9-0482b20d2d28" width=500>
</p>

1. Client 给 primary 节点发送写请求。
2. Primary 执行请求，并推进 opTime 到 T2，并记录 oplog 进行异步复制。
3. Primary 将 opTime T2 作为返回结果的元数据传递到 Client.
4. Client 推进 opTime 到 T2.
5. Client 给 secondary 节点发起读请求，并携带 {afterClusterTime: T2}，表示要读取 T2 及之后版本的数据。
6. Secondary 判断自己的同步进度有没有到 T2, 如果没有则等待。
7. 如果满足条件，则执行读请求，并返回文档。

HLC 主要还是解决如何定序，管理数据版本的问题。在此之前，业界已经存在一些经典的解决方案，比较知名的有 Lamport 的逻辑时钟，Google Spanner 的时钟服务等。MongoDB 最终还是采用了 HLC 方案，可以从以下几个问题进行分析。    
1.  **为什么不采用逻辑时钟，而加入了物理时间（wall clock）?**    
真实的业务场景往往是伴随物理时间的，比如按时间点读取数据，按时间点回档数据库等，能精确到 xx年xx月xx 日xx时xx分xx秒。如果只用逻辑时间，很多业务场景都无法满足。
2.  **为什么不使用 Spanner 的 TrueTime 时钟服务？**    
MongoDB 官方认为会增加读写请求的延迟（每次执行请求之前都要调用 TrueTime 的 API），而且增加部署难度（额外的服务和硬件）。
3.  **为什么不直接用更精细的纳秒时钟，而是采用 秒级时钟 + 秒内counter 的方式？**    
引入物理时钟之后，面临的问题是如何保证时钟一直会持续递增？一般部署 MongoDB 的机器都会采用 NTP 服务进行时钟对齐，那物理时间就有回拨的风险。引入 counter 能够保证即使机器时钟回拨，也不倒退 HLC的物理时钟，而是推进 counter 来保证递增。换句话说，机器时间只是 HLC 的参考，但不是绝对权威，具体可以参考 [LogicalClock::reserveTicks](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/logical_clock.cpp#L98) 代码的实现。但是一般来说，NTP 能够保证机器时钟不会有巨幅波动，所以可认为 HLC 的物理时间 == 机器时间。
4. **HLC如何保证安全性？**       
如果由于黑客攻击，将集群中的 HLC 推进到了 Unix 时间的最大值，则集群将无法继续处理写请求。因此，MongoDB 对集群中传递的 HLC 增加了 Hash 签名校验，密钥只有集群内部知道，而且定期更新。另外代码中也对 HLC的推进作了限制，默认每次推进的时间不能超过 1年。
Hash 校验保证了安全性，但是牺牲了计算资源。MongoDB 为了提升性能进行了2点针对性地优化：a. 区分[特权认证](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/logical_time_validator.cpp#L200)的连接，这些被信任连接的请求可以免去 Hash 校验；b. 充分利用 HLC [只能向前推进](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/logical_time_validator.cpp#L161)的特性，通过[缓存](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/time_proof_service.h#L44) 避免重复的 Hash 计算。
5.  **HLC 如何传播？**  
采用 gossip 机制，所有读写请求涉及的节点（Client 和 MongoDB中的各个节点）都会参与传播。但是只有 primary 节点有权限推进 HLC。

### 2PC 协议
在 MongoDB 的 2PC 协议中主要包含以下 3 种角色：
1. Router: 由 mongos 节点承担，负责将事务中的操作路由到正确的 Shard 节点，将 commit/abort 操作路由到 coordinator。并承担一定的事务决策，比如统计参与事务的 shard 列表，是否为只读事务，是否需要进行 2PC 提交等。      
2. Coordinator：由第 1 个参与事务的 shard 承担，是 2PC 的协调者，负责协调各个参与事务的 shard 节点执行事务。    
3. Participant：参与事务的 shard 节点，负责执行事务中的操作，并执行 Coordinator 发过来的 prepare/commit/abort 请求。    

### 全局事务 ID
不使用传统 GTID 会面临至少以下  2 个问题：    
1. 如何在多个节点之间串联这个事务？    
多个 shard 节点之间通过 lsid(id+uid+txnNumber) 来串联全局的分布式事务。
2. 如何给多个事务定序？    
在现有架构下，无法保证线性一致。比如：2 个不相关的事务 t1 和 t2，也分别操作了不同的 shard，即使 t1 在真实世界中先提交，也无法保证 t1->commit_timestamp < t2->commit_timestamp.

但是，基于现有逻辑时钟的方式，能够保证因果一致性。比如同一个客户端先后发起的事务 t1 和 t2，能保证 t1->commit_timestamp < t2->commit_timestamp.

## 2.2 分布式事务执行流程
### 2 阶段提交
如果涉及到多个 shard 更新，会走 2 阶段提交方式。
以下图为例，客户端插入 2 条数据，涉及到 2 个 shard：

<p align="center">
  <img src="https://github.com/user-attachments/assets/8978f05d-59f5-4b7c-926d-f74fc1357071" width=600>
</p>

mongos 侧的行为：    
1. client -> mongos 发送第1条 insert 命令，携带 lsid, startTransaction:true, txnNumber:1, autocommit: false
2. client -> mongos 发送第2条 insert 命令，携带的参数少了 startTransaction: true, 其他类似
3. client -> mongos 发送 commitTransaction 命令，携带参数有 lsid,txnNumber,autocommit(false)和 recoveryToken: { recoveryShardId: "shard2" }

shard1 侧的行为：    
1. mongos->shard1 发送 insert {a:3} 命令， 携带 lsid,uid, txnNumber, startTransaction: true, autocommit: false.
2. coordinator(shard2) -> shard1 发送 prepareTransaction 命令，携带 lsid,uid, txnNumber, autocommit: false. 
3. coordinator(shard2) -> shard1 发送 commitTransaction 命令，携带 lsid,uid,txnNumber,autocommit 以及  commitTimestamp: Timestamp(1720856237, 4) 信息。

shard2（coordinator shard） 侧的行为:    
1. mongos -> shard2 发送 insert 命令，携带参数 lsid, uid, txnNumber, startTransaction: true, coordinator: true, autocommit: false.  
2. mongos -> shard2 发送 coordinateCommitTransaction 命令，携带的参数除了 lsid, uid, txnNumber, coordinator:true 之外，还有重要的 participants: [ { shardId: "shard2" }, { shardId: "shard1" } ].    
    - 2.1 shard2 更新本地的 config.transaction_coordinators 表，设置 participants: [ "shard1", "shard2" ]。
    - 2.2 shard2 执行 prepareTransaction 命令， prepare 时间为（Timestamp(1720856237, 4)）
    - 2.3 shard2 更新 config.transaction_coordinators 表，设置 participants: [ "shard1", "shard2" ], decision: { decision: "commit", commitTimestamp: Timestamp(1720856237, 4) } }
    - 2.4 shard2 上执行 commitTransaction，指定 commitTimestamp: Timestamp(1720856237, 4)，并在几毫秒后结束。
    - 2.5 shard2 上删除 config.transaction_coordinators 中的内容，过滤条件是 lsid+uid+txnNumber.

### 1 阶段提交
如果只涉及到 1 个 shard 的更新，则mongos 直接透传 commitTransaction 给对应的 shard，走 1 阶段提交流程。    

<p align="center">
  <img src="https://github.com/user-attachments/assets/4c383cae-05f4-4ead-b986-4adc27374084" width=600>
</p>

mongos 侧行为：    
1. client->mongos 发送一条 insert 命令，并指定 lsid,txnNumber, autocommit:false, 以及 startTransaction: true
2. client->mongos 发送 commitTransaction 命令，

shard1 侧的行为：    
1. mongos->shard2 发送第 1 条 insert 命令，并指定 startTransaction: true, coordinator: true.
2. mongos->shard2 发送 commitTransaction 命令。


## 2.3 多副本的一致性保证
每个 shard 是多副本架构，所有的读写和事务操作都在 primary 节点上进行，通过 oplog 同步到其他节点。    

在 4.0 版本，由于没有 2 阶段事务，事务涉及的所有操作，都是在事务提交之后打包成 1 条 oplog， 然后同步到其他节点。这种方式处理起来简单，但是会受限 BSON 的 16MB 大小限制。    

### 2 阶段事务的 oplog 同步
在 4.2 版本引入 2 阶段事务之后，事务的 oplog 同步机制有以下变化：    
1. 事务在 prepare 和 commit 阶段都会生成 oplog，分别进行同步。其中  prepare 的 oplog 会包含具体的操作和数据， commit 的 oplog 只会包含最后的提交决定；    
2. prepare 的oplog 不再局限于 1 条，可以是多条（每条 oplog会存储前一条的指针，每条 oplog 会尽量打包到 16MB，以此降低 oplog 链表的长度），即突破了 16MB 的限制；    

举个例子，集群中有 2 个 shard，客户端开启一个事务后，往 shard2（coordinator shard） 写入了 3 条数据，往 shard1 写入了 1 条数据，则对应的 oplog 信息如下。     
shard1 有 2 条 oplog，分别是 prepare 和 commit (日志展示为从新到旧)：
```
{ "ts" : Timestamp(1720856237, 7), "t" : NumberLong(1), "h" : NumberLong(0), "v" : 2, "op" : "d", "ns" : "config.transaction_coordinators", "ui" : UUID("1ccf732c-1733-4c37-80cc-a8b251fad60e"), "wall" : ISODate("2024-07-13T07:37:17.978Z"), "o" : { "_id" : { "lsid" : { "id" : UUID("a3b2abaa-5223-4473-a358-dbf914067f40"), "uid" : BinData(0,"p7MxbwRFANsxxxnZIap+ho8v8Gvc5VLdCcD6t77QNwQ=") }, "txnNumber" : NumberLong(1) } } }
{ "ts" : Timestamp(1720856237, 6), "t" : NumberLong(1), "h" : NumberLong(0), "v" : 2, "op" : "c", "ns" : "admin.$cmd", "wall" : ISODate("2024-07-13T07:37:17.970Z"), "lsid" : { "id" : UUID("a3b2abaa-5223-4473-a358-dbf914067f40"), "uid" : BinData(0,"p7MxbwRFANsxxxnZIap+ho8v8Gvc5VLdCcD6t77QNwQ=") }, "txnNumber" : NumberLong(1), "prevOpTime" : { "ts" : Timestamp(1720856237, 4), "t" : NumberLong(1) }, "o" : { "commitTransaction" : 1, "commitTimestamp" : Timestamp(1720856237, 4) } }
{ "ts" : Timestamp(1720856237, 5), "t" : NumberLong(1), "h" : NumberLong(0), "v" : 2, "op" : "u", "ns" : "config.transaction_coordinators", "ui" : UUID("1ccf732c-1733-4c37-80cc-a8b251fad60e"), "o2" : { "_id" : { "lsid" : { "id" : UUID("a3b2abaa-5223-4473-a358-dbf914067f40"), "uid" : BinData(0,"p7MxbwRFANsxxxnZIap+ho8v8Gvc5VLdCcD6t77QNwQ=") }, "txnNumber" : NumberLong(1) } }, "wall" : ISODate("2024-07-13T07:37:17.958Z"), "o" : { "_id" : { "lsid" : { "id" : UUID("a3b2abaa-5223-4473-a358-dbf914067f40"), "uid" : BinData(0,"p7MxbwRFANsxxxnZIap+ho8v8Gvc5VLdCcD6t77QNwQ=") }, "txnNumber" : NumberLong(1) }, "participants" : [ "shard1", "shard2" ], "decision" : { "decision" : "commit", "commitTimestamp" : Timestamp(1720856237, 4) } } }
{ "ts" : Timestamp(1720856237, 4), "t" : NumberLong(1), "h" : NumberLong(0), "v" : 2, "op" : "c", "ns" : "admin.$cmd", "wall" : ISODate("2024-07-13T07:37:17.945Z"), "lsid" : { "id" : UUID("a3b2abaa-5223-4473-a358-dbf914067f40"), "uid" : BinData(0,"p7MxbwRFANsxxxnZIap+ho8v8Gvc5VLdCcD6t77QNwQ=") }, "txnNumber" : NumberLong(1), "prevOpTime" : { "ts" : Timestamp(0, 0), "t" : NumberLong(-1) }, "o" : { "applyOps" : [ { "op" : "i", "ns" : "db1.coll1", "ui" : UUID("216d9637-01bd-4db5-b44f-779438ef0294"), "o" : { "_id" : ObjectId("66922ead7b91f46778b88812"), "a" : 1 } }, { "op" : "i", "ns" : "db1.coll1", "ui" : UUID("216d9637-01bd-4db5-b44f-779438ef0294"), "o" : { "_id" : ObjectId("66922ead7b91f46778b88813"), "a" : 2 } }, { "op" : "i", "ns" : "db1.coll1", "ui" : UUID("216d9637-01bd-4db5-b44f-779438ef0294"), "o" : { "_id" : ObjectId("66922ead7b91f46778b88815"), "a" : 4 } } ], "prepare" : true } }
{ "ts" : Timestamp(1720856237, 1), "t" : NumberLong(1), "h" : NumberLong(0), "v" : 2, "op" : "i", "ns" : "config.transaction_coordinators", "ui" : UUID("1ccf732c-1733-4c37-80cc-a8b251fad60e"), "wall" : ISODate("2024-07-13T07:37:17.935Z"), "o" : { "_id" : { "lsid" : { "id" : UUID("a3b2abaa-5223-4473-a358-dbf914067f40"), "uid" : BinData(0,"p7MxbwRFANsxxxnZIap+ho8v8Gvc5VLdCcD6t77QNwQ=") }, "txnNumber" : NumberLong(1) }, "participants" : [ "shard1", "shard2" ] } 
```

### 1 阶段事务的 oplog 同步
如果分布式事务走的是 1 阶段提交，或者客户端使用的是 4.2 版本的副本集事务，则只有 commit 的 oplog，并且也可以是多条。    

举个例子，客户端只往 shard2 上写入了1 条数据，然后提交。此时不涉及 prepare 流程。    
shard2 涉及的 oplog 日志，只有 1 条：    
```
{ "ts" : Timestamp(1720938340, 1), "t" : NumberLong(1), "h" : NumberLong(0), "v" : 2, "op" : "c", "ns" : "admin.$cmd", "wall" : ISODate("2024-07-14T06:25:40.858Z"), "lsid" : { "id" : UUID("295440e5-c972-4c9e-819c-97655f8d9885"), "uid" : BinData(0,"p7MxbwRFANsxxxnZIap+ho8v8Gvc5VLdCcD6t77QNwQ=") }, "txnNumber" : NumberLong(1), "prevOpTime" : { "ts" : Timestamp(0, 0), "t" : NumberLong(-1) }, "o" : { "applyOps" : [ { "op" : "i", "ns" : "db1.coll1", "ui" : UUID("216d9637-01bd-4db5-b44f-779438ef0294"), "o" : { "_id" : ObjectId("66936f64200c042e039d8211"), "a" : 1 } } ] } }
```

### 事务中的 oplog 回放流程
对于 4.0 内核版本的副本集事务，是比较好处理的。事务在提交时，会将所有操作通过一个数组打包好，并放在一条 oplog 中。这种方式虽然对事务大小有 [16MB 限制](https://www.mongodb.com/docs/manual/core/transactions-production-consideration/#oplog-size-limit)，但是 secondary 节点在回放时非常好处理。回放时只需要将事务 oplog 中的数组解开，得到具体的写操作后进行 hash 就能并发回放了。本质上和普通的写操作并无太大区别。      

但是， 4.2 内核版本 oplog 的变化给回放流程带来了新的挑战，主要有：    
1. 如何保证**原子性**。假如 1 个事务有 4 条 oplog，secondary 节点在第 1 次拉取时拉到了 2 条 oplog（每次拉 oplog 的大小也是有限制的，可能刚好拉到前 2 条 oplog 时就达到了大小限制（每次拉取的 oplog 大小总和不超过 16MB），后面 2 条只能下一次再拉）。那么前面 2 条 oplog 能被回放吗，只回放事务的部分操作，岂不是破坏了事务的原子性？另外，第 2 次将事务的后 2 条 oplog 也拉过来了，如何找到这个事务前 2 条 oplog 呢？
2. 如何进行 **2 阶段事务**的并发。为了回放的正确性，要保证同一事务的 prepare 在 commit/abort 之前执行。Prepare 中的操作也不能直接解析成普通的写操作进行回放，因为此时还不确定事务能否成功提交。另外，Commit/abort 操作也不能和普通的写操作并发回放。

我们通过一个例子，看看如何处理 2 阶段事务。
假设事务包含 5 条 oplog，前 4 条是 prepare 阶段产生的，最后 1 条是 commit。如下图所示：    

<p align="center">
  <img src="https://github.com/user-attachments/assets/2e53457c-234f-4145-8e5c-a42bcaea9907" width=700>
</p>

第 1 阶段的回放流程和后续副本集事务例子的第 1 阶段相同，请参考第 3 节。     
第2 阶段流程和副本集事务有些区别：     
1. 处理到 t3，由于 partial = true, 先放到 cache 中。    
2. 由于 t4 是 prepare = true 的 oplog， 会独占一个 oplog batch，不和其他操作共享。处理到 t4 时，会从 oplog 表中把  t3、t2、t1 都回溯出来，然后根据这些 oplog 中的操作[执行 prepareTransaction](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/repl/transaction_oplog_application.cpp#L458)。注意这里是 prepareTransaction，而不是hash成普通的写操作执行，因为此时还不确定这些事务能否提交。    
3. 处理到 t5，这是 1 条 commit  oplog，会独占一个 oplog batch。Secondary 节点根据  commit oplog 中携带的 txnId sessionId 信息，找到之前 prepare 的上下文，然后[执行 commit](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/repl/transaction_oplog_application.cpp#L163)。    


## 2.4 关键的数据结构

### TransactionRouter
mongos 侧会给每个事务关联一个TransactionRouter，用于处理 mongos 侧的事务逻辑。主要包括：    
1. 根据路由转发事务内的读写命令。
2. 记录 participate shard list。    
3. 选择 coordinator shard：第 1 个参与的shard。    
4. 选择 recovery shard：第 1 个参与写操作的 shard。如果没有事务参与写操作（只读事务），则 recovery shard 为空。这些信息也会放到  recovery token 中，作为事务恢复流程的重要参考。    
5. 选择 commit/abort 策略：根据事务是否只读，以及写操作 shard 的个数是否大于 1，决定选择 1 阶段提交（commitTransaction 命令）还是 2 阶段提交（ coordinatorCommitTransaction）。    
6. （异常场景）执行 recovery 流程：正常情况下，事务的多个操作不能跨client，也不能跨 mongos(官方版本驱动的连接池机制都会保证这一点)。如果 mongos 中途挂了，client 可以根据 recovery token 去另外一个 mongos 上恢复对事务的控制。    

### TransactionCooordinator
TransactionCooordinator 实现了 shard 侧的 2 阶段提交协议（对应了 coordinatorCommitTransaction 命令的执行流程）。核心实现逻辑参考[代码链接](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/s/transaction_coordinator.cpp#L135-L320)，主要流程如下：    
1. 持久化 participantsList 到 config.transaction_coordinators 表中，并等待这个操作完成 majority 复制。    
2. 并发发送 prepareTransaction 命令到相关的 shard，收集各个 shard 的 prepare 结果。    
3. 如果 prepare 阶段有 shard 投了 abort，则最终决定为 abort。    
4. 如果prepare阶段都投了赞成票，则最终决定为 commit。则采用[最大 prepareTimestamp](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/s/transaction_coordinator_util.cpp#L214) 作为整个事务的 commitTimestamp，并推进clusterTime。    
5. 将最终决定（abort 或者commit）持久化到 config.transaction_coordinators 表中，并等待这个操作完成 majority 复制。    
6. 执行 2 PC 的第 2 阶段流程。如果最终决定是 commit，则给 participantsList 中的 shard 发 commitTransaction 命令；如果最终决定是 abort，则给 participantsList 中的 shard 发 abortTransaction 命令。并等待命令执行完成。    
7. 清理 config.transaction_coordinators 中存储的临时数据。    

### TransactionParticipant
[TransactionParticipant](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/transaction_participant.h#L79) 定义了每一个参与事务的 shard 的具体执行逻辑。比如 commitTransaction 和 abortTransaction 命令，都会调用 TransactionParticipant 的接口。为了支持分布式事务，TransactionParticipant 需要支持包含 prepareTransaction、commitPreparedTransaction 在内的所有事务相关接口。    

另外，TransactionParticipant 引入了 SideTransactionBlock 的概念，用于在  prepareTransaction 阶段切换到一个新事务去写 oplog（写完之后再切换回来），但是还能继承当前 OperationContext 持有的锁资源。    

上述设计主要是解决这样一个问题：在 4.0 副本集事务场景下，对表、索引和oplog 的修改是在同一个事务中进行的，必须要保证原子性。但是在 4.2 分布式 2PC 事务场景下，如果还这样做，prepareTransaction 的 oplog 在事务提交之前是不可见的，不能同步到 secondary 节点。因此，prepareTransaction 写 oplog 需要在独立的事务中操作才能正常同步。而 commitTransaction 操作，仍然需要在原先的事务中一起提交。

## 2.5 异常处理
### recover 恢复流程
#### 为什么要有 recover 流程    
想象一种场景：client 给 mongos 发了一个 commitTransaction 命令，然后这个 mongos 节点马上就挂了，此时事务在 shard 节点上提交了没有呢？
能想到的可能性有很多：  
1. mongos 先挂了，来不及给 coordinatorShard 发 coordinatorCommitTransaction 命令。
2. mongos 发出了 coordinatorCommitTransaction：    
	2.1 2PC 流程正在执行。    
	2.2 2PC 流程提交成功。    
	2.3 2PC 流程提交失败，置为 abort。     

作为 client，此时已经和 mongos 失去了联系，它急切地需要通过其他 mongos 连上去，看看事务到底是什么状态，并决定下一步动作（比如是否重试）。    

但是正如前文所说，事务是不能跨 mongos 的。Client 在 mongos1 上执行的事务，mongos2 中并没有相关的元数据信息，那么在 mongos2 上如何查询之前的事务状态呢？    

#### recoveryToken 具体是啥
recoveryToken 是 client 在跨 mongos 恢复时的纽带。    
recoveryToken 中包含的信息为 recoveryShardId, 举例如下：    
```
 { recoveryShardId: "shard2" }
```
recoveryShard 是第一个参与写操作的 shard, 而 coordinatorShard 是第一个参与事务操作的 shard。    
因此 recoveryShard 和 coordinatorShard 可能相同（事务的第一个操作就是写操作），也可能不同（事务的第一个操作是读操作）。而且 recoveryShard 也可能是空（只读事务）。    

>为啥要将 recoveryShard 和 coordinatorShard 分开呢？
>个人认为：
>1. 可以通过 recoveryShard 为空来确认只读事务，从而在 mongos 可以快速处理。
>2. coordinatorShard 上可能并不会执行具体的事务提交操作，如果到 coordinatorShard 上执行 recover 操作可能并不合适。理论上来讲 coordinatorShard 上的 config.transaction_coordinators 表很快就被清理了，TransactionCoordinator 对象也会很快被销毁。

#### 分片集群中的 recover 执行流程    
Client 在换 mongos 节点重试 commitTransaction 命令时，会携带 recoveryToken。        
mongos 收到命令后，会执行 [commitWithRecoveryToken](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/s/transaction_router.cpp#L1221) 逻辑：给对应的 recoveryShard 发 coordinateCommitTransaction 命令，命令中的 participantsList 为空（用来重试的 mongos 也不知道 participantsList 都有谁）。    

recoveryShard 的主节点收到 coordinateCommitTransaction 命令后，会执行如下逻辑：    
1. 如果能找到 transactionCoordinator (recovery 和 coordinator 属于同一个 shard)，则等待 transactionCoordinator 的 2PC 流程结束。    
2. 如果没有找到 transactionCoordinator (recovery 和 coordinator 属于不同 shard)，则根据本地 transactionParticipant 中的事务状态进行判断：    
    2.1 如果本地事务处于 commit 状态，则表明整个分布式事务也处于 commit 状态，此时直接返回。    
    2.2 如果本地事务处于 prepare 状态，说明正处于 2PC 流程中，此时等待下一步的提交结果并返回。    
    2.3 如果本地事务处于 abort 状态，返回 abort 状态。    
    2.4 如果本地事务处于 InProgress 状态（事务开启了，但是还没到 prepare），此时终止本地事务，返回 abort 状态。    
    
>为什么本地事务处于 InProgress 状态时要主动终止，而不是等待 2PC 状态发起提交？      
个人认为：      
如果此时本地事务处于 InProgress，则说明上一个正常的 commit 请求很有可能没有通过 mongos 转发到 coordinatorShard 并启动 2PC 流程。      
此时，recover 请求也没有办法准确的知道事务进行到了哪一步，它也没有能力自己来 1 次 2PC，唯一能做的就是把自己shard 的本地事务 abort 掉。如果 abort 之后， coordinator 的 prepare 请求又过来了也会失败。这种情况下，主动失败然后交给 client 做下一步决策可能是更好的选择（否则可能要等到事务的 60s 超时才会失败，这样处理的时长不可接受）。      
上述策略看起来有些“赌博行为”，因为主动 abort 本地事务，是有可能导致“迟迟到来”的 2PC 流程直接失败的。       
不过我认为这个策略的正向意义要更大一点：如果 recover 请求都到了，说明距离上一次的正常 commit 请求已经过去很久了（要算上客户端超时的话，一般都是秒级）。这么久了， 2PC 的 prepare 还没过来，十有八九是丢了。       

综上所述， recover 流程并**不是继续执行事务命令，或者尝试提交事务**。相反，它只是用来**获取事务的最终状态，以及尽快终止大概率会失败的事务**。

# 3. 副本集事务
## 3.1 持久性和 WriteConcern
在单机场景下，事务通过 **WAL** 来保证持久性。    
在多副本场景下，即使 primary 节点能通过 WAL 保证自己的持久性，也可能会由于切主操作导致已经执行的事务出现回滚。因此，MongoDB 在副本集模式下，通过 **writeConcern: majority** 来保证事务的持久性。   

另外，即使 readPreference 的存在可以允许读写分离，但是在事务中是不允许的。事务中的读操作只能发给 Primary 节点。      
从前面的分析我们可以知道，事务只有在 commit （或者 2 阶段的 prepare）之后才会同步到 secondary 节点。如果事务仍在进行中，此时去 secondary 节点进行读操作，是没有事务的元信息的。    

## 3.2 Speculative majority commit
      
如果仍然将传统 majority/snapshot 的处理方式应用到事务中，会导致事务都是基于比较老版本的数据进行读写操作，这样在提交阶段会有很大的概率出现写冲突，然后进入失败重试逻辑。    
MongoDB 为了解决上述问题，引入了 speculative 机制。事务中的读写操作不再是根据大多数提交的时间戳去读，而是使用 "[no overlap](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/read_concern_mongod.cpp#L338)" 时间：存储引擎 [all_durable](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/storage/wiredtiger/wiredtiger_recovery_unit.cpp#L650) 时间戳和 [lastApplied](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/storage/wiredtiger/wiredtiger_recovery_unit.cpp#L676) 时间戳（secondary 节点）取小值。    

通过上述策略，事务在提交之前能够保证读取到更加新鲜的数据，降低冲突回滚的概率，但是不保证读取到的数据在未来是不会回滚的。在同时指定事务的writeConcern 也是 majority 时，就能保证在事务提交成功时，之前读取的数据也是 majority 提交的。引用官方文档中的描述：    
>This means a transaction speculatively executes without ensuring that the data read won't be rolled back until it commits. No matter the read concern, when a node goes to commit a transaction, it waits for the data that it read to be majority committed *as long as the transaction was run with write concern majority*. Because of speculative behavior, this means that the transaction can only provide the guarantees of majority read concern, that data that it read won't roll back, if it is run with write concern majority.

《[Tunable Consistency in MongoDB](https://www.vldb.org/pvldb/vol12/p2071-schultz.pdf)》 论文中列举了一个例子来说明 speculative 机制的效果：    
<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/b1020a7c-0ff7-472c-913f-992cd287d77f" width=500>
</p>

C1 和 C2 是2 个客户端，它们几乎同时修改了同一条文档。P 是 primary 节点， S 是 secondary 节点。时间线从上到下。    
上图左侧是采用 speculative 机制的情况。update1 在 P 节点上执行成功后， update2 就能基于这个版本成功执行事务，不需要等待 update1 完成大多数提交。    
上图右侧是不采用 speculative 机制的情况。update1 在 P 上执行完成后， update2 还读不到它，而update2如果基于老版本的数据去执行之后会由于冲突检测判定失败。因此，update2 要等待 update1 复制到 S 节点后才能成功执行事务。可以看到，由于**图中红圈部分的等待，update2 的执行时间会大大增加。**

## 3.3 oplog 同步

我们通过一个副本集事务的 oplog 回放流程，来探讨如何解决回放的原子性问题。    
假设一个很大的副本集事务依次生成了 4 条 oplog，时间戳分别是 t1、t2、t3、t4。Secondary 节点第 1 批拉取到了 t1 和t2, 第 2 批拉取到了t3 和 t4。如下图所示：    

<p align="center">
  <img src="https://github.com/user-attachments/assets/f1810c15-8de6-400b-9af0-c20b12cca90f" width=700>
</p>

先看第 1 批的回放流程：    
1. Secondary节点先将拉取到的所有 oplog 都写到自己的 oplog 表中（异步执行，**包括 t1 和 t2**）。
2. 执行 oplog 的 hash 流程：    
2.1 处理到 t1, 发现是事务的一部分 oplog，即 partial = true。先不回放，而是将这条 oplog 放在[内存 cache](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/repl/sync_tail.cpp#L1199-L1212) 中。这里是为了方便后续回溯，而 第 1 步的oplog 写表异步任务可能还没完成，因此必须引入内存 cache。    
2.2 处理到 t2，还是 partial = true, 也放到内存 cache 中。    
3. 等待第 1 步写 oplog 表的异步任务执行完，然后将第 1 批 oplog 全部回放完。在此过程中，t1 和 t2 中的操作并没有被执行，而且内存 cache 也被清空。

接着再回放第 2 批：    
1. Secondary节点先将拉取到的所有 oplog 都写到自己的 oplog 表中（异步执行）。
2. 执行 oplog 的 hash 流程：    
2.1 处理到 t3，发现 partial = true, 放到内存 cache中。    
2.2 处理到 t4，发现是事务的最后一条 oplog，要准备真正执行回放了。则通过 sessionId 找 cache 中的 t3，然后通过 t3->[prevOpTime](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/repl/transaction_oplog_application.cpp#L269)字段找到上一条 oplog 的时间戳是 t2, 并使用这个时间戳到[本地 oplog 表](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/transaction_history_iterator.cpp#L59-L61)中把 t2 的完整 oplog 找到。接着用同样的方法找 t2->prevOpTime，得到t1 的完整 oplog。最后发现 t1->prevOpTime = null，此时 oplog 的回溯流程结束。得到事务的完整 oplog： t1、t2、t3、t4。    
2.3 在回溯 oplog 的过程中，也会将上述 4 条 oplog 中包含的操作都解析出来，放在一个数组中并且[排好序](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/repl/transaction_oplog_application.cpp#L290-L305)。然后将这个数组 hash 到多个并发线程中。    
3. 等待第 1 步写 oplog 表的异步任务执行完，然后执行这批 oplog 的操作。由于是在一个 batch 中回放完成的，因此能够保证事务回放的原子性。


**"临时"事务：SideTransactionBlock**    

在一个事务 prepare 完成时，会同步写入对应的 oplog，这部分 oplog 需要被 secondary 节点拉取，避免阻塞后续其他事务的 oplog 同步。    
但是这个事务此时只是 prepared 状态，并没有提交，如果写 oplog 的动作也放在用户操作的底层事务中，那么这个 oplog 必然对 secondary 节点不可见。而且由于 oplog 空洞的原因，必然也无法同步后续其他事务的 oplog.    

如何解决这个问题呢？    
显然 prepare 操作的 oplog 不能放在用户事务中，需要单独开一个事务去执行。       
MongoDB 内核中提供了一个 RAII 类 [TransactionParticipant::SideTransactionBlock](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/transaction_participant.h#L231) . 在对象构造时能够保存当前 opCtx 的锁资源和事务信息，后续代码可以开启一个新的事务进行操作并提交，最后 SideTransactionBlock 析构时，会还原最初的事务状态从而继续执行原有事务。     

内核中使用 SideTransactionBlock 的场景主要有：    
1. [更新索引的 multikey 属性](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/catalog/index_catalog_entry_impl.cpp#L311)。通过开启一个独立事务完成本不属于用户事务的操作，减少事务冲突。    
2. [写 profile](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/ops/write_ops_exec.cpp#L143) 信息。
3. [分配 oplog slot](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/transaction_participant.cpp#L665).
4. [写 prepare 状态的 oplog](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/op_observer_impl.cpp#L1504), 避免出现 oplog 可见性问题，阻塞同步流程。 

>注1： SideTransactionBlock 与关系型数据库中常见的**子事务**并不相同。SideTransactionBlock 是一个完全独立的事务，而子事务是在一个父事务中开启的事务。

>注2：常见的关系型数据库（比如 PostgreSQL）通过 WAL 文件来记录 prepared 事务的信息，所以不会有可见性问题。但是 MongoDB 中，使用了 oplog 表来记录，因此会遇到上述问题。
 
# 4. 单机事务
## 4.1 MongoServer 层的封装
### RecoveryUnit

[RecoveryUnit](https://github.com/mongodb/mongo/blob/master/src/mongo/db/catalog/README.md#recoveryunit) 是 MongoServer 侧封装的事务接口，WireTigerRecoveryUnit 是针对 WT 引擎的具体实现。     

每个 OperaionContext 都会有一个 RecoveryUnit，每个 RecoveryUnit 都绑定一个 WT_Session 去完成具体的事务操作。    

### WUOW(Write Unit Of Work)
[WUOW](https://github.com/mongodb/mongo/blob/master/src/mongo/db/catalog/README.md#writeunitofwork) 是 MongoServer 侧的事务单元，RAII 模式。    
WUOW 支持嵌套，处于顶层的 WUOW 能够完成真正的事务提交。

## 4.2 WT 引擎的事务处理
### 事务接口
WT 引擎的事务接口封装在 [WT_SESSION](http://source.wiredtiger.com/11.1.0/struct_w_t___s_e_s_s_i_o_n.html) 中，具体包括：        
1. [begin_transaction](https://github.com/mongodb/mongo/blob/r4.2.25/src/third_party/wiredtiger/src/include/wiredtiger.in#L1789): 开启事务。    
2. [timestamp_transaction](https://github.com/mongodb/mongo/blob/r4.2.25/src/third_party/wiredtiger/src/include/wiredtiger.in#L1906): 设置事务的时间戳(prepare_timestamp, commit_timestamp)。    
3. [prepare_transaction](https://github.com/mongodb/mongo/blob/r4.2.25/src/third_party/wiredtiger/src/include/wiredtiger.in#L1854): 准备事务。    
4. [commit_transaction](https://github.com/mongodb/mongo/blob/r4.2.25/src/third_party/wiredtiger/src/include/wiredtiger.in#L1829): 提交事务。     
5. [rollback_transaction](https://github.com/mongodb/mongo/blob/r4.2.25/src/third_party/wiredtiger/src/include/wiredtiger.in#L1878): 回滚事务。    

### MVCC
MVCC 部分请参考前述 [4.1 章节](https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/blob/main/4.ING-%E6%89%A7%E8%A1%8C%E5%99%A8/1.%E9%94%81%E4%B8%8E%E5%B9%B6%E5%8F%91%E6%8E%A7%E5%88%B6.md#32-mvcc-%E5%B9%B6%E5%8F%91%E6%8E%A7%E5%88%B6)中的描述。


### WriteConflict 和 PrepareConflict
WriteConflict:  在 WT 引擎的 MVCC 模型采用了谁先更新谁成功的策略。如果 2 个事务并发更新了同一条数据，则有 1 个事务会报错 writeConflict 异常。MongoServer 层在捕获这个异常之后，如果不是多文档事务，会在内核中直接对事务进行[重试](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/concurrency/write_conflict_exception.h#L81)。       
如果是多文档事务，则会将错误抛给客户端，并在错误中带上 [TransientTransactionError 标签](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/base/transaction_error.cpp#L34)。客户端代码对于`TransientTransactionError` 类型的错误，可以直接进行重试。比如 go driver 中的 [Session::WithTransaction](https://github.com/mongodb/mongo-go-driver/blob/v2.0.0/mongo/session.go#L152) 接口，就会自动对这类错误进行重试。       

PrepareConflict: 在事务 prepare 之后，对应的 snapshot 也会释放（推测是为了性能考虑）。在事务的执行过程中，如果遇到 prepare 状态的版本，则抛出 [PrepareConflict 异常](https://github.com/mongodb/mongo/blob/r4.2.25/src/third_party/wiredtiger/src/include/txn.i#L780)。MongoServer 层在捕获这个异常之后，进行[等待和检查](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/storage/wiredtiger/wiredtiger_prepare_conflict.h#L69)，直到 prepare 状态的事务提交或者回滚。

# 5. MongoDB 事务的局限性

事务具备 ACID 特性。个人认为，其中的 A、I、D 特性的最终目标还是为了保证 C ，即数据的一致性。而关于数据的一致性，个人认为至少有 2 层含义：

1. 狭义的理解：数据库的约束（比如唯一性约束、外键约束） 以及相关的检查逻辑（比如触发器等）。
2. 广义的理解：数据的正确性。除了在数据库层面需要保证正确性（比如脏读等）之外，还需要业务层面进行配合。**那么作为业务系统开发者，就非常有必要底层数据库能提供哪些能力，在不同的使用模式下是否会造成数据正确性问题？**

那么，在使用 MongoDB 事务时，需要注意哪些呢？

首先需要知道哪些是事务支持的命令和参数，哪些是不支持的。关于这点，可以参考[官方文档](https://www.mongodb.com/docs/manual/core/transactions/)中的描述。

接下来，还需要知道相比其他常用的数据库（比如 MySQL、PostgreSQL），MongoDB 的事务存在哪些不足。这些信息可以给业务选型时提供重要参考。

## 5.1 Snapshot 隔离级别无法解决的写偏斜（write skew）问题
MongoDB 事务中支持以下 [3 种隔离](https://www.mongodb.com/docs/manual/core/transactions/#transactions-and-read-concern)级别（read concern）    
- **local**: 类似传统关系型单机数据库中的快照读。不过扩展到 MongoDB 的副本集事务模型， 这里更准确的说法是 read local snapshot。读到的数据可能只在主节点上提交，可能由于后续的 fail-over 导致读到的数据被回滚。    
- **majority**：比 local 的级别更高，读取到的数据都是在多数节点上提交的。    
- **snapshot**：比 majority 的级别更高，全局一致快照读，能够解决分片集群中可能出现的不一致读问题。

需要注意的是，snapshot 隔离级别距离可串行化级别还是有差距的，比如无法解决写偏斜（write skew）问题。    

比如医院有一个医生的值班系统，数据库中记录了医生的 ID 和今天是否值班的信息。   
医生 A 发起了一个值班事务1 ：检查到数据库表中没有人值班，即医生 A 和 医生 B 的值班状态都是 false。    
同时医生 B 也发起了一个一样的值班事务2： 也检查到数据库表中没有人值班。

事务 1 将医生 A 的值班状态改为 true， 并提交。    
事务 2 将医生 B 的值班状态改为 true， 并提交。    

由于 snapshot 隔离级别主要判断的依据是写冲突，因此上述 2 个事务并没有写冲突，因此都提交成功了。但是，这显然是不符合业务预期的。    

不同的数据库系统，可以对应不同的解决方案，比如：     
- 在 MySQL 中，可以通过 SELECT ... FOR UPDATE 锁定读来解决（后执行该语句的事务会等待前一个事务结束释放锁），或者通过 SERIALIZABLE 隔离级别来解决（1个事务能提交成功，另外的事务提交失败依赖重试）。   
- PostgreSQL 类似，也可以通过 SELECT ... FOR UPDATE 锁定读来解决，或者通过 SERIALIZABLE 隔离级别来解决（依赖数据库的可串行化调度检测来发现上述 2 个事务的读写依赖）。    
- 在 MongoDB 中，则需要通过其他方式制造锁冲突或者写冲突。比如引入一个分布式锁。

## 5.2 不支持子事务
MongoDB 不支持嵌套子事务，即 PostgreSQL 和 MySQL 中的 SAVEPOINT 和 ROLLBACK TO SAVEPOINT 相关语法。     
因此，对于一些长事务的场景，MongoDB 事务的回滚代价会非常大。

## 5.3 同一事务内，多个命令的可见性问题
除了事务之间的隔离问题之外，同一个事务内部多条命令相互之间的可见性问题也是一个有趣的话题。       
对于 PostgreSQL 这种传统关系型数据库来说，会在一个事务内部通过 CommandId 来标志不同命令，后一个命令的 CommandId 会比前一个的大。因此，后一个命令可以看到前一个命令的修改结果，而不能反过来。    
对于 MongoDB 来说，存储引擎中 snapshot 级别的可见性判断只依赖与事务 ID 和 timestamp，并没有对单个命令进行特殊标志。而在这种 snapshot 级别中，每个命令是能看到本事务中的修改的。      

下面列举一个例子来说明 2 者的区别 ： 在事务中先创建 CURSOR， 随后插入一条数据，随后迭代 CURSOR 继续读取数据。    
对于 PostgreSQL 来说，前面定义的 CURSOR 看不到后面插入的数据。而 MongoDB 则可以看到。    
PostgreSQL:
```sql
-- 表中有 2 条数据
postgres=# SELECT * FROM t1;
 c1 | c2 
----+----
  1 |  1
  2 |  2
(2 rows)
-- 开启事务，隔离级别为可重复读，或者默认的提交读
postgres=# BEGIN ISOLATION LEVEL REPEATABLE READ;
BEGIN
-- 先定义 CRUSOR
postgres=# DECLARE c CURSOR FOR SELECT * FROM t1;
DECLARE CURSOR
-- 后插入新数据
postgres=# INSERT INTO t1 VALUES (3,3);
INSERT 0 1
postgres=# SELECT * FROM t1;
 c1 | c2 
----+----
  1 |  1
  2 |  2
  3 |  3
(3 rows)

postgres=# FETCH c;
 c1 | c2 
----+----
  1 |  1
(1 row)

postgres=# FETCH c;
 c1 | c2 
----+----
  2 |  2
(1 row)

-- 不断迭代 CURSOR, 看不到后插入的数据 (3,3)
postgres=# FETCH c;
 c1 | c2 
----+----
(0 rows)
```

MongoDB:
```js
mymongo4.2:PRIMARY> s = db.getMongo().startSession()
session { "id" : UUID("602f0d29-b84c-4101-a768-69b9b08c1977") }
mymongo4.2:PRIMARY> s.startTransaction({readConcern:{level:"snapshot"}})
mymongo4.2:PRIMARY> coll = s.getDatabase("db1").tt1
db1.tt1
mymongo4.2:PRIMARY> db1 = s.getDatabase("db1")
db1
// 表中有 2 条数据
mymongo4.2:PRIMARY> coll.find()
{ "_id" : ObjectId("67791846a9737c048dfa72fa"), "a" : 1, "b" : 1 }
{ "_id" : ObjectId("67791846a9737c048dfa72fb"), "a" : 2, "b" : 2 }
// 先开启一个 CURSOR
mymongo4.2:PRIMARY> db1.runCommand({find:"tt1", batchSize:1})
{
	"cursor" : {
		"firstBatch" : [
			{
				"_id" : ObjectId("67791846a9737c048dfa72fa"),
				"a" : 1,
				"b" : 1
			}
		],
		"id" : NumberLong("856596613456525767"),
		"ns" : "db1.tt1"
	},
	"ok" : 1,
	"$clusterTime" : {
		"clusterTime" : Timestamp(1735990440, 1),
		"signature" : {
			"hash" : BinData(0,"AAAAAAAAAAAAAAAAAAAAAAAAAAA="),
			"keyId" : NumberLong(0)
		}
	},
	"operationTime" : Timestamp(1735990440, 1)
}
// 后插入 2 条新数据
mymongo4.2:PRIMARY> coll.insertOne({a:3,b:3})
{
	"acknowledged" : true,
	"insertedId" : ObjectId("67791cb489451e6cd7184296")
}
mymongo4.2:PRIMARY> coll.insertOne({a:4,b:4})
{
	"acknowledged" : true,
	"insertedId" : ObjectId("67791cb889451e6cd7184297")
}

// 迭代 CURSOR，可以看到后插入的数据
mymongo4.2:PRIMARY> db1.runCommand({getMore: NumberLong("856596613456525767"), collection:"tt1", batchSize:1})
{
	"cursor" : {
		"nextBatch" : [
			{
				"_id" : ObjectId("67791846a9737c048dfa72fb"),
				"a" : 2,
				"b" : 2
			}
		],
		"id" : NumberLong("856596613456525767"),
		"ns" : "db1.tt1"
	},
	"ok" : 1,
	"$clusterTime" : {
		"clusterTime" : Timestamp(1735990470, 1),
		"signature" : {
			"hash" : BinData(0,"AAAAAAAAAAAAAAAAAAAAAAAAAAA="),
			"keyId" : NumberLong(0)
		}
	},
	"operationTime" : Timestamp(1735990470, 1)
}
// 找到了新数据
mymongo4.2:PRIMARY> db1.runCommand({getMore: NumberLong("856596613456525767"), collection:"tt1", batchSize:1})
{
	"cursor" : {
		"nextBatch" : [
			{
				"_id" : ObjectId("67791cb489451e6cd7184296"),
				"a" : 3,
				"b" : 3
			}
		],
		"id" : NumberLong("856596613456525767"),
		"ns" : "db1.tt1"
	},
	"ok" : 1,
	"$clusterTime" : {
		"clusterTime" : Timestamp(1735990470, 1),
		"signature" : {
			"hash" : BinData(0,"AAAAAAAAAAAAAAAAAAAAAAAAAAA="),
			"keyId" : NumberLong(0)
		}
	},
	"operationTime" : Timestamp(1735990470, 1)
}
mymongo4.2:PRIMARY> db1.runCommand({getMore: NumberLong("856596613456525767"), collection:"tt1", batchSize:1})
{
	"cursor" : {
		"nextBatch" : [
			{
				"_id" : ObjectId("67791cb889451e6cd7184297"),
				"a" : 4,
				"b" : 4
			}
		],
		"id" : NumberLong("856596613456525767"),
		"ns" : "db1.tt1"
	},
	"ok" : 1,
	"$clusterTime" : {
		"clusterTime" : Timestamp(1735990470, 1),
		"signature" : {
			"hash" : BinData(0,"AAAAAAAAAAAAAAAAAAAAAAAAAAA="),
			"keyId" : NumberLong(0)
		}
	},
	"operationTime" : Timestamp(1735990470, 1)
}
mymongo4.2:PRIMARY> db1.runCommand({getMore: NumberLong("856596613456525767"), collection:"tt1", batchSize:1})
{
	"cursor" : {
		"nextBatch" : [ ],
		"id" : NumberLong(0),
		"ns" : "db1.tt1"
	},
	"ok" : 1,
	"$clusterTime" : {
		"clusterTime" : Timestamp(1735990470, 1),
		"signature" : {
			"hash" : BinData(0,"AAAAAAAAAAAAAAAAAAAAAAAAAAA="),
			"keyId" : NumberLong(0)
		}
	},
	"operationTime" : Timestamp(1735990470, 1)
}
mymongo4.2:PRIMARY> s.commitTransaction()

```

## 5.4 全表扫描能否读到不一致的数据
如果 read committed 级别事务中，有一个大表扫描命令，执行时间很长。在扫描表的前半部分时，另外一个事务在表的后半部分修改了一些数据并完成提交。那么这个扫描能否读到这些新修改的数据？

对于 PostgreSQL 来说，不管是什么隔离级别，事务内的每个单独的 command 都是用同一个 MVCC 快照，因此不会出现上述情况。    
我们可以执行下面的实验：    

先建表并插入 300 条数据，共占 2个 8 KB的 page：
```sql
postgres=# create table t3 (c1 int, c2 int);
CREATE TABLE
postgres=# insert into t3 select generate_series(1,300) as c1,  generate_series(1,300) as c2;
INSERT 0 300
postgres=# select pg_relation_size('t3');
 pg_relation_size 
------------------
            16384
(1 row)
```

然后开启事务 1，并启动全表遍历。为了方便测试，每扫描 1 行，停顿 1 秒：
```sql
-- 客户端11
postgres=# begin transaction isolation level read committed;
BEGIN
postgres=*# select c1,c2,pg_sleep(1) from t3;
```

然后在另外一个窗口将第 300 行数据的 c2 字段从 300 更新为 301：
```sql
-- 客户端2
postgres=# update t3 set c2=301 where c1=300;
UPDATE 1
postgres=# select * from t3 where c1=300;
 c1  | c2  
-----+-----
 300 | 301
(1 row)
```

事务 1 在执行约300秒后结束，从返回的结果来看并未读到新版本数据：
```sql
-- 客户端 1
postgres=*# select c1,c2,pg_sleep(1) from t3;
 c1  | c2  | pg_sleep 
-----+-----+----------
   1 |   1 | 
   2 |   2 | 
...
 299 | 299 | 
 300 | 300 | 
(300 rows)

postgres=*# commit;
COMMIT
```

需要注意，在 read committed 及以下隔离级别时，事务内不同的 command 可能因为不同的快照读到不同的数据。在上述例子中，如果在事务内再执行一遍 select 全表扫描，就能看到新提交的版本。


对于 MongoDB 来说，对于单机引擎只支持 “snapshot” 级别，因此不会有上述问题。
我们进行下述实验。

先插入 300 行数据：
```js
mymongo:PRIMARY> for (var i=1;i<=300;i++) {db.coll7.insert({c1:i, c2:i})}
WriteResult({ "nInserted" : 1 })
```

事务 1 启动一个事务进行扫描：
```js
// 事务 1
mymongo:PRIMARY> s.startTransaction({readConcern:{level:"local"}})
mymongo:PRIMARY> coll = s.getDatabase("db1").coll7
db1.coll7
mymongo:PRIMARY> coll.find()
{ "_id" : ObjectId("67cd92a2a94bb26da82af9a7"), "c1" : 1, "c2" : 1 }
.... // 默认的 BatchSize 是 101 条，MongoShell 界面上按 20 每组显示
```

在事务 1 没有扫描到第 300 条之前，启动事务 2，将第 300 条的 c2 字段从 300 改成 301，并提交：
```js
mymongo:PRIMARY>  s.startTransaction({readConcern:{level:"local"}})
mymongo:PRIMARY> coll = s.getDatabase("db1").coll7
db1.coll7
mymongo:PRIMARY> coll.update({c1:300},{$set:{c2:301}})
WriteResult({ "nMatched" : 1, "nUpserted" : 0, "nModified" : 1 })
mymongo:PRIMARY> s.commitTransaction()
```

事务 1 继续扫描，看到的第 300 条数据仍然是该事务开始时刻的版本：
```js
Type "it" for more
mymongo:PRIMARY> it
...
{ "_id" : ObjectId("67cd92a5a94bb26da82afad1"), "c1" : 299, "c2" : 299 }
{ "_id" : ObjectId("67cd92a5a94bb26da82afad2"), "c1" : 300, "c2" : 300 }
mymongo:PRIMARY> s.commitTransaction()
```

**MongoDB 有些特殊的地方在于：如果不显示开启事务，则不会在 MongoServer 层开启默认的事务。对应到执行器中，就是同一个扫描命令，可能会在执行过程中由于 yield 机制不断更新 snapshot，导致整个过程中不是一致性快照读。**

还是上述例子，在**非事务模式**下启动一个全表扫描：
```js
mymongo:PRIMARY> db.coll7.find()
{ "_id" : ObjectId("67cd92a2a94bb26da82af9a7"), "c1" : 1, "c2" : 1 }
{ "_id" : ObjectId("67cd92a2a94bb26da82af9a8"), "c1" : 2, "c2" : 2 }
```

然后在另外一个客户端修改第 300 条数据：
```js
mymongo:PRIMARY> db.coll7.update({c1:300},{$set:{c2:301}})
WriteResult({ "nMatched" : 1, "nUpserted" : 0, "nModified" : 1 })
```

那么在第 1 个客户端扫描到第 300 行时，有**很大概率看到修改之后的数据**：
```js
Type "it" for more
mymongo:PRIMARY> it
...
{ "_id" : ObjectId("67cd92a5a94bb26da82afad1"), "c1" : 299, "c2" : 299 }
{ "_id" : ObjectId("67cd92a5a94bb26da82afad2"), "c1" : 300, "c2" : 301 }
```

## 5.5 MySQL、PostgreSQL、MongoDB 常见的事务陷阱及处理方法总结
||MySQL|PostgreSQL|MongoDB|
|--|--|--|--|
|lost update|RU/RC/RR/Serial 隔离级别下会发生；一般通过锁定读来解决|RU/RC 隔离级别下会发生；可通过锁定读或者 RR/Serial来解决。<br>RR/Serial 隔离级别下会有冲突报错，first-commit win.|不会发生，因为存储引擎侧都是 snapshot 隔离级别。<br>冲突的事务依赖客户端重试，first-update win.|
|dirty reads|RU 隔离级别可能发生|不会发生，因为RU 和 RC 在 Postgres 中的表现一致|不会发生|
|non-repeatable reads|RU/RC 隔离级别下会发生。<br>可以通过 RR/Serial 或者锁定读来解决|RU/RC 隔离级别下会发生。<br>可以通过 RR/Serial 或者锁定读来解决|不会发生|
|phantom reads|RU/RC 隔离级别下会发生。<br>可以通过 RR/Serial 或者锁定读来解决|RU/RC 隔离级别下会发生。<br>可以通过 RR/Serial 解决（锁定读不能解决，因为 Postgres 没有间隙锁）|不会发生|
|write-skew|RU/RC/RR/Serial 隔离级别下会发生。<br>可通过锁定读解决|RU/RC/RR 隔离级别下会发生。<br>可通过 Serial 隔离级别解决，也可通过表锁、锁定读解决（但要注意没有间隙锁）|会发生，而且不好解决（需要引入其他的锁，或者制造写冲突）|



# 6. 参考资料
1. https://github.com/mongodb/mongo/blob/master/src/mongo/db/catalog/README.md
2. https://github.com/mongodb/mongo/blob/r4.2.24
3. https://github.com/mongodb/mongo-go-driver/blob/v2.0.0
4. https://www.vldb.org/pvldb/vol12/p2071-schultz.pdf
5. http://source.wiredtiger.com/11.1.0/struct_w_t___s_e_s_s_i_o_n.html
6. MongoDB分布式事务之时间戳与混合逻辑时钟：https://zhuanlan.zhihu.com/p/61298232
7. Mongo4.2分布式事务实现Overview：https://mp.weixin.qq.com/s/MZOrOCE1aNnUe56RpwVd6g
8. MongoDB 事务，复制和分片的关系: https://mp.weixin.qq.com/s/QX9tm5TLfW7SghjDBD6QoQ
9. https://www.mongodb.com/docs/manual/core/transactions/
