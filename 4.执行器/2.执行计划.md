# 1. 导语
如何快速准确地找到需要的数据，是每个数据库需要考虑的核心问题。   

参考《数据库系统概念》书中的描述，查询处理的基本步骤一般包括：语法分析与翻译，优化器，查询计划和执行引擎。    
因此，本文将对照上述流程分析 MongoDB 中请求执行模块的实现。

# 2. 词法解析
由于 MongoDB 采用 BSON 格式表达命令，因此词法解析非常容易。不像传统关系型数据库 SQL需要进行字符串匹配，还要考虑大小写、空格等问题。    

具体来说，一个 BSON 格式的查询命令会通过 [parseFromFindCommand](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/query/query_request.cpp#L133) 解析成标准的 [QueryRequest](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/query/query_request.h#L53)。在解析的过程中还会进行合法性判断，比如 filter 必须是 Object 类型，limit 必须是数字等。    
QueryRequest 中包含了 nss、filter、skip、limit、batchSize、project、sort、hint 等参数。    

# 3. 语法解析
语法解析的核心是将 filter 解析成 [MatchExpression](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/matcher/expression.h#L57) 组成的语法树。MatchExpression 有 AndMatchExpression等派生类表示 "$and" 等逻辑运算，也有 GTEMatchExpression 等派生类表示 "$gte" 等比较运算。通过 VSCode 可以看到类的继承关系如下：    

<p align="center">
  <img src="https://github.com/user-attachments/assets/fc7992b6-0890-4b13-9c7c-eea4543d0081" width=400>
</p>

以下面的查询命令为例：    
```
{ find: "coll1", filter: { $and: [ { a: { $gte: 1 } }, { a: { $lte: 100000 } } ] }, skip: 5, limit: 10, sort: { c: 1 }, projection: { b: 1, c: 1 }}
```

首先会通过词法解析生成 QueryRequest。然后会调用 [parse](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/matcher/expression_parser.cpp#L244) 方法将 filter 解析成语法树，生成 [CanonicalQuery](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/query/canonical_query.h#L46)：     

<p align="center">
  <img src="https://github.com/user-attachments/assets/ae18e594-f3f0-497f-ae48-f35ecdc15c15" width=500>
</p>

# 4. 执行计划

## 4.1 执行流程
### 4.1.1 预处理阶段
[prepareExecution](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/query/get_executor.cpp#L371) 核心逻辑：    
1. 调用 [fillOutPlanerParams](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/query/get_executor.cpp#L252) 填充参数，用于指导后续执行计划的生成。完成的工作包括遍历索引、使用索引过滤器（相关用法可以参考 [planCacheSetFilter](https://www.mongodb.com/zh-cn/docs/v5.0/reference/command/planCacheSetFilter/) 命令）、是否支持 noTableScan、对于分片集群则判断是否需要 ShardFilter 过滤孤儿文档、是否需要 INDEX_INTERSECTION 和 GENERATE_COVERED_IXSCANS、是否需要 SPLIT_LIMITED_SORT、是否需要 OPLOG_SCAN_WAIT_FOR_VISIBLE。
2. 调用 [IDHackStage::supportsQuery](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/exec/idhack.cpp#L166) 判断能否直接用 _id 索引进行 IDHACK。如果能，则快速生成执行计划并返回。
3. 调用 [shouldCacheQuery](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/query/plan_cache.cpp#L136) 检查能否直接从 planCache 中获取执行计划。如果能从 planCache 中获取，则生成执行计划并快速返回。
4. 调用 [QueryPlanner::plan](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/query/query_planner.cpp#L539) 生成执行计划。具体来说会根据查询条件进行一轮规则匹配，比如 tailable cursor 会选择全表扫描 + 过滤的方式。
5. 如果是 count 查询，调用 [turnIxscanIntoCount](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/query/get_executor.cpp#L1143) 判断能否将 indexScan 或者 fetch with indexScan 转换为 countScan 进行加速。
6. 如果只有一个 querySolution，则直接生成 planStage 并返回给后续的流程执行。如果有多个 querySolution，则生成 MultiPlanStage 给后续流程去选择。

### 4.1.2 生成最优执行计划
[PlanExecutorImpl::make](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/query/plan_executor_impl.cpp#L189) 会生成真正的执行计划，如果在预处理阶段生成的是 STAGE_MULTI_PLAN、STAGE_SUBPLAN、 STAGE_CACHED_PLAN、 STAGE_TRIAL，则需要选择最优的执行计划。    

下面主要介绍 MultiPlanStage::pickBestPlan 的选择流程。

>如何选择最优的执行计划，是数据库优化器需要考虑的问题: RBO、CBO 还是其他？    
>RBO(Rule-Based Optimizer) 优化器相对简单，根据预先设定的规则，选择最优的执行计划。这种优化器对于数据并不敏感，可能会选择一些代价较大的执行计划。这种策略一般应用于早期的数据库系统。    
>CBO(Cost-Based Optimizer) 优化器相对复杂，需要结合实际的运行数据，计算出每个执行计划的代价，然后选择代价最小的执行计划。数据库需要有专门的流程（analyze/vacuum）去更新数据的统计信息（直方图），否则优化器无法正确选择代价最小的执行计划。这种策略在当前的关系型数据库中广泛使用。    
>MongoDB 作为 NoSQL 数据库，并没有照搬 RBO 和 CBO 的优化器，个人认为的原因：
>1. MongoDB 作为 "schema-free" 的数据库，没有固定的列定义，而且每一列的数据类型也是不固定的，因此本身没有像 SQL 数据库那样的统计信息，因此无法照搬 CBO 优化器。
>2. RBO 的缺点非常明显，显然也不是一个很好的选择。
>
>因此，MongoDB 选择了另外一种策略，即根据不同执行计划实际去运行采样，选择代价最小的执行计划。另外结合自身的 planCache 缓存机制，可以减少不必要的代价计算。    
相比传统的 CBO 优化器，MongoDB 的执行计划选择可能需要更多的 IO 和运行时间（因为要实际去执行一次），但是减少了数据统计信息的维护开销。 

MongoDB 选择最优执行计划的核心思路是：**将备选计划都执行一遍并打分，然后选择分数最高的去执行。**     
显然，将备选计划完整的执行一遍需要消耗很多资源，特别是全表扫描以及大数据量排序的场景。因此，MongoDB 对备选计划的执行做了一些限制，包括：    
1. 每个备选计划最多的调用次数（numWorks, 可以简单理解为扫描的行数）。对于小表来说默认为 [1 万](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/query/query_knobs.idl#L40)，对于大表来说为表的记录总数的 [30%](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/query/query_knobs.idl#L49).
2. 有备选计划获取了足够的结果数。默认为 [101 条](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/query/query_knobs.idl#L59)，或者有备选计划 EOF。

---

在“采样”执行结束后，会对备选计划打分。具体参考 [PlanRanker::pickBestPlan](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/query/plan_ranker.cpp#L68) 以及 [PlanRanker::scoreTree](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/query/plan_ranker.cpp#L191) 的流程。简单来说，每个执行计划的得分来源是：    
1. **baseScore** 基础分，固定为 **1.0** 。主要的目的是防止 0 分出现，因为 0 分对应的是 "no plan selected"。
2. **productivity** 生产力评分，取值为 [0,1]。计算公式如下：
    ```
    static_cast<double>(stats->common.advanced) / static_cast<double>(workUnits)

    // workUnits 是调用执行计划的次数
    // advanced 是每次调用产生有效结果的次数

    // 这个公式能够表示执行计划是否高效，如果接近 1 ，说明几乎每次执行都能产生有效结果
    ``` 
3. **noFetchBonus** 奖励分，取值为 std::min(1.0 / static_cast<double>(10 * workUnits), 1e-4)。如果有“covered projections” 则加上这个奖励分，因为不需要额外的 fetch 阶段来进行 projection，效果会更优。
4. **noSortBonus** 奖励分，取值为 std::min(1.0 / static_cast<double>(10 * workUnits), 1e-4)。如果不需要单独的"blocking sort stage" 就能完成排序则加上这个奖励分。一个比较典型的场景是直接利用索引完成排序，显然效果更优。
5. **noIxisectBonus** 奖励分，取值为 std::min(1.0 / static_cast<double>(10 * workUnits), 1e-4)。MongoDB 优化器认为同等情况下 "single index" 优于 "index intersection"。
6. **eofBonus** 奖励分，取值为 1.0 。如果有执行计划达到了 EOF， 则加上这个奖励分。
7. 如果设置了 [internalQueryForceIntersectionPlans](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/query/query_knobs.idl#L68) 参数（默认 false），则对包含 index intersection 的备选计划加 3.0 分。

### 4.1.3 执行阶段

**火山模型**    

MongoDB 的查询执行计划是按照“火山模型”来设计的，具有以下特点：
1. 每个操作抽象为独立的 Stage，最终构成一个树形结构。
2. 自顶向下调用。从根节点开始调用其子节点的 work() 接口, 逐级向下调用，并向上返回结果。是一个 pull-based 模型。
3. 在 MongoDB 中这些任务都是由单线程来执行的。

以一个常见的查询：`find({$or:[{a:100},{b:200}]})` 为例，假设已经分别对字段 a 和 b 建立索引，那么最终生成的执行计划如下图所示：

<p align="center">
  <img src="https://github.com/user-attachments/assets/eff80a94-7d5d-432b-a850-84e350ac3b75" width=300>
</p>

这种模型的优势有：
1. 灵活性；
2. 扩展性；
3. 逻辑清晰，易于理解和维护。

但是其缺点也非常明显：
1. 虚函数调用开销大；
2. 执行效率低，特别是全表扫描以及大数据量排序的场景。

和很多 AP 数据库具有的多线程调度、pipleline 机制、向量化执行等特性相比，MongoDB 的执行引擎并不具备优势。

**Yield机制**

Yield 机制可用于释放锁、快照等资源。不同的请求类型可以设置不同的 yield 策略。常见的 yield 策略可以按照特性和使用场景进行如下分类：

|[Yield 策略](https://github.com/mongodb/mongo/blob/r4.2.5/src/mongo/db/query/plan_executor.h#L103)|[释放锁](https://github.com/mongodb/mongo/blob/r4.2.5/src/mongo/db/query/plan_yield_policy.h#L95)|[释放快照](https://github.com/mongodb/mongo/blob/r4.2.5/src/mongo/db/query/plan_yield_policy.cpp#L84)|[检查是否主动终止*](https://github.com/mongodb/mongo/blob/r4.2.5/src/mongo/db/query/plan_yield_policy.cpp#L84)|[遵循 auto yield 策略](https://github.com/mongodb/mongo/blob/r4.2.5/src/mongo/db/query/plan_yield_policy.h#L117)|典型使用场景|
|:--|:--|:--|:--|:--|:--|
|YIELD_AUTO|**Y**|**Y**|**Y**|**Y**|普通的 find|
|WRITE_CONFLICT_RETRY_ONLY|N|**Y**|**Y**|**Y**|foreground indexBuilding|
|YIELD_MANUAL|**Y**|**Y**|**Y**|N| range deletion|
|NO_YIELD|N|N|N|N|findOne、dbHash、dbSize、listCollection|
|INTERRUPT_ONLY|N|N|**Y**|N|multiDocument-Transaction|

>_说明：checkForInterruptNoAssert 函数会检查是否需要主动终止。除了 yieldOrInterrupt，在很多流程中都会调用（比如加锁的时候）。_

PlanExecutorImpl 每轮迭代会[调用 shouldYieldOrInterrupt](https://github.com/mongodb/mongo/blob/r4.2.5/src/mongo/db/query/plan_executor_impl.cpp#L516) 检查是否需要 yield，如果没有设置强制 yield 标志（比如出现 WriteConflictException 时强制 yield），则会采用自定义的 yield “合并”策略。该策略会采取一些合并机制避免过于频繁的 yield 导致性能受损，具体来说有 2 个参数控制：
1. 距离上一次 yield，到现在调用 shouldYield 的次数internalQueryExecYieldIterations([默认 128](https://github.com/mongodb/mongo/blob/r4.2.5/src/mongo/db/query/query_knobs.idl#L215)).
2. 距离上一次 yield，到现在的时间 internalQueryExecYieldPeriodMS（[默认10ms](https://github.com/mongodb/mongo/blob/r4.2.5/src/mongo/db/query/query_knobs.idl#L222)）.    

满足上述条件之一，才会 yield。

有了 Yield 机制后，一条耗时很长的扫描操作会定期释放锁，因此不会一直阻塞其他流程的删表、建索引、甚至停服务等操作。另外 yield 机制定期释放快照的操作，也会减轻 WT 引擎维护 MVCC 的压力。

Yield 机制带来的警示是：**普通的扫描操作可能读取的是涉及多个快照的非一致性数据**。在一些需要一致性保证的场景中，需要特别注意。    
比如，在使用 mongodump 进行备份时，使用的就是普通的全表扫描操作。如果要执行一致性备份，需要[配合 oplog 的备份](https://www.mongodb.com/docs/database-tools/mongodump/#std-option-mongodump.--oplog) 来完成。


## 4.2 核心数据结构
### 4.2.1 QuerySolution
QuerySolution 定义了“抽象化”的查询解决方案，内部由不同类型的 QuerySolutionNode 连接成“树型结构”。   

每个 QuerySolutionNode 定义了对应的操作类型。比如 IndexScanNode 表示了索引扫描操作，里面会包含索引信息，但是不包含具体执行的 startKey 和 endKey。

QuerySolution 会使用在 PlanCache 中。

### 4.2.2 PlanCache

1. 存储结构： 参考 [PlanCache](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/query/plan_cache.h#L443) 的定义，里面会使用一个 LRUKeyValue 结构缓存执行计划。

2. 生命周期管理： CacheEntryState 中给每条执行计划缓存制定了 3 中状态：不存在，未激活和激活。这意味着，在一个执行计划加入缓存之后，默认是未激活状态，只有后续[评估](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/query/plan_cache.cpp#L487)之后，才转变成激活状态。

3. 如何感知外部状态变化： 最常见的就是索引状态变化，比如增加和删除索引，需要[通知](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/catalog/collection_info_cache_impl.cpp#L239) PlanCache 进行对应的清理和更新。

### 4.2.3 PlanStage
[PlanStage](https://github.com/mongodb/mongo/blob/r4.2.5/src/mongo/db/exec/plan_stage.h#L106) 是执行计划的基本组成单元。Stage 可以执行数据访问，比如从表和索引读数据，也可以执行数据转换生成数据流，比如 AND,OR,SORT 等操作。

每个 Stage 有 0 个或者多个输入流，但是只有一个输出流。数据访问的 stage 是整个执行计划的“叶子节点”，数据转换的 stage 是“非叶子节点”。多种类型的 stage 按照规则连接在一起，组成“执行树”来完成查询任务。

外部通过调用 work() 接口来驱动 stage 执行任务，根节点会调用其子节点的 work() 接口来逐级驱动。每个 stage 的 work() 流程会执行特定的任务并返回结果。有些阻塞式的 stage，比如 AND、SORT 等，可能需要多次被调用 work() 累积到足够的输入数据之后才能向更上层节点输出结果。在不能立即输出结果时，会向上层返回 [NEED_TIME](https://github.com/mongodb/mongo/blob/r4.2.5/src/mongo/db/exec/plan_stage.h#L140)，告诉上层节点需要继续调用 work().

### 4.2.4 WorkingSet
每个查询的执行器会分配 1 个 WorkingSet 内存结构用于存储中间结果，并在自己的多个 PlanStage 中共享。

以一个常见的查询： db.coll.find({a:{$gt:100, $lt:200}}, {_id:0 , b:1, c:1}).skip(5).limit(5) 为例，会生成下图左侧的执行计划，并分配下图右侧的 WorkingSet 结构。

<p align="center">
  <img src="https://github.com/user-attachments/assets/6be9a763-81ed-46d0-9f42-3624dee5d736" width=800>
</p>

按照自顶向下的视角，[WorkingSet](https://github.com/mongodb/mongo/blob/r4.2.5/src/mongo/db/exec/working_set.h#L52) 中包含：    
1. _data: 存储中间结果数据，本质上是一个 vector, vector 中的每个槽位是一个 MemberHolder.
2. _freeList: 类型为 WorkingSetID(本质上是一个 size_t)，存储了 _data 中空闲的 MemberHolder。可以看做这是一个空闲链表的头部。
3. _yieldSensitiveIds：直译过来就是“对 yield 操作敏感的 WorkingSetID 数组”。具体来说，对应的是 indexScan 获取的中间结果。这些中间结果，可能会由于 yield 操作，导致的 snapshot 更新后，fetch 出来的文档已经不能匹配 indexScan 的查询条件。

[MemberHolder](https://github.com/mongodb/mongo/blob/r4.2.5/src/mongo/db/exec/working_set.h#L118) 中包含：    
1. nextFreeOrSelf: 类型为 WorkingSetID。如果WorkingSetMember中存储的是有效值，则 nextFreeOrSelf 存储自己在 WorkingSet::_data数组中的偏移。否则，nextFreeOrSelf 存储下一个空闲 Holder 的偏移，如果自己就是最后一个空闲 Holder，则设置为 -1.
2. [WorkingSetMember](https://github.com/mongodb/mongo/blob/r4.2.5/src/mongo/db/exec/working_set.h#L237): 存储具体的中间结果。包含的内容：    
	2.1 recordId: 数据在表中的 RecordId.    
	2.2 obj: 带 snapshot 版本信息的 BSONObj。    
	2.3 keyData: 类型为 vector<IndexKeyDatum>， 每个 IndexKeyDatum 包含了索引信息，对应的索引 Key 信息。    
	2.4 _state: 当前的状态，包含 INVALID、RID_AND_IDX（从索引获取的数据）、RID_AND_OBJ（从表获取的数据）、OWNED_OBJ（运算后的 BSONObj）    
	2.5 _computed: 存储运算后的中间结果，比如 sortKey、geo point、text score 等。    
	2.6 _isSuspicious: 直译为“可疑的”，和前面的 _yieldSensitiveIds 对应。一般用于 indexScan 的中间结果，在 yield 之前，会先将这个标志置位 true. 在 yield 重新调度回来时，会检查这个标记，将表中 fetch 的结果和索引 key 进行比对。    

对于 WorkingSet::_yieldSensitiveIds 和 WorkingSetMember::_isSuspicious，下面说明为啥这样设计。    
假设系统中有 2 个请求，依次执行如下流程：        
1. 请求 A 根据条件 {a:1} 去查询文档，在 indexScan 阶段得到中间结果 {a:1} -> recordId:1, 并将这个中间结果存储到 WorkingSet 中。然后请求 A 就 yield 出去了，并释放了 snapshot.    
2. 请求 B 是一个 update 操作，将 recordId:1 这条记录从 {a:1} 改成了 {a:2}.    
3. 请求 A yield 回来，获取了新的 snapshot，然后根据 recordId 去查表，读到 {a:2} 这条文档。    

可以看到，如果使用默认的“非一致性快照”读，可能会由于 yield 操作，导致 indexScan 获取的结果并不符合预期。    

因此，需要将 indexScan（idHack, distinct 等其他使用索引的 stage 类似） 得到的中间结果[记录在 WorkingSet::_yieldSensitiveIds](https://github.com/mongodb/mongo/blob/r4.2.5/src/mongo/db/exec/working_set.cpp#L96) 中，在执行器被 yield 出去之前，会遍历 WorkingSet::_yieldSensitiveIds 并将其中的 WorkingSetMember::_isSuspicious [设置为 true](https://github.com/mongodb/mongo/blob/r4.2.5/src/mongo/db/exec/working_set_common.cpp#L43).
在 yield 调度回来之后，在从表里面 fetch 数据后，如果设置了 isSuspicious 标记，会[再次检查](https://github.com/mongodb/mongo/blob/r4.2.5/src/mongo/db/exec/working_set_common.cpp#L76)读出来的文档是否和索引条件匹配。


# 5. 分片集群的执行计划

接下来讨论分片集群中，mongos 侧是如何执行的。

## 5.1 写请求

MongoDB 提供了功能强大的批量 insert/update/delete 接口，并提供了非常丰富的响应信息用于错误处理。    
下面结合 4.2.25 版本的内核代码，介绍mongos 是如何调度写请求的。

在介绍具体流程之前，先了解一下关键的数据结构：
1. [BatchedCommandRequest](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/s/write_ops/batched_command_request.h#L45): 存储了原始的写请求，并归一化成内核代码可以直接访问的数据结构。
2. [BatchWriteOp](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/s/write_ops/batch_write_op.h#L122): 按原始写请求的数组拆分成多个子请求（item），后续再根据每个子请求要发往的 shard 数量进行进一步拆分。经过这 2 次拆分后的子请求，能够明确表示要发往哪个 shard，去执行哪个命令，可以直接用于后续调度。MongoDB 使用了“数组下标引用”的设计，避免原始请求的内存拷贝（对于一个很大的插入请求，每次对原始请求的内存拷贝都是消耗很大的操作）。比如一个批量更新请求中，第 3 个请求要发往 shard1 和 shard2, 则发往 shard1 的子请求会记录为 {3,0}+endpoint(包含shardId和 shardversion 信息)，发往 shard2 的子请求会记录为 {3,1}+endpoint，其中第 1 个下标表示了在原始请求数组中的下标，而第 2 个下标则是区分不同的 shard .
3. [childBatches](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/s/write_ops/batch_write_exec.cpp#L147): 一个 std::map 临时结构，表示每一轮要转发的子请求。包含了要进行转发的 shardId，以及子请求的命令，shardVersion 等信息。


BatchWrite 的整体流程为（参考 [BatchWriteExec::executeBatch](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/s/write_ops/batch_write_exec.cpp#L102)）：
1. 根据原始请求 BatchedCommandRequest，生成对应的 BatchWriteOp 管理请求状态的生命周期。生成 BatchWriteOp 的过程中避免了内存拷贝，而是通过“数组下标引用”这种巧妙的方式，来建立和原始请求中每个子请求的对应关系。
2. 通过 BatchWriteOp::targetBatch 生成可以并行转发给 shard 的子请求 （childBatches）.
3. 通过网络模块给各个 shard 转发子请求，并在子请求中携带了 mongos 维护的 shardVersion 信息，用于路由版本校验。
4. 收集每个 shard 的响应，并进行必要的错误处理。比如在出现 StaleConfig路由版本较低的错误时，需要进行必要的路由刷新。
5. 跳转到第 2 步，重试或者生成下一轮要执行的 childBatches，然后继续发送流程。直到 BatchWriteOp 中的请求全部执行完毕，或者执行遇到不可重试的错误，或者连续 [5轮](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/s/write_ops/batch_write_exec.cpp#L98)没有实质性进展，则返回执行结果或者报错。

mongos 侧执行写请求的难点，主要集中在 `ordered` 顺序，以及子请求的路由处理。下面分开进行讨论。

**Ordered**    

`ordered` 参数指定了子请求是否严格按照顺序执行，默认为 true.    

假设现在分片集群有 2 个 shard， 有一个分片表 coll1, 分片建为 sk. 表中有几条文档，它们的 shardKey 和所属分片的对应关系为：
```
sk:3  --> shard1
sk:2  --> shard2
sk:1  --> shard2
```

假设现在有如下 `{ordered: true}` 的 Update 命令：
```
db.runCommand({update: "coll1",
	updates:[
		{q:{sk: 3}, u: {$set: {a:1}}, multi: true},
		{q:{sk: 2}, u: {$set: {a:2}}, multi: true},
		{q:{sk: 1}, u: {$set: {a:1}}, multi: true},
		{q:{}, u: {$set: {d:0}}, multi:true},
		{q:{sk:3}, u: {$set:{b:1}}, multi: true}
	],
	ordered: true})
```

其对应的转发流程会拆分为 4 轮，如下图所示：

<p align="center">
<img width="1121" height="541" alt="image" src="https://github.com/user-attachments/assets/a5d7547f-2aa6-414c-8e05-7a60e6703566" />
</p>

如果将请求设置为  `{ordered: false}`, 即：
```
db.runCommand({update: "coll1",
	updates:[
		{q:{sk: 3}, u: {$set: {a:1}}, multi: true},
		{q:{sk: 2}, u: {$set: {a:2}}, multi: true},
		{q:{sk: 1}, u: {$set: {a:1}}, multi: true},
		{q:{}, u: {$set: {d:0}}, multi:true},
		{q:{sk:3}, u: {$set:{b:1}}, multi: true}
	],
	ordered: false})
```

则对应的转发流程会拆分为 3 轮，如下所示：

<p align="center">
<img width="1121" height="541" alt="image" src="https://github.com/user-attachments/assets/8a3a0e05-bdeb-4f4d-8b34-3a424a454dd5" />
</p>

这里可能会好奇，为啥第 3 轮发往 shard1 的请求，没有和第 1 轮发往 shard1 的请求合并？    
在 mongos 中， 如果某个子请求是 `multi-shard`请求，且不处于分布式事务中，且与之前的子请求发往的 shard 有重叠，则会单独作为一个childBatch 执行（这种情况会降低整体调度的并发度）。
这么做的原因，是方便子请求的重试处理，正如[代码注释](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/s/write_ops/write_op.cpp#L98)中的描述：     
>Outside of a transaction, multiple endpoints currently imply no versioning, since we can't retry half a regular multi-write.
 
具体的做法，是将子请求对应的 [shardVersion 置为0](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/s/write_ops/write_op.cpp#L98-L103) ， 这样在 [isNewBatchRequiredUnordered](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/s/write_ops/batch_write_op.cpp#L116) 中进行判断时，发现与前面存在 shard 重叠的子请求的 shardVersion 不同，则不将这两者放在同一批次中调度。    


如果没有  `multi-shard`子请求，则可以非常方便的进行合并。比如下面这个请求：
```
db.runCommand({update: "coll1",
	updates:[
		{q:{sk: 3}, u: {$set: {a:1}}, multi: true},
		{q:{sk: 2}, u: {$set: {a:2}}, multi: true},
		{q:{sk: 1}, u: {$set: {a:1}}, multi: true},
		{q:{sk:3}, u: {$set:{b:1}}, multi: true}
	],
	ordered: false})
```
会进行合并后在 1 轮内搞定，如下所示：

<p align="center">
<img width="1121" height="521" alt="image" src="https://github.com/user-attachments/assets/28edeb8c-5451-47a5-b55d-ba1b6458ae5b" />
</p>

**子请求路由**    

以最复杂的 update 请求为例，其路由策略可以参考 [ChunkManagerTargeter::targetUpdate](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/s/write_ops/chunk_manager_targeter.cpp#L421)（同理， insert和 delete 请求的路由过程可以参考[targetInsert](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/s/write_ops/chunk_manager_targeter.cpp#L378) 和 [targetDelete](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/s/write_ops/chunk_manager_targeter.cpp#L521)），核心流程为：    
1. 根据 query 条件确定目标 shard 列表。和读请求相似，会根据 shardKey 的查询条件，根据 shardKey 索引去生成 indexBounds（mongos 上没有真正维护索引，只是模拟了一下 indexScan 生成执行计划的流程）, 然后根据路由表获取目标 shard 列表。
2. 除了上述常规的路由流程之外，对于 Update 还有一些快速路径：    
  a. 对于 {upsert: true} 类型的请求，请求中必然包含了 shardKey，因此可以提取 shardKey 之后直接去查路由表。    
  b. 对于 replacement 结构的请求，其 u 字段中包含了全量文档信息，因此必然也包含了 shardKey，因此可以用于对前面生成的目标 shard 列表进行裁剪（如果之前生成的目标 shard 数大于 1 个）。



## 5.2 读请求
假设分片集群有 2 个分片，创建如下表，写入文档后执行查询：
``` 
sh.shardCollection("db2.tt3",{a:"hashed"})
for (var i = 0; i < 1000; i++) {db.tt3.insert({a:i, b:i, c:i})}
db.tt3.find({a:{$gte:100, $lt:200}}, {a:1,b:1}).sort({c:1}).skip(10).limit(10)
```
则读操作的执行流程为：     
1. 生成要发送给每个 shard 节点的[子请求](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/s/query/cluster_find.cpp#L92)。
2. 将子请求发送给对应的 shard，并生成对应的 [remoteCursor](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/s/query/cluster_find.cpp#L287).
3. 结合 remoteCursor，生成 mongos 侧的[执行计划](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/s/query/cluster_find.cpp#L302)。

需要注意的是：    
1. cursorId = 0, 表示 cursor 没有剩余的数据了。
2. mongos 的 merge 操作，对于 sort 和 非sort 的处理行为是不一样的：
   1. [对于 sort](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/s/query/async_results_merger.cpp#L293) 来说，需要等所有的 remoteCursor 都准备好才能执行，如果有 remoteCursor 没有准备好（mongos 上没有该 remoteCursor 的缓存数据，而且对应的 shard 也没有通过设置 cursorId=0 来标识 exhuast 状态），则先通过 getmore 从对应的 shard 上获取数据（参考 参考 AsyncResultsMerger::_scheduleGetMores ->AsyncResultsMerger::_askForNextBatch）。为了完成归并排序，会创建一个 priority queue，根据 sortKey 的值进行比较，完成归并排序。
   2. [对于非 sort](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/s/query/async_results_merger.cpp#L325) 来说，由于不需要归并排序，只需要任意一个 shard 的 remoteCursor 准备好即可读取并向上层返回数据。
3. CursorManager：有一个全局的 [ClusterCursorManager](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/s/query/cluster_cursor_manager.h#L72)， 里面会使用 map 缓存 cursorId 和对应的 ClusterClientCursorImpl，对于一个查询请求来说 :
   1. 如果没有后续数据，则返回 cursorId = 0.
   2. 如果存在后续数据，则将 ClusterClientCursorImpl 注册到全局的 ClusterCursorManager 中，并返回对应的 cursorId. 客户端可以使用这个 cursorId 发起 getmore 请求得到后续的数据。

最终生成的执行计划如下图所示：

<p align="center">
<img width="682" height="1071" alt="image" src="https://github.com/user-attachments/assets/a712757f-6cc9-4ba4-8476-268a22c2f121" />
</p>

## 5.3 聚合请求
MongoDB 的 aggregate 请求功能非常强大。既可以使用丰富的算子并结合“管道式”编程完成非常强大的分析操作，也能结合 `$merge`、`$out` 等完成数据的导出，也能结合 `$replceRoot`等操作或者和 update 语句融合完成一些比较复杂的数据更行功能。    

在分片集群场景中，会根据表分布情况、以及聚合算子的具体特性，决定不同的执行策略：
- 完全下推到 shard 上执行，比如非分片表场景。    
- 完全在 mongos 上执行， 比如 `$listLocalSesions`.   
- 部分在 mongos 上执行，部分下推到 shard 执行。然后根据情况在 mongos 或者 primary shard 上汇聚中间结果。     

下面通过一个常用的 group 聚合操作，来说明分片表的聚合执行流程。    
假设一个 2 分片的表中，存放了如下的人员信息(使用 `_id` 进行分片，包含了人员的城市、年龄信息)：
```
{ "_id" : ObjectId("691c1091f4ae1a2d300bc259"), "city" : "beijing", "age" : 16 }
{ "_id" : ObjectId("691c1094f4ae1a2d300bc25a"), "city" : "beijing", "age" : 18 }
{ "_id" : ObjectId("691c1096f4ae1a2d300bc25b"), "city" : "beijing", "age" : 28 }
{ "_id" : ObjectId("691c1098f4ae1a2d300bc25c"), "city" : "beijing", "age" : 38 }
{ "_id" : ObjectId("691c109af4ae1a2d300bc25d"), "city" : "beijing", "age" : 58 }
{ "_id" : ObjectId("691c10a2f4ae1a2d300bc25e"), "city" : "shanghai", "age" : 58 }
{ "_id" : ObjectId("691c10a4f4ae1a2d300bc25f"), "city" : "shanghai", "age" : 68 }
{ "_id" : ObjectId("691c10a6f4ae1a2d300bc260"), "city" : "shanghai", "age" : 48 }
{ "_id" : ObjectId("691c10b0f4ae1a2d300bc261"), "city" : "guangzhou", "age" : 48 }
{ "_id" : ObjectId("691c10b2f4ae1a2d300bc262"), "city" : "guangzhou", "age" : 78 }
```
现在需要统计每个城市的年龄在 18-60 周岁的适龄人数，并列举人数超过 2 的城市信息，并按人口数进行升序排序。 使用的 aggregate 语句为：
```
> db.coll3.aggregate([{$match:{age:{$gte:18, $lte:60}}}, {$group: {_id: "$city", totalCount: {$sum:1}}}, {$match: {totalCount: {$gte:2}}}, {$sort: {totalCount:1}}])

{ "_id" : "shanghai", "totalCount" : 2 }
{ "_id" : "beijing", "totalCount" : 4 }
```
类似如下 SQL 语句：
```
SELECT city, COUNT(*) AS totalCount
FROM coll3
WHERE age>=18 AND age<=60
GROUP BY city
HAVING COUNT(*)>=2 
ORDER BY totalCount;
```

命令发送到 mongos 之后，mongos 会经过如下几个步骤完成聚合操作：    
1. **命令接收与初始处理**, 首先由[ClusterAggregate::runAggregate](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/s/query/cluster_aggregate.cpp#L811)方法处理。步骤包括：参数验证、获取路由信息、检测是否为分片集合、根据不同情况处理（直接传递到特定分片、在mongos本地执行或传递到主分片、分发到多个分片）。
2. 管道拆分。对于分片集合，mongos 需要将聚合管道拆分为两部分：    
   - **分片管道**：在各个分片上执行的部分。    
   - **合并管道**：在mongos或特定分片上执行的部分，用于合并各分片的结果。    
	其中一个很重要的步骤，就是遍历聚合管道中的每个 stage，找到一个合适的点进行拆分。通常，在 `$group` 阶段之后是一个比较好的选择，因为在此之前的数据可以在各个分片上并行处理，而之后的聚合操作则需要合并结果。具体执行逻辑可以参考 [findSplitPoint](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/s/query/cluster_aggregation_planner.cpp#L72).
3. 管道分发。拆分完成后，mongos需要将分片管道分发到各个分片上执行。这一过程由[sharded_agg_helpers::dispatchShardPipeline](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/pipeline/sharded_agg_helpers.cpp#L237)函数实现。
4. 合并策略选择。根据管道特性，mongos会选择不同的合并策略：    
	- **主分片合并**：在主分片上执行合并操作，比如 `$merge`到非分片集合的场景。
   	- **mongos合并**：在mongos上执行合并操作，常见场景，比如本例就是这种场景。
   	- **exchange优化**：使用$exchange操作符在分片间重新分区数据，比如 `$merge` 到分片集合的场景。  
5. 结果合并。在mongos上执行合并时，会创建`$mergeCursors`阶段来合并各分片的结果。参考 [addMergeCursorsSource](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/s/query/cluster_aggregation_planner.cpp#L484).

对于本例来说，会在 `$group`处进行拆分，包括 shard 上的 `$group`上半部，以及 mongos 上的 `$group`下半部（merge 阶段），最后生成的执行计划如下所示：

<p align="center">
<img width="1041" height="446" alt="image" src="https://github.com/user-attachments/assets/4779142d-59fd-45dd-9578-04d7fd249054" />
</p>

# 6. 索引
和其他数据库一样，索引在 MongoDB 的执行引擎中起着举足轻重的作用。    
下面，对于 MongoDB 中索引的类型，属性和使用经验准则进行讨论。
## 6.1 索引类型
### 普通 Btree索引
普通的 Btree 索引可以有 1个或者多个字段，定义索引时字段的顺序非常重要。因为在索引的使用上遵循最左前缀匹配原则。    
关于普通 Btree 索引的内核实现方式，可以参考前面 1.2 章节对于 KeyString 的讲述。

### Multikey 索引

### Wildcard 索引

### Geo 索引

### Hashed 索引
MongoDB 的 Hashed 索引，会用 btree 结构对 key 的 hash 值进行索引。     
和其他很多数据库不一样（比如 PostgreSQL 的 Hash 索引），**MongoDB 并不会使用 hash 表存储索引**，而是用 btree.

对于一个很长的字段，比如一个 1000 个字符的字符串，如果直接使用该字段作为索引 key，则索引非常大。如果使用 Hash 索引，由于使用的是 8 字节的 Hash 值创建索引，因此索引大小很大大降低。关于 Hash 索引的空间优势，可以参考 6.2 章节的讨论。    

## 6.2 索引属性
### Collation(Case-Insensitive)
MongoDB 内核使用 ICU(International-Component-for-Unicode) 库提供了对字符的国际化支持。    
比如常见的使用方式，就是在查询的时候忽略大小写和音标等。    

一个典型的例子如下：
```
插入数据：
db.coll3.insertMany([{a:"apple"}, {a:"Apple"}, {a:"APPLE"}, {a:"banana"}])

创建索引：
db.coll3.createIndex({a:1},{collation:{locale:"en", strength:2}})

读取数据：
db.coll3.find({a:"apple"}).collation({locale:'en', strength:2})
能够查询 "apple","Apple" 和 "APPLE".
```

如果定义表时指定了 [collation](https://www.mongodb.com/docs/manual/reference/collation/#std-label-collation-document-fields)，则该表的索引和查询默认会继承该 collation.

collation 使用标准的 icu 库进行索引 key 的转换， 参考 CollatorInterfaceICU::getComparisonKey.


### Hidden


### Partial

### Sparse

### TTL

### Unique
用户可以创建 unique 索引进行字段的唯一性约束，当尝试插入相同 key 的文档时，会报 "duplicate key" 错误。     

很多用户习惯使用 upsert 操作来处理 "duplicate key" 报错。但是如果没有创建唯一索引，可能由于并发操作导致出现重复 key 的情况。    
例如：
```
# session1 执行 upsert
mymongo:PRIMARY> s = db.getMongo().startSession()
session { "id" : UUID("a7bc3ed7-cb8c-4db1-950b-2f82d64ec7fc") }
mymongo:PRIMARY> s.startTransaction()
mymongo:PRIMARY> coll5=s.getDatabase("db1").coll5
db1.coll5
mymongo:PRIMARY> coll5.update({a:3},{$set:{b:1}},{upsert:true})
WriteResult({
        "nMatched" : 0,
        "nUpserted" : 1,
        "nModified" : 0,
        "_id" : ObjectId("692c3f349b543feb807d310b")
})

# session2 执行同样的 upsert
mymongo:PRIMARY> s2 = db.getMongo().startSession()
session { "id" : UUID("3aedec76-13f7-4482-9ab1-2ca7fa64bf52") }
mymongo:PRIMARY> s2.startTransaction()
mymongo:PRIMARY> coll5=s2.getDatabase("db1").coll5
db1.coll5
mymongo:PRIMARY> coll5.update({a:3},{$set:{b:1}},{upsert:true})
WriteResult({
        "nMatched" : 0,
        "nUpserted" : 1,
        "nModified" : 0,
        "_id" : ObjectId("692c3f329b543feb807d3107")
})

# session1 提交事务
mymongo:PRIMARY> s.commitTransaction()
# session2 提交事务
mymongo:PRIMARY> s2.commitTransaction()

# 查表，发现有两个相同的 key ！！
mymongo:PRIMARY> db.coll5.find({a:3})
{ "_id" : ObjectId("692c3f329b543feb807d3107"), "a" : 3, "b" : 1 }
{ "_id" : ObjectId("692c3f349b543feb807d310b"), "a" : 3, "b" : 1 }
```
究其原因，是数据库中没有唯一索引，所以 upsert 能不能预期执行，全凭运气（是否有相同 key 并发 upsert）。    
**所以，如果要使用 upsert 操作，切记看看有没有创建 unique 索引。**

相对而言，PostgreSQL 在执行 `insert into ... on conflict do update` 时，会先检查是否有唯一索引，如果没有则会报错。个人认为这种直接报错的行为，要更加严谨一点。    
例如：
```
postgres=# create table t1 (c1 int, c2 int);
CREATE TABLE
postgres=# insert into t1 values (1,1) on conflict(c1,c2) do update set c2=3;
ERROR:  there is no unique or exclusion constraint matching the ON CONFLICT specification
```


## 6.3 使用准则
### 最左前缀匹配

### ESR

### 选择区分度高的字段，建索引

### 覆盖索引

### 索引条件下推 ICP

### 删除不使用的索引



# 参考文档
1. 数据库内核杂谈（七）：数据库优化器（上）：https://www.infoq.cn/article/GhhQlV10HWLFQjTTxRtA
2. 数据库内核杂谈（八）：数据库优化器（下）：https://www.infoq.cn/article/JCJyMrGDQHl8osMFQ7ZR
3. 数据库内核杂谈（九）：开源优化器 ORCA：https://www.infoq.cn/article/5o16eHOZ5zk6FzPSJpT2
4. SQL 查询优化原理与 Volcano Optimizer 介绍：https://zhuanlan.zhihu.com/p/48735419
5. Cascades Optimizer：https://zhuanlan.zhihu.com/p/73545345
6. MongoDB 内核代码：https://github.com/mongodb/mongo/tree/r4.2.25
7. 官方文档： https://www.mongodb.com/docs/manual/reference/collation/#std-label-collation-document-fields



