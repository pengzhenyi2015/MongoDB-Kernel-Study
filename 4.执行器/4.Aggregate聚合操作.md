# 1. 导语    
在不少开发者看来，MongoDB 更像是具备二级索引能力的 KV 存储系统。     
但是 Aggregate 的出现，打破了这一“成见”，其丰富的 Stage 和 Operator，使得 MongoDB 的数据能力直接上了一个新台阶。     

和关系型数据库中的 GROUP/JOIN/WITH 等语法不同，MongoDB 中的 Aggregate 操作使用了全新的"管道式"语法，类似如下：
```
[ {stage1}, {stage2}, ... ,{stageN}]
```
整体上，呈现一种类似 Shell 命令中的管道式命令，或者 pipeline 的格式。从 Aggregate 命令，可以很直观的看出具体的执行计划（真实场景下，优化器会进行一些调整）。     

每个 Stage 完成特定的功能，里面可以通过组织不同的 operator 来实现具体的计算任务。比如在 `$group` 阶段，就可以通过 `$sum` 等 operator 来实现对数据的聚合计算：     
```
{
    $group: {
        _id:"$a", 
        totalCount: {
            $sum: 1
        }
    }
}
```

截至目前，MongoDB 开源版本已经支持了约 40 个 [Stage](https://www.mongodb.com/zh-cn/docs/manual/reference/mql/aggregation-stages/) 和 200+ [Operator](https://www.mongodb.com/zh-cn/docs/manual/reference/mql/query-predicates/)。这些 Stage 包括了数据的读取、过滤、转换、聚合、排序等操作，而 Operator 则提供了丰富的计算功能，如算术运算、字符串操作、日期处理、数组操作、位运算等。      

接下来，本文将分两部分进行介绍：    
1. 常用的 Stage，并列举对应的 SQL 语法（PostgreSQL 版本）方便理解。    
2. 从原理上，深入分析常用的 Stage 实现。    

# 2. 初探 Stages
下面对一些关键的 Stages进行介绍。    

## $addFields
为文档添加新字段。与 $project 类似，$addFields 重塑了流中的每个文档；具体来说，就是在输出文档中添加新字段，这些输出文档既包含输入文档中的现有字段，也包含新添加的字段。    
$set 是 $addFields 的别名。

增加一个字段 sum，统计所有字段的和：
```
db.testcoll.aggregate([{$addFields:{sum:{$add:["$a", "$b"]}}}])
```
单纯的增加一个字段，内容是 {"test" : "abcd"}:
```
db.testcoll.aggregate([{$addFields:{"test":"abcd"}}])
```

**PostgreSQL 语法**    
在 SELECT 中增加一个表达式，并使用  **AS** 重命名：    
```
SELECT *, a+b AS sum FROM testcoll;
```

## $bucket    
根据指定的表达式和存储桶边界将传入的文档分为多个组（称为存储桶）。       
按 "year_born" 字段进行分组，并指定分组边界。如果不在分组边界内的，则归类为 other:    
```
db.artists.aggregate( [
  {
    $bucket: {
      groupBy: "$year_born",                        // Field to group by
      boundaries: [ 1840, 1850, 1860, 1870, 1880 ], // Boundaries for the buckets
      default: "Other",                             // Bucket ID for documents which do not fall into a bucket
      output: {                                     // Output for each bucket
        "count": { $sum: 1 },
        "artists" :
          {
            $push: {
              "name": { $concat: [ "$first_name", " ", "$last_name"] },
              "year_born": "$year_born"
            }
          }
      }
    }
  }
] )
```
简化版本：    
```
db.artists.aggregate( [
  {
    $bucket: {
      groupBy: "$year_born",                        // Field to group by
      boundaries: [ 1840, 1850, 1860, 1870, 1880 ], // Boundaries for the buckets
      default: "Other",                             // Bucket ID for documents which do not fall into a bucket
      output: {                                     // Output for each bucket
        "count": { $sum: 1 },
      }
    }
  }
] )
```

输出内容会包含 _id 字段：如果数据处于指定的分组中，则对应分组的下边界。    
如果不处于分组中，则 _id 的值是指定的 default 字段，本例子中是 "_id":"Other".    

**PostgreSQL 语法**      
```
SELECT
  CASE
    WHEN year_born >= 1840 AND year_born < 1850 THEN '1840-1850'
    WHEN year_born >= 1850 AND year_born < 1860 THEN '1850-1860'
    WHEN year_born >= 1860 AND year_born < 1870 THEN '1860-1870'
    WHEN year_born >= 1870 AND year_born < 1880 THEN '1870-1880'
    ELSE 'Other'
  END AS bucket,
  COUNT(*) AS count
FROM artists
GROUP BY bucket
ORDER BY bucket;
```

## $bucketAuto    
根据指定的表达式，将接收到的文档归类到特定数量的群组中（称为“存储桶”）。自动确定存储桶边界，以尝试将文档均匀地分配到指定数量的存储桶中。    

指定分桶字段或者表达式，并执行分桶的数量，进行自动分桶（自动划分边界，并保证每个桶的数量相同或者相近）。    
按 "price"字段划分 4 个桶：     
```
db.artwork.aggregate( [
   {
     $bucketAuto: {
         groupBy: "$price",
         buckets: 4
     }
   }
] )
```
按 "year" 字段划分3 个桶，并指定输出内容：    
```
db.artwork.aggregate( [
   {
     $bucketAuto: {
         groupBy: "$year",
         buckets: 3,
         output: {
           "count": {$sum:1},
           "years": {$push: "$year"}
         }
     }
   }
] )
```
使用表达式进行分组：    
```
db.artwork.aggregate( [
   {
     $bucketAuto: {
        groupBy: {
              $multiply: [ "$dimensions.height", "$dimensions.width" ]
            },
            buckets: 4,
            output: {
              "count": { $sum: 1 },
              "titles": { $push: "$title" }
            }
     }
   }
] )
```
输出的 _id 字段是一个 object,里面包含了 min 表达的下边界，和 max 表达的 上边界。    

**PostgreSQL 语法**      
PostgreSQL 里用 NTILE 窗口函数即可一次性实现同样的“等频分桶”：    
```
WITH tiled AS (
  SELECT
    NTILE(4) OVER (ORDER BY price) AS bucket_no,
    price
  FROM artwork
  WHERE price IS NOT NULL
)
SELECT
  bucket_no,
  MIN(price) AS bucket_min,
  MAX(price) AS bucket_max,
  COUNT(*)   AS count
FROM tiled
GROUP BY bucket_no
ORDER BY bucket_no;
```
输出的范围边界有些不同。    
MongoDB会输出连续边界：    
- `[min1, max1)    `
- `[min2, max2)    `

其中 max1 = min2.    

而 PostgreSQL 采用这种写法，边界是不连续的：    
- `[min1, max1]    `
- `[min2, max2]    `

其中 max1 < min2.    

## $changeStream
返回集合的 Change Stream 游标。此阶段只能在 aggregation pipeline 中发生一次，并且必须作为第一阶段。    
监听 artwork 表的变更：   
```
var cur = db.artwork.aggregate( [
   { $changeStream: {} }
] )
```

在另外一个终端插入一条数据：
```
db.artwork.insert({a:1})
```

在之前的终端获得更改：
```
cur.next()
```

## $count
统计个数：    
```
db.artwork.aggregate([
  {
    $count: "totalNumber"
  }
])
```

**PostgreSQL 语法**      
```
SELECT count(*) AS totalNumber from artwork;
```

## $facet    
在单个阶段内处理同一组输入文档上的多个聚合管道。支持创建多分面聚合，能够在单个阶段中跨多个维度或分面描述数据特征。    

在一个聚合操作中，输出多个维度的结果。比如在一个聚合请求中列出排序后的标签，列出按 price分桶后的 titles, 列出按 "year" 分桶后的统计（ **1 次 facet，输出 3 种结果**）：
```
db.artwork.aggregate( [
  {
    $facet: {
      "categorizedByTags": [
        { $unwind: "$tags" },
        { $sortByCount: "$tags" }
      ],
      "categorizedByPrice": [
        // Filter out documents without a price e.g., _id: 7
        { $match: { price: { $exists: 1 } } },
        {
          $bucket: {
            groupBy: "$price",
            boundaries: [  0, 150, 200, 300, 400 ],
            default: "Other",
            output: {
              "count": { $sum: 1 },
              "titles": { $push: "$title" }
            }
          }
        }
      ],
      "categorizedByYears(Auto)": [
        {
          $bucketAuto: {
            groupBy: "$year",
            buckets: 4
          }
        }
      ]
    }
  }
])
```

**PostgreSQL 语法**      

使用 With 语法，分别进行计算。然后再用 jsonb 操作符拼接后返回：
```
WITH
/* -------- 1. categorizedByTags : unwind + sortByCount -------- */
tag_counts AS (
  SELECT tag, COUNT(*) AS cnt
  FROM artwork
  CROSS JOIN LATERAL unnest(tags) AS t(tag)
  GROUP BY tag
),
categorizedByTags AS (
  SELECT jsonb_agg(
           jsonb_build_object('tag', tag, 'count', cnt)
           ORDER BY cnt DESC, tag
         ) AS val
  FROM tag_counts
),

/* -------- 2. categorizedByPrice : match + bucket -------- */
price_bucket AS (
  SELECT
    CASE
      WHEN price >= 0   AND price < 150 THEN 0
      WHEN price >= 150 AND price < 200 THEN 150
      WHEN price >= 200 AND price < 300 THEN 200
      WHEN price >= 300 AND price < 400 THEN 300
      ELSE -1          -- -1 代表 Other
    END AS bucket_id,
    title
  FROM artwork
  WHERE price IS NOT NULL
),
price_agg AS (
  SELECT
    CASE WHEN bucket_id = -1 THEN 'Other'
         ELSE format('[%s,%s)', bucket_id, bucket_id+50)
    END AS bucket_range,
    COUNT(*) AS count,
    jsonb_agg(title ORDER BY title) AS titles
  FROM price_bucket
  GROUP BY bucket_id
  ORDER BY bucket_id
),
categorizedByPrice AS (
  SELECT jsonb_agg(
           jsonb_build_object('bucket', bucket_range,
                              'count', count,
                              'titles', titles)
         ) AS val
  FROM price_agg
),

/* -------- 3. categorizedByYears(Auto) : bucketAuto 4 桶 -------- */
year_ntile AS (
  SELECT year, NTILE(4) OVER (ORDER BY year) AS bucket_no
  FROM artwork
  WHERE year IS NOT NULL
),
year_agg AS (
  SELECT
    bucket_no,
    MIN(year) AS year_min,
    MAX(year) AS year_max,
    COUNT(*)  AS count
  FROM year_ntile
  GROUP BY bucket_no
  ORDER BY bucket_no
),
categorizedByYearsAuto AS (
  SELECT jsonb_agg(
           jsonb_build_object('bucket', bucket_no,
                              'year_min', year_min,
                              'year_max', year_max,
                              'count', count)
         ) AS val
  FROM year_agg
)

/* -------- 4. 把三个 facet 拼成一条 JSON 返回 -------- */
SELECT jsonb_build_object(
         'categorizedByTags',      coalesce((SELECT val FROM categorizedByTags), '[]'::jsonb),
         'categorizedByPrice',     coalesce((SELECT val FROM categorizedByPrice), '[]'::jsonb),
         'categorizedByYears(Auto)', coalesce((SELECT val FROM categorizedByYearsAuto), '[]'::jsonb)
       ) AS facet_result;
```

## $fill    
填充文档中的 null 和缺失的字段值。    
可以填充固定值，可以填充排序后的线性值，可以填充排序后上一个观察到的值。    
比如填充固定值，将没有销售记录的字段设置为默认值 0：    
```
db.dailySales.aggregate( [
   {
      $fill:
         {
            output:
               {
                  "bootsSold": { value: 0 },
                  "sandalsSold": { value: 0 },
                  "sneakersSold": { value: 0 }
               }
         }
   }
] )
```

**PostgreSQL 语法**      

```
UPDATE dailySales
SET bootsSold   = COALESCE(bootsSold, 0),
    sandalsSold = COALESCE(sandalsSold, 0),
    sneakersSold= COALESCE(sneakersSold, 0);
```

## $geoNear
根据与地理空间点的接近程度返回有序的文档流。针对地理空间数据，整合了 $match、$sort 和 $limit 功能。输出文档包含一个额外的距离字段，并可包含一个位置标识符字段。    

查找距离中心最多 2 米且 category 等于 Parks 的文档：    
```
db.places.aggregate([
   {
     $geoNear: {
        near: { type: "Point", coordinates: [ -73.99279 , 40.719296 ] },
        distanceField: "dist.calculated",
        maxDistance: 2,
        query: { category: "Parks" },
        includeLocs: "dist.location",
        spherical: true
     }
   }
])
```

**PostgreSQL 语法**      
使用 PostGIS 插件提供的 GIST 索引，进行球面计算。和 2dsphere 索引类似。    
```
SELECT
  id,
  name,
  category,
  ST_X(location::geometry) AS lon,
  ST_Y(location::geometry) AS lat,
  ST_DistanceSphere(
        location,
        ST_MakePoint(-73.99279, 40.719296)
      ) AS dist_calculated,                    -- distanceField : "dist.calculated"
  ST_AsGeoJSON(location)::jsonb AS dist_location  -- includeLocs : "dist.location"
FROM places
WHERE category = 'Parks'                              -- query
  AND ST_DWithin(
        location::geography,
        ST_MakePoint(-73.99279, 40.719296)::geography,
        2000,                                         -- maxDistance 2 km = 2000 m
        true                                          -- use_spheroid=true 球面精确
      )
ORDER BY dist_calculated;                            -- 默认按距离升序
```

## $graphLookup
对集合执行递归搜索。为每个输出文档添加一个新数组字段，其中包含该文档的递归搜索遍历结果。    
进行深度遍历，实现图数据库的功能。    
适合进行社交网络遍历，路径遍历等。    

递归匹配 employees 集合中的 reportsTo 和 name 字段，返回每个人员的报告层次结构：    
```
db.employees.aggregate( [
   {
      $graphLookup: {
         from: "employees",
         startWith: "$reportsTo",
         connectFromField: "reportsTo",
         connectToField: "name",
         as: "reportingHierarchy"
      }
   }
] )
```

**PostgreSQL 语法**      

使用 WITH RECURSIVE:    
```
WITH RECURSIVE chain AS (
  /* 1. 基础：所有员工起点 */
  SELECT
    name AS start_name,
    reportsTo AS parent
  FROM employees
),
rec AS (
  /* 2. 递归：向上追老板 */
  SELECT
    start_name,
    parent,
    1 AS lvl,
    ARRAY[name] AS visited   -- 防循环
  FROM chain
  WHERE parent IS NOT NULL

  UNION ALL

  SELECT
    r.start_name,
    e.reportsTo,
    r.lvl + 1,
    r.visited || e.name
  FROM rec r
  JOIN employees e ON e.name = r.parent
  WHERE e.reportsTo IS NOT NULL
    AND e.name <> ALL(r.visited)   -- 避免环
)
SELECT
  e.*,
  /* 3. 把整条链聚成 JSONB 数组，顺序从顶到底 */
  (
    SELECT jsonb_agg(
             jsonb_build_object('name', e2.name, 'title', e2.title)
             ORDER BY lvl DESC
           )
    FROM rec r2
    JOIN employees e2 ON e2.name = r2.parent
    WHERE r2.start_name = e.name
  ) AS reportingHierarchy
FROM employees e
ORDER BY e.name;
```

## $group
按指定的标识符表达式对输入文档进行分组，并将累加器表达式（如果指定）应用于每个群组。接收所有输入文档，并为每个不同群组输出一个文档。输出文档仅包含标识符字段和累积字段（如果指定）。    
执行 group 操作，通过 _id 指定要 group 的字段。如果 _id 指定为 null 或者任何常量，则聚合所有输入文档。    
比如 group 计算，然后 having($match)：    
```
db.sales.aggregate(
  [
    // First Stage
    {
      $group :
        {
          _id : "$item",
          totalSaleAmount: { $sum: { $multiply: [ "$price", "$quantity" ] } }
        }
     },
     // Second Stage
     {
       $match: { "totalSaleAmount": { $gte: 100 } }
     }
   ]
 )
```

**PostgreSQL 语法**      

GROUP BY + HAVING：
```
SELECT item                          AS _id,
       SUM(price * quantity)         AS totalSaleAmount
FROM   sales
GROUP  BY item
HAVING SUM(price * quantity) >= 100;
```

## $lookup
对另一个集合执行左外连接。    
本地表 orders, 本地字段 item， 外表 invenroty, 字段 sku，进行左外链接的等值查询：    
```
db.orders.aggregate( [
   {
     $lookup:
       {
         from: "inventory",
         localField: "item",
         foreignField: "sku",
         as: "inventory_docs"
       }
  }
] )
```

匹配上的外表文档，会以数组的形式，放在 "as" 执行的字段中。    
如果没有匹配的，则是空数组。    

**PostgreSQL 语法**      

子查询（这里使用 ARRAY_AGG只是为了突出 lookup 的输出格式，真实场景下，ARRAY_AGG 要指定具体的名字, 不能是整行）：
```
SELECT *, (
   SELECT ARRAY_AGG(*)
   FROM inventory
   WHERE sku = orders.item
) AS inventory_docs
FROM orders;
```

或者 left join:
```
SELECT o.*,
       i.*                                  -- 或具体列
FROM   orders      AS o
LEFT JOIN inventory AS i
       ON i.sku = o.item;                  -- 对应 localField/foreignField
```

需要注意的是，MongoDB 只支持 left outer join，不过使用 `$unwind` 进行数组拆分之后，会自动过滤掉空数组，实现类似 inner join 的效果。    
另外，MongoDB 只支持 nested loop，在某些场景下，可能性能不太行。而 PostgreSQL 支持非常全面的 nested loop, hash， merge 等 join 算法。     

除了上述基础用法之外，`$lookup` 还支持使用 let 和 pipeline 参数进行更加复杂的查询。

## $match $project $set $skip $sort $unset $limit
基础的过滤、投影、排序等操作。对应 SQL 中的 WHERE、OFFSET、ORDER BY 、LIMIT 等操作。


## $merge
将 aggregation pipeline 的结果文档写入集合。该阶段可将结果（插入新文档、合并文档、替换文档、保留现有文档、操作失败、使用自定义更新管道处理文档）纳入输出集合。要使用 $merge 阶段，它必须是管道中的最后一个阶段。    

将 coll1 中 a>=1 and a <=10 的文档合入到 newcoll1 中（可以相同 db，也可以不同的 db）：    
```
db.coll1.aggregate([{$match:{a:{$gte:1, $lte:10}}}, {$merge: {into: "newcoll1", on: "_id", whenMatched: "replace", whenNotMatched:"insert"}}])
```

on: 指定一个字段，用于去重操作。    
whenMatched: 如果有重复，则 "replace"， “keepExisting”, "merge"（默认）, "fail".    
whenNotMatched: 如果没有重复，则"insert"(默认), "discard", "fail".    

>注：这里的 fail， 只是说这一个文档的 merge fail，不是说整个操作全部失败回滚。其他文档还是能正常 merge 的。

**PostgreSQL 语法**      

过滤 +  upsert：
```
INSERT INTO newcoll1
SELECT *
FROM   coll1
WHERE  a BETWEEN 1 AND 10
ON CONFLICT (_id) DO UPDATE              -- whenMatched: replace
SET    column1 = EXCLUDED.column1,      -- 把需要覆盖的列逐一写出
       column2 = EXCLUDED.column2,
       ... ;
```

## $out    
将 aggregation pipeline 的结果文档写入集合。要使用 $out 阶段，它必须是管道中的最后一个阶段。    

将 coll1 中 a>=1 and a <=10 的文档合入到 newcoll2 中（可以相同 db，也可以不同的 db）：    
```
db.coll1.aggregate([{$match:{a:{$gte:1, $lte:10}}}, {$out: "newcoll2"}])
```

**PostgreSQL 语法**      

```
DROP TABLE IF EXISTS newcoll2;          -- 先清空/重建（与 $out 行为一致）

CREATE TABLE newcoll2 AS
SELECT *
FROM   coll1
WHERE  a BETWEEN 1 AND 10;              -- $match: {a:{$gte:1,$lte:10}}
```

## $replaceRoot    
用指定的嵌入文档替换文档。该操作会替换输入文档中的所有现有字段，包括 _id 字段。指定嵌入在输入文档中的文档，以将嵌入的文档提升到顶层。    

$replaceWith 是 $replaceRoot 阶段的别名。   

将 "d" 字段替换整个文档，如果没有 “d” 字段，则用 {c:0 } 代替：
```
db.coll1.aggregate([
  {
    $replaceRoot:{
       newRoot: {
         $mergeObjects:[
           {c:0}, 
           "$d"
         ]}
       }
    }
])
```
或者过滤掉 d 字段不存在或者不是 object 类型的文档，然后用 d 字段替换这个文档：
```
db.coll1.aggregate([
  {
    $match: {
       d: {$exists: true, $not: {$type: "array"}, $type: "object"}   
  }},
  {
    $replaceRoot:{
       newRoot: "$d"
       }
    }
])
```

## $sample
从其输入中随机选择指定数量的文件。    

采样 2 条数据：
```
db.coll1.aggregate([{$sample: {size: 2}}])
```

**PostgreSQL 语法**      

```
SELECT *
FROM artwork
TABLESAMPLE BERNOULLI (50)   -- 50 % 的行
LIMIT 2;  -- 严格上限
```

## $setWindowFields
将文档分组到窗口中，并将一个或多个操作符应用于每个窗口中的文档。    
版本 5.0 中的新增功能。    

使用 $setWindowFields 中的文档窗口输出 orderDate 中每个 $year 的累积和最大蛋糕销售 quantity 值：
```
db.cakeSales.aggregate( [
   {
      $setWindowFields: {
         partitionBy: { $year: "$orderDate" },
         sortBy: { orderDate: 1 },
         output: {
            cumulativeQuantityForYear: {
               $sum: "$quantity",
               window: {
                  documents: [ "unbounded", "current" ]
               }
            },
            maximumQuantityForYear: {
               $max: "$quantity",
               window: {
                  documents: [ "unbounded", "unbounded" ]
               }
            }
         }
      }
   }
] )
```
指定时间范围，返回最近 10 个月的订单情况：
```
db.cakeSales.aggregate( [
   {
      $setWindowFields: {
         partitionBy: "$state",
         sortBy: { orderDate: 1 },
         output: {
            recentOrders: {
               $push: "$orderDate",
               window: {
                  range: [ "unbounded", 10 ],
                  unit: "month"
               }
            }
         }
      }
   }
] )
```

**PostgreSQL 语法**      

```
SELECT
    *,
    /* 1. 从年初到当前行的累计数量 */
    sum(quantity) OVER (
        PARTITION BY date_trunc('year', orderDate)
        ORDER BY orderDate
        ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW
    ) AS cumulativeQuantityForYear,

    /* 2. 当前年份全局最大数量 */
    max(quantity) OVER (
        PARTITION BY date_trunc('year', orderDate)
    ) AS maximumQuantityForYear
FROM cakeSales
ORDER BY orderDate;
```

## $unionWith
执行两个集合的联合；即将两个集合的管道结果合并到一个结果集中。    

合并 2017年 - 2020 年共 4 年的销售数据，并排序：
```
db.sales_2017.aggregate( [
   { $set: { _id: "2017" } },
   { $unionWith: { coll: "sales_2018", pipeline: [ { $set: { _id: "2018" } } ] } },
   { $unionWith: { coll: "sales_2019", pipeline: [ { $set: { _id: "2019" } } ] } },
   { $unionWith: { coll: "sales_2020", pipeline: [ { $set: { _id: "2020" } } ] } },
   { $sort: { _id: 1, store: 1, item: 1 } }
] )
```

**PostgreSQL 语法**      

使用 UNION ALL 进行结果集合并：
```
SELECT '2017' AS _id, * FROM sales_2017
UNION ALL
SELECT '2018' AS _id, * FROM sales_2018
UNION ALL
SELECT '2019' AS _id, * FROM sales_2019
UNION ALL
SELECT '2020' AS _id, * FROM sales_2020
ORDER BY _id, store, item;
```

## $unwind
对输入文档中的某一数组字段进行解构，以便为每个元素输出文档。每个输出文档均会将此数组替换为元素值。对于每个输入文档，输出 n 个文档，其中 n 为数组元素的数量，且对于空数组可为零。    

对数组字段 "b" 拆分：
```
db.coll2.aggregate([{$unwind:"$b"}])
```
拆分之后，1 条文档会变成多条，具体的条数对应数组中的元素个数。

**PostgreSQL 语法**       

对应 UNNEST 操作符，比如将 ARRAY_AGG 结果展开：    
```
WITH ORIGIN_RES AS (SELECT *, (
  SELECT ARRAY_AGG(description)
  FROM inventory
  WHERE sku = orders.item
) AS inventory_docs
FROM orders)
SELECT *, UNNEST(inventory_docs) FROM ORIGIN_RES;
```

# 3. 深入分析关键的 Stages
由于篇幅的原因，下面精选一些非常重要以及很有特点的 Stages 进行原理分析：
- $sort: 如果输入的文档没有索引序，则会进行阻塞式排序。是常见的性能杀手之一，特别是数据量大，涉及到外部排序的场景。
- $lookup: 被高频问到的 stage 之一，如何进行 join?
- $group: 高频使用的 stage 之一。
- $changeStream：MongoDB 的杀手锏之一，方便好用的 CDC 特性。

其中 `$changeStream` 最复杂，而且在不同的内核版本中进行了多次迭代，后续单独开一个章节进行分析。

## $sort
如果通过扫描索引读取数据，并且索引顺序满足排序需求，那么就可以避免额外的排序操作。          
否则，就需要进行阻塞式排序（blocking sort）。阻塞式排序会将所有数据加载到内存中，然后进行排序。如果数据量很大，并且设置了 allowDiskUse 为 true，就会将排序的中间结果溢出磁盘。并最终进行归并之后返回结果。         

如果 `$sort` 之后存在 `$limit`， 则可以提前进行过滤，精简中间结果，将时间复杂度和空间复杂度降低。      
针对不同场景，MongoDB 实现了 [LimitOneSorter](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/sorter/sorter.cpp#L605)、[TopKSorter](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/sorter/sorter.cpp#L646) 和 [NoLimitSorter](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/sorter/sorter.cpp#L487) 3 种算法。

### LimitOneSorter

如果 `$sort` 之后存在 `$limit 1`， 则可以使用 LimitOneSorter 算法。      
该算法只需要在内存中存储最优解即可，时间复杂度 O(N), 空间复杂度 O(1).

<p align="center">
<img width="429" height="477" alt="image" src="https://github.com/user-attachments/assets/11070fad-56e9-4b94-9938-36c9eb79226b" />
</p>

### TopKSorter
如果 `$sort` 之后存在 `$limit k`， 则可以使用 TopKSorter 算法。      
该算法会不断通过 2 种采样机制更新 cutoff 值：
- worst：worstSeen 存储当前看到的最差的值，如果 worstCount > limit， 则更新为 cutoff. 这种策略特别适用于输入数据本身有序的场景。
- median： medianSeen 存储当前看到的中位数的值，如果 medianCount > limit， 则更新为 cutoff. 这种策略适用于输入数据无序的场景。

如果看见一个差于 cutoff 的值，可以直接丢弃。        
最坏情况下，输入的数据顺序刚好和预期的相反，则 cutoff 机制失效。

输入的数据首先会在内存中组织排序，如果数据量很小（低于 100MB），则可以通过 [InMemIterator](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/sorter/sorter.cpp#L111) 返回。      

如果数据量很大，则会不断溢出（spill）到磁盘文件中。每次 `$sort` 的外部排序会对应一个独立的大文件，每轮溢出操作会在文件尾部追加一段 range. 每个 range 内部都是有序的，但是 range 之间无序。     

溢出到磁盘时，每次攒满 64K 会进行一次批量写入。写入格式为 `(uint32_t)size + data`.     
其中 data 部分会尝试用 snappy 进行压缩，如果压缩率低于 90% 则存储的时压缩数据，并将 size 置为负数以作标记。     

对于外部排序场景，每个文件 range 会对应一个 [FileIterator](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/sorter/sorter.cpp#L151)，用于顺序读取该 range 中的数据。多个 FileIterator 构成一个 std::vector 承载的 heap 结构，并通过 [MergeIterator](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/sorter/sorter.cpp#L343) 接口返回数据。    

<p align="center">
<img width="1477" height="1019" alt="image" src="https://github.com/user-attachments/assets/75213a0f-3baf-4379-97a0-ea590d549a17" />
</p>

时间复杂度，最好情况 O(N), 最坏情况 O(NlogN)。    
空间复杂度，最好情况 O(K) 内存，最坏情况 O(M) 内存 + O(N) 磁盘。

### NoLimitSorter
如果 `$sort` 之后不存在 `$limit`， 则可以使用 NoLimitSorter 算法。         

和 TopKSorter 算法类似，只是没有 worst 和 median 采样进行提前过滤。

<p align="center">
<img width="1472" height="1010" alt="image" src="https://github.com/user-attachments/assets/ea832698-98d3-4f7a-b2f8-974d3dc9324d" />
</p>

时间复杂度 O(N*logN), 空间复杂度 O(M)内存 + O(N)磁盘。    


## $lookup


## $group


## stage 优化


# 参考资料
1. 聚合操作的 Stages： https://www.mongodb.com/zh-cn/docs/manual/reference/mql/aggregation-stages/
2. 聚合操作的 Operators： https://www.mongodb.com/zh-cn/docs/manual/reference/mql/query-predicates/
3. 内核代码：https://github.com/mongodb/mongo/tree/r4.2.25/src
