# 1. 什么是 ChangeStream
ChangeStream是 MongoDB 提供的 CDC 解决方案。
```
Change streams allow applications to access real-time data changes without the complexity and risk of tailing the oplog.
```

使用传统的 tailing oplog 在 CDC（change data capture）场景下非常不便：    
1. oplog理论上仅供内部 replication 使用：    
    - 比如 noop oplog、migrate 对用户来说没有意义；
    - 比如 5.0 引入了 diff 格式，oplog 格式发生了变化，而且没有文档说明；
    - 比如事务相关的 oplog 有一系列的解析规则；
    - ...
2. 分片集群下每个 shard 都有自己的 oplog，排序和断点续传复杂。
3. 如果只需要关注某个表的数据变更，也要把oplog全部读出来，然后在客户端过滤。
4. 扩展性差，oplog 只是普通的数据，定制化逻辑需要客户端实现，比如post-image、pre-image 等等。

```
Applications can use change streams to subscribe to all data changes on a single collection, a database, or an entire deployment, and immediately react to them.
```

Change stream 支持库表级别的监听，相比 tailing oplog，易用性更强、而且节省了网络传输和客户端的资源消耗。

```
Because change streams use the aggregation framework, applications can also filter for specific changes or transform the notifications at will.
```

有着 aggregate 框架的加成，change stream 扩展性更强。比如支持获取 PreImages 和 PostImages，也支持 UpdateLookUp。

Change stream 的缺点：    
- 早期内核版本实现不完善。自3.6版本引入，后续版本对DDL操作支持不够，到6.0版本有所改善。
- 资源开销较大，性能跟不上。   

# 2. 总体流程
如图所示：    

<p align="center">
  <img src="https://github.com/user-attachments/assets/3e0eb9c6-4888-4f2c-8b27-078a4ba2dc06" width=900>
</p>

总体流程为：    
1. 客户端调用driver的watch函数。    
2. driver向mongos发送aggregate请求。    
3. mongos向各个shard 发送aggregate请求。    
4. 各个shard把各自的cursor返回给mongos。    
5. mongos返回一个cursor给driver。    
6. driver返回对象给客户端。    
7. 客户端后续调用driver的next函数。    
8. driver发送getMore给mongos，请求更多数据。    
9. mongos向各个shard发送getMore请求。    
10. 各个shard返回数据给mongos。    
11. mongos返回数据给driver。    
12. driver返回数据给客户端。    

客户端程序如下所示：    
```
func run(cli *mongo.Client, ts primitive.Timestamp) error {
	cs, err := clie.Watch(context.Background(), bson.A{}, &options.ChangeStreamOptions{
		StartAtOperationTime: &ts,
	})
	if err != nil {
		return err
	}

	for cs.Next(context.Background()) {
		var doc bson.D
		err = bson.Unmarshal(cs.Current, &doc)
		if err != nil {
			return err
		}
       // other code to consume doc
	}

    return cs.Err()
}
```

后面主要分享mongodb内核对change stream相关的处理，也上面的步骤2~11。基于 [4.2](https://github.com/mongodb/mongo/tree/v4.2) 代码进行分析。

带着问题看下面的讲解：     
- 如何汇聚、排序多个shard上的事件？    
- 如何做断点续传？不同于tailing oplog，要考虑到长时间没有事件的情况。    
- 如何处理事务相关的事件？    
- 性能差在哪？    

# 3. 详细流程
下面以一个真实的 change stream 使用场景为例，遵循时间顺序依次讲解 mongos 和 mongod 的处理逻辑。

## 3.1 mongos处理driver发来的aggregate请求
请求：    
```
{
  aggregate: 1,
  pipeline: [
    {
      '$changeStream': {
        allChangesForCluster: true,
        startAtOperationTime: Timestamp({ t: 1698997099, i: 0 }),
      }
    }
  ],
  cursor: {},
  lsid: { id: new UUID("14265384-a9d8-4aa5-a60c-f95d499ea833") },
  '$clusterTime': {
    clusterTime: Timestamp({ t: 1698997490, i: 1 }),
    signature: {
      hash: Binary("000000000000000000000000000000"),
      keyId: Long("0")
    }
  },
  '$db': 'admin'
}
```

mongos返回一个cursor id给driver:    
```
{
  cursor: {
    firstBatch: {},
    postBatchResumeToken: {_data: "8200000001000000002B0229296E04"},
    id: 9208559579072114181,
    ns: admin.$cmd.aggregate
  },
  ok: 1,
  operationTime: Timestamp({ t: 1698997500, i: 1 }),
  $clusterTime: {
    clusterTime: Timestamp({ t: 1698998143, i: 1 }),
    signature: {
      hash: Binary("00000000000000000000000"),
      keyId: Long("0")
    }
  }
}
```

下面来探究mongos是如何处理这个请求的。

### 代码处理流程
```
ClusterPipelineCommand::run    src/mongo/s/commands/cluster_pipeline_cmd.cpp   
    ClusterAggregate::runAggregate    src/mongo/s/query/cluster_aggregate.cpp  
        LiteParsedPipeline::LiteParsedPipeline
            LiteParsedDocumentSource::parse
                DocumentSourceChangeStream::LiteParsed::parse
        ClusterAggregate::runAggregate
            Pipeline::parse                                     // 构建pipeline
                Pipeline::parseTopLevelOrFacetPipeline
                    DocumentSource::parse
                        DocumentSourceChangeStream::createFromBson
                            buildPipeline
            sharded_agg_helpers::dispatchShardPipeline
                mustRunOnAllShards
                cluster_aggregation_planner::splitPipeline         // 分离出需要在shard上运行的pipeline
                    findSplitPoint
                createCommandForTargetedShards
                    genericTransformForShards
                establishShardCursors                              // 在每个shard上创建cursor
                    establishCursors
            dispatchMergingPipeline
                cluster_aggregation_planner::addMergeCursorsSource // 创建在mongos运行的pipeline
                    DocumentSourceMergeCursors::create
                runPipelineOnMongoS                                // 在mongos上运行pipeline
                    establishMergingMongosCursor                   // 建立一个总的cursor，用来吐出汇聚后的数据
                        cluster_aggregation_planner::buildClusterCursor
                        ccc->next(kInitialFind)
                            RouterStagePipeline::next
                                Pipeline::getNext
                                    DocumentSourceCloseCursor::getNext
                                        DocumentSourceUpdateOnAddShard::getNext
                                            DocumentSourceMergeCursors::getNext          // return EOF
                                                DocumentSourceMergeCursors::populateMerger
                                                    BlockingResultsMerger::BlockingResultsMerger
                                                        AsyncResultsMerger::AsyncResultsMerger
                                                BlockingResultsMerger::next
                                                    BlockingResultsMerger::awaitNextWithTimeout
                        setPostBatchResumeToken(ccc->getPostBatchResumeToken())
                            RouterStagePipeline::getPostBatchResumeToken
                                DocumentSourceMergeCursors::getHighWaterMark
                                    BlockingResultsMerger::getHighWaterMark
                                        AsyncResultsMerger::getHighWaterMark
                        registerCursor                               // 保存cursor信息，等待getMore请求
```

最初构建的pipeline，是由如下几个 stage 组成：    
- DocumentSourceOplogMatch  （不要与DocumentSourceMatch混淆）
- DocumentSourceChangeStreamTransform
- DocumentSourceCheckInvalidate
- DocumentSourceCheckResumability
- DocumentSourceCloseCursor

前 4 个 stage 是 shard stage，不会运行在 mongos 上。对于 mongos 来说这四个 stage 的意义就是生成发送给 shard 的命令，之后会把这几个 stage 丢弃。DocumentSourceCloseCursor 是 merge stage，要在 mongos 上做，merge 的时候以 _id._data 的值排序。      

cluster_aggregation_planner::addMergeCursorsSource 函数中，真正建立了要在mongos运行的pipeline（数据从上往下流动）：    
- DocumentSourceMergeCursors
- DocumentSourceUpdateOnAddShard
- DocumentSourceCloseCursor

建立好 pipeline 之后，会调用 getNext 运行一下，初始化各个stage，从各个 shard 拿到 cursor id 和 post batch resume token，之后返回 EOF。    

## 3.2 mongod 处理 mongos 发来的 aggregate 请求
mongos 向 mongod 发送的 aggregate 请求如下（pipeline 跟driver 发送给 mongos 的 pipeline 一样）：      

<p align="center">
  <img src="https://github.com/user-attachments/assets/56379713-bc62-4840-a195-e0ba21f2cf60" width=550>
</p>

mongod 把 cursor id 返回给 mongos :    
```
{
  cursor: {
    firstBatch: {},
    postBatchResumeToken: {_data: "8200000001000000002B0229296E04"},
    id: 9208559579072114181,
    ns: admin.$cmd.aggregate
  },
  $_internalLatestOplogTimestamp: Timestamp({ t: 1698997500, i: 1 }),
  ok: 1,
  operationTime: Timestamp({ t: 1698997500, i: 1 }),
  $gleStats: {
    lastOpTime: Timestamp({ t: 1234356, i: 0 }),
    electionId: ObjectID("7fffffff0000000000000001")
  },
  lastCommittedOpTime: Timestamp({ t: 1698997500, i: 1 }),
  $configServerState: {
    opTime: { ts: Timestamp({ t: 1698998143, i: 1 }), t: Long("1") }
  },
  $clusterTime: {
    clusterTime: Timestamp({ t: 1698998143, i: 1 }),
    signature: {
      hash: Binary("00000000000000000000000"),
      keyId: Long("0")
    }
  }
  operationTime: Timestamp({ t: 1698997500, i: 1 }),
}
```

### 代码处理流程
```
PipelineCommand::Invocation::run
    runAggregate         src/mongo/db/commands/run_aggregate.cpp
        LiteParsedPipeline::LiteParsedPipeline
            LiteParsedDocumentSource::parse
                DocumentSourceChangeStream::LiteParsed::parse
        Pipeline::parse                                      // 构建pipeline，mongos走的也是这个
            Pipeline::parseTopLevelOrFacetPipeline
                DocumentSource::parse
                    DocumentSourceChangeStream::createFromBson
                        buildPipeline
                            DocumentSourceChangeStream::buildMatchFilter
        PipelineD::buildInnerQueryExecutor                  // 创建aggregate框架下层的executor
            PipelineD::buildInnerQueryExecutorGeneric
                PipelineD::prepareExecutor                  // 生成对local.oplog.rs的执行计划
                    attemptToGetExecutor
                        getExecutorFind
                            _getExecutorFind
                                getOplogStartHack
                                    WiredTigerRecordStore::oplogStartHack // 给出collection scan的起始位置
        PipelineD::attachInnerQueryExecutorToPipeline      
            DocumentSourceCursor::create
            PipelineD::addCursorSource
                Pipeline::addInitialSource
        createOuterPipelineProxyExecutor                    // 为change stream加一个proxy stage
            ChangeStreamProxyStage::ChangeStreamProxyStage
                PipelineProxyStage::PipelineProxyStage
        registerCursor                                      // 保存cursor
        handleCursorCommand
            setPostBatchResumeToken(exec->getPostBatchResumeToken())
                PlanExecutorImpl::getPostBatchResumeToken
                    ChangeStreamProxyStage::getPostBatchResumeToken
```

跟 mongos 差不多，最初构建的 pipeline 由如下几个 stage 组成：    
- DocumentSourceOplogMatch  （不要与DocumentSourceMatch类混淆）
- DocumentSourceChangeStreamTransform
- DocumentSourceCheckInvalidate
- DocumentSourceCheckResumability

最后经过调整，pipeline 是这样：    
- CollectionScan
- DocumentSourceCursor
- DocumentSourceChangeStreamTransform
- DocumentSourceCheckInvalidate
- DocumentSourceCheckResumability
- ChangeStreamProxyStage

## 3.3 mongos 收到 driver 发来的 getMore 请求
请求如下：    
```
{
  getMore: Long("6439458890814903597"),
  collection: '$cmd.aggregate',
  lsid: { id: new UUID("14265384-a9d8-4aa5-a60c-f95d499ea833") },
  '$clusterTime': {
    clusterTime: Timestamp({ t: 1698997500, i: 1 }),
    signature: {
      hash: Binary("000000000000000000000000000000000"),
      keyId: Long("0")
    }
  },
  '$db': 'admin'
}
```

mongos 返回数据给 driver:
```
{
  cursor: {
    id: 9208559579072114181,
    ns: admin.$cmd.aggregate,
    nextBatch: {},
    postBatchResumeToken: {_data: "8200000001000000002B0229296E04"}
  },
  ok: 1,
  operationTime: Timestamp({ t: 1698997500, i: 1 }),
  $clusterTime: {
    clusterTime: Timestamp({ t: 1698998143, i: 1 }),
    signature: {
      hash: Binary("00000000000000000000000"),
      keyId: Long("0")
    }
  }
}
```

### 代码处理流程
```
ClusterGetMoreCmd::Invocation::run
    ClusterFind::runGetMore
        cursorManager->checkOutCursor
        ClusterCursorManager::PinnedCursor::next
            ClusterClientCursorImpl::next
                RouterStagePipeline::next
                    Pipeline::getNext
                        DocumentSourceCloseCursor::getNext
                            DocumentSourceUpdateOnAddShard::getNext
                                DocumentSourceMergeCursors::getNext
                                    BlockingResultsMerger::next
                                        BlockingResultsMerger::awaitNextWithTimeout
                                            BlockingResultsMerger::getNextEvent
                                                AsyncResultsMerger::nextEvent    
                                                    AsyncResultsMerger::_scheduleGetMores
```

mongos对driver发送来的getMore请求处理比较简单：    
- 取出cursor.
- 调用next.
- 底层stage发送getMore请求给mongod，获取数据。

接下来先看mongod如何处理getMore请求，以及返回给mongos什么数据。然后再看mongos如何返回数据给客户端。

## 3.4 mongod 处理 mongos 发来的 getMore 请求
请求如下：    

<p align="center">
  <img src="https://github.com/user-attachments/assets/1203e489-7c31-4f2e-934f-1cf13da8c0b2" width=550>
</p>

之后，mongod 把数据返回给 mongos:    
```
{
  cursor: {
    nextBatch: {},
    postBatchResumeToken: {_data: "8200000001000000002B0229296E04"},
    id: 9208559579072114181,
    ns: admin.$cmd.aggregate
  },
  $_internalLatestOplogTimestamp: Timestamp({ t: 1698997500, i: 1 }),
  ok: 1,
  $gleStats: {
    lastOpTime: Timestamp({ t: 0, i: 0 }),
    electionId: ObjectID("7fffffff0000000000000001")
  },
  lastCommittedOpTime: Timestamp({ t: 1698997500, i: 1 }),
  $configServerState: {
    opTime: { ts: Timestamp({ t: 1698998143, i: 1 }), t: Long("1") }
  },
  $clusterTime: {
    clusterTime: Timestamp({ t: 1698998143, i: 1 }),
    signature: {
      hash: Binary("00000000000000000000000"),
      keyId: Long("0")
    }
  },
  operationTime: Timestamp({ t: 1698997500, i: 1 })
}
```

这里面有两个重要的东西 **nextBatch** 和 **postBatchResumeToken**，分别是数据和断点续传标记。下面重点看这两个东西怎么来的。

### 代码处理流程
```
GetMoreCmd::Invocation::run
    GetMoreCmd::Invocation::acquireLocksAndIterateCursor
        GetMoreCmd::Invocation::generateBatch
            PlanExecutorImpl::getNext
                PlanExecutorImpl::_getNextImpl
                    PlanStage::work
                        PipelineProxyStage::doWork   （ChangeStreamProxyStage继承自PipelineProxyStage）
                            ChangeStreamProxyStage::getNextBson
                                Pipeline::getNext
                                    DocumentSourceCheckResumability::getNext
                                        DocumentSourceCheckInvalidate::getNext
                                            DocumentSourceChangeStreamTransform::getNext
                                                DocumentSourceCursor::getNext
                                                    DocumentSourceCursor::loadBatch
                                                        ...
                                                            CollectionScan::doWork
            setPostBatchResumeToken(exec->getPostBatchResumeToken())
                PlanExecutorImpl::getPostBatchResumeToken
                    ChangeStreamProxyStage::getPostBatchResumeToken    // read _postBatchResumeToken
```

跟 mongos 一样，取出 cursor 之后，就开始走 pipeline 流程。下面从数据流动的方向开始讲解。    

### CollectionScan    
Query 请求长这样：      

<p align="center">
  <img src="https://github.com/user-attachments/assets/028bd52b-73a1-4f1c-99cb-29e4fc546ff3" width=700>
</p>

过滤条件有很多：    
- oplog 的 ts 大于等于 {11,0} 。这个时间戳是客户端指定的，通过mongos透传到各个shard，用作断点续传。    
    - 考虑一个问题：不同的shard有不同的时间，仅用一个时间戳用作断点续传有没有问题？    
- 捕获 fromMigrate 为 false，非a dmin、config、local 库且 op 不是 n 的 oplog.    
- 捕获 fromMigrate 为 false，op 是 n，且 o2.type 为migrateChunkToNewShard 的 oplog.
- 另外捕获了 drop、dropDatabase、renameCollection、commitTransaction、applyOps 等 DDL 操作。

CollectionScan 不是从头开始的，而是从小于等于客户端指定时间戳，且最近的那条 oplog 开始。如果所有 oplog 的 ts 都比客户端指定时间戳大，collectionScan 就从头开始。然后 collectionScan 会检查这条 oplog，如果不满足下面任一条件，就说明 oplog 被冲了，会抛出ErrorCodes::OplogQueryMinTsMissing 异常：    
- 此 oplog 是类型是 n，o.msg 是 initiating set ，表示这是一个新副本集。
- 此 oplog 的 ts 小于等于给定的 ts.

只要有一个 shard 上的 oplog 被冲了，change stream 命令就会失败。    

另外需要关注，collectionScan 每次返回文档之前都更新一下_latestOplogEntryTimestamp:    
```
Status CollectionScan::setLatestOplogEntryTimestamp(const Record& record) {
    auto tsElem = record.data.toBson()[repl::OpTime::kTimestampFieldName];
    _latestOplogEntryTimestamp = std::max(_latestOplogEntryTimestamp, tsElem.timestamp());
    return Status::OK();
}
```

### DocumentSourceCursor
DocumentSourceCursor 对接 CollectionScan 等普通查询，作为aggregate pipeline 的数据来源。    

这里注意一下 _latestOplogTimestamp 这个私有变量，每次 getNext 都会被更新：    
```
DocumentSource::GetNextResult DocumentSourceCursor::getNext() {
    // ......
    // If we are tracking the oplog timestamp, update our cached latest optime.
    if (_trackOplogTS && _exec)
        _updateOplogTimestamp();
    // ......
}

void DocumentSourceCursor::_updateOplogTimestamp() {
    // If we are about to return a result, set our oplog timestamp to the optime of that result.
    if (!_currentBatch.isEmpty()) {
        const auto& ts = _currentBatch.peekFront().getField(repl::OpTime::kTimestampFieldName);
        invariant(ts.getType() == BSONType::bsonTimestamp);
        _latestOplogTimestamp = ts.getTimestamp();       // 设置为要返回的oplog的ts
        return;
    }

    // If we have no more results to return, advance to the latest oplog timestamp.
    _latestOplogTimestamp = _exec->getLatestOplogTimestamp(); // 从collection scan取时间戳
}

Timestamp CollectionScan::getLatestOplogTimestamp() const {
    return _latestOplogEntryTimestamp;
}
```

### DocumentSourceChangeStreamTransform
这个 stage 负责把 oplog 转换成 **event**。      
```
DocumentSource::GetNextResult DocumentSourceChangeStreamTransform::getNext() {
    while (1) {
        // If we're unwinding an 'applyOps' from a transaction, check if there are any documents we
        // have stored that can be returned.
        if (_txnIterator) {
            if (auto next = _txnIterator->getNextTransactionOp(pExpCtx->opCtx)) {
                return applyTransformation(*next);
            }
            _txnIterator = boost::none;
        }

        // Get the next input document.
        auto input = pSource->getNext();
        if (!input.isAdvanced()) {
            return input;
        }

        auto doc = input.releaseDocument();
        auto op = doc[repl::OplogEntry::kOpTypeFieldName];
        auto opType =
            repl::OpType_parse(IDLParserErrorContext("ChangeStreamEntry.op"), op.getStringData());
        auto commandVal = doc["o"];
        if (opType != repl::OpTypeEnum::kCommand ||
            (commandVal["applyOps"].missing() && commandVal["commitTransaction"].missing())) {
            return applyTransformation(doc);
        }

        // The only two commands we will see here are an applyOps or a commit, which both mean we
        // need to open a "transaction context" representing a group of updates that all occurred at
        // once as part of a transaction. 
        invariant(!_txnIterator);
        _txnIterator.emplace(pExpCtx->opCtx, pExpCtx->mongoProcessInterface, doc, *_nsRegex);
    }
}
```

会把 applyOps 拆开处理。对于普通 oplog，直接进行 transform：    
```
Document DocumentSourceChangeStreamTransform::applyTransformation(const Document& input) {
    MutableDocument doc;

    // Extract the fields we need.
    string op = input[repl::OplogEntry::kOpTypeFieldName].getString();
    Value ts = input[repl::OplogEntry::kTimestampFieldName];
    Value ns = input[repl::OplogEntry::kNssFieldName];
    Value uuid = input[repl::OplogEntry::kUuidFieldName];
    std::vector<FieldPath> documentKeyFields;

    // Deal with CRUD operations and commands.
    auto opType = repl::OpType_parse(IDLParserErrorContext("ChangeStreamEntry.op"), op);

    NamespaceString nss(ns.getString());
    // Ignore commands in the oplog when looking up the document key fields since a command implies
    // that the change stream is about to be invalidated (e.g. collection drop).
    if (!uuid.missing() && opType != repl::OpTypeEnum::kCommand) {
        documentKeyFields = _documentKeyCache.find(uuid.getUuid())->second.documentKeyFields;
    }
    Value id = input.getNestedField("o._id");
    // Non-replace updates have the _id in field "o2".
    StringData operationType;
    Value fullDocument;
    Value updateDescription;
    Value documentKey;

    switch (opType) {
        case repl::OpTypeEnum::kInsert: {
            operationType = DocumentSourceChangeStream::kInsertOpType;
            fullDocument = input[repl::OplogEntry::kObjectFieldName];
            documentKey = Value(document_path_support::extractPathsFromDoc(
                fullDocument.getDocument(), documentKeyFields));
            break;
        }
        case repl::OpTypeEnum::kDelete: {
            operationType = DocumentSourceChangeStream::kDeleteOpType;
            documentKey = input[repl::OplogEntry::kObjectFieldName];
            break;
        }
        case repl::OpTypeEnum::kUpdate: {
            if (id.missing()) {
                // non-replace style update
                operationType = DocumentSourceChangeStream::kUpdateOpType;
                // ......
            } else {
                // replace style update
                operationType = DocumentSourceChangeStream::kReplaceOpType;
                fullDocument = input[repl::OplogEntry::kObjectFieldName];
            }
            documentKey = input[repl::OplogEntry::kObject2FieldName];
            break;
        }
        case repl::OpTypeEnum::kCommand: {
            if (!input.getNestedField("o.drop").missing()) {
                operationType = DocumentSourceChangeStream::kDropCollectionOpType;
                nss = NamespaceString(nss.db(), input.getNestedField("o.drop").getString());
            } else if (!input.getNestedField("o.renameCollection").missing()) {
                operationType = DocumentSourceChangeStream::kRenameCollectionOpType;
                // ......
            } else if (!input.getNestedField("o.dropDatabase").missing()) {
                operationType = DocumentSourceChangeStream::kDropDatabaseOpType;
                nss = NamespaceString(nss.db());
            } else {
                // All other commands will invalidate the stream.
                operationType = DocumentSourceChangeStream::kInvalidateOpType;
            }

            // Make sure the result doesn't have a document key.
            documentKey = Value();
            break;
        }
        case repl::OpTypeEnum::kNoop: {
            operationType = DocumentSourceChangeStream::kNewShardDetectedOpType;
            // Generate a fake document Id for NewShardDetected operation so that we can resume
            // after this operation.
            documentKey = Value(Document{{DocumentSourceChangeStream::kIdField,
                                          input[repl::OplogEntry::kObject2FieldName]}});
            break;
        }
        default: { MONGO_UNREACHABLE; }
    }

    // Note that 'documentKey' and/or 'uuid' might be missing, in which case they will not appear
    // in the output.
    auto resumeTokenData = getResumeToken(ts, uuid, documentKey);
    auto resumeToken = ResumeToken(resumeTokenData).toDocument();

    // Add some additional fields only relevant to transactions.
    if (_txnIterator) {
        doc.addField(DocumentSourceChangeStream::kTxnNumberField,
                     Value(static_cast<long long>(_txnIterator->txnNumber())));
        doc.addField(DocumentSourceChangeStream::kLsidField, Value(_txnIterator->lsid()));
    }

    doc.addField(DocumentSourceChangeStream::kIdField, Value(resumeToken));
    doc.addField(DocumentSourceChangeStream::kOperationTypeField, Value(operationType));
    doc.addField(DocumentSourceChangeStream::kClusterTimeField, Value(resumeTokenData.clusterTime));

    doc.setSortKeyMetaField(resumeToken.toBson());
    // ......
    doc.addField(DocumentSourceChangeStream::kDocumentKeyField, documentKey);
    // ......
}
```

这里就是对CURD操作和一些 DDL 操作进行了转换。需要注意的是：    
- 相比 oplog，event 多了 replace、invalidate 和kNewShardDetected 等很多类型。
- oplog 的 ts 字段被用来生成 resume token，也被用来当作 event 的 clusterTime 字段的值。
- resume token 同时作为 event 的 _id 和 sortKey.     

### DocumentSourceCheckInvalidate
这个 stage 检查 event，如果符合下面条件就生成一个 invalidate 类型的e vent：
- 要监听的 ns、db 被 drop 或者 rename.

### DocumentSourceCheckResumability
这个 stage 做断点续传的恢复工作。    
```
DocumentSource::GetNextResult DocumentSourceCheckResumability::getNext() {
    if (_resumeStatus == ResumeStatus::kSurpassedToken) {
        return pSource->getNext();
    }

    while (_resumeStatus != ResumeStatus::kSurpassedToken) {
        // The underlying oplog scan will throw OplogQueryMinTsMissing if the minTs in the change
        // stream filter has fallen off the oplog. Catch this and throw a more explanatory error.
        auto nextInput = [this]() {
            try {
                return pSource->getNext();
            } catch (const ExceptionFor<ErrorCodes::OplogQueryMinTsMissing>&) {
                uasserted(ErrorCodes::ChangeStreamHistoryLost,
                          "Resume of change stream was not possible, as the resume point may no "
                          "longer be in the oplog.");
            }
        }();

        // If we hit EOF, return it immediately.
        if (!nextInput.isAdvanced()) {
            return nextInput;
        }

        // Determine whether the current event sorts before, equal to or after the resume token.
        _resumeStatus =
            compareAgainstClientResumeToken(pExpCtx, nextInput.getDocument(), _tokenFromClient);
        switch (_resumeStatus) {
            case ResumeStatus::kCheckNextDoc:
                // If the result was kCheckNextDoc, we are resumable but must swallow this event.
                continue;
            case ResumeStatus::kSurpassedToken:
                // In this case the resume token wasn't found; it may be on another shard. However,
                // since the oplog scan did not throw, we know that we are resumable. Fall through
                // into the following case and return the document.
            case ResumeStatus::kFoundToken:
                // We found the actual token! Return the doc so DSEnsureResumeTokenPresent sees it.
                return nextInput;
        }
    }
    MONGO_UNREACHABLE;
}
```

这个 getNext 函数的意思是：     
- oplog被冲，不可能恢复时，抛出ErrorCodes::ChangeStreamHistoryLost异常。    
- 如果可以恢复:
    - 能找到这个token，就从这个token开始吐出事件；
    - 找不到这个token，说明这个token很可能是其他shard生成的，就从大于这个token的地方开始吐出事件。

不返回重复事件。

### ChangeStreamProxyStage    
```
boost::optional<BSONObj> ChangeStreamProxyStage::getNextBson() {
    if (auto next = _pipeline->getNext()) {
        auto nextBSON = _validateAndConvertToBSON(*next);
        _latestOplogTimestamp = PipelineD::getLatestOplogTimestamp(_pipeline.get());
        _postBatchResumeToken = next->getSortKeyMetaField();
        _setSpeculativeReadTimestamp();
        return nextBSON;
    }

    // We ran out of results to return. 
    auto highWaterMark = PipelineD::getLatestOplogTimestamp(_pipeline.get());
    if (highWaterMark > _latestOplogTimestamp) {
        auto token = ResumeToken::makeHighWaterMarkToken(highWaterMark);
        _postBatchResumeToken = token.toDocument().toBson();
        _latestOplogTimestamp = highWaterMark;
        _setSpeculativeReadTimestamp();
    }
    return boost::none;
}
```
PipelineD::getLatestOplogTimestamp() 返回的就是 DocumentSourceCursor::_latestOplogTimestamp。

这个 stage 做的主要工作就是维护 postBatchResumeToken 变量。postBatchResumeToken 就是用作断点续传的 resume token。
- 返回每个文档之前，都把postBatchResumeToken更新成这个文档的resume token.
- 没有文档要返回时，就把 postBatchResumeToken 设置成一个high-water-mark resume token，它的 ts 是 collectionScan 里面的 latest oplog timestamp。

为什么要有 high-water-mark resume token？    
想象一个场景——客户端 watch 一个 collection，但是这个 collection 一直没有写入，如果 postBatchResumeToken 一直不推进，那等客户端拿着很旧的 resume token 续传时，发现无法恢复。high-water-mark resume token 的意义就是即使没有客户端关注的事件产生，也能推进同步点。    

## 3.5 mongos汇聚数据返回给 driver
每个 shard 上的事件是有序的，shard 间的事件依赖混合逻辑时钟拥有偏序关系。mongos 的任务就是汇聚各个 shard 上的事件并排序，提供一个全局顺序。

### 代码处理流程
```
ClusterGetMoreCmd::Invocation::run
    ClusterFind::runGetMore
        cursorManager->checkOutCursor
        ClusterCursorManager::PinnedCursor::next
            ClusterClientCursorImpl::next
                RouterStagePipeline::next
                    Pipeline::getNext
                        DocumentSourceCloseCursor::getNext
                            DocumentSourceUpdateOnAddShard::getNext
                                DocumentSourceMergeCursors::getNext
```

### DocumentSourceMergeCursors::getNext
```
DocumentSourceMergeCursors::getNext
    BlockingResultsMerger::next
        BlockingResultsMerger::awaitNextWithTimeout
            BlockingResultsMerger::getNextEvent
                AsyncResultsMerger::nextEvent    （整个函数持有_mutex）
                    AsyncResultsMerger::_scheduleGetMores
                        AsyncResultsMerger::_askForNextBatch
                            TaskExecutor::scheduleRemoteCommand
                            AsyncResultsMerger::_handleBatchResponse
                                AsyncResultsMerger::_processBatchResults
                                    AsyncResultsMerger::_addBatchToBuffer
                                        AsyncResultsMerger::_updateRemoteMetadata
            AsyncResultsMerger::nextReady
                AsyncResultsMerger::_nextReadySorted
```

汇聚各个 shard 上的事件并做排序其实很简单，就是一个归并排序。     

这里的核心是 AsyncResultsMerger。给每个 shard 维护了一个docBuffer queue，用来存放 getMore response 里面的 events。每次会按照 _id._data 对比所有 docBuffer queue 的第一条文档，返回最小的那条。    

<p align="center">
  <img src="https://github.com/user-attachments/assets/090ff6ae-ebda-4214-821f-e243af3945ba" width=600>
</p>

这里有个问题需要考虑：如果一个 shard 的 docBuffer 目前是空，mongos 该怎么办呢？    
- 一种可能是网络问题或者mongod负载高，响应慢了。
- 另一种可能是这个shard上根本就没有事件产生。    

如果是因为响应慢了，必须要多等会，否则无法保证全局顺序；如果是因为没有事件，不需要等待。

那么，到底等不等呢？如何适配这两种情况？    

这里就要讲到 Post Batch Resume Token（简称 PBRT）和 minPromisedSortKey 的概念了。前面已经简单介绍过 mongod 是怎么产生 PBRT 的——PBRT 要么是最近事件的 resume token，要么是已扫描的最新 oplog 的时间戳构成的 high-water-mark resume token，取决于是否遇到 EOF。PBRT 也可以看作是 shard 给 mongos 的一个承诺——承诺后面返回的事件都大于等于这个时间戳。mongos 从所有 shard 的 PBRT 中取一个最小值，叫做 minPromisedSortKey。有了 minPromisedSortKey 就可以解决上面的问题了。    

解决方法就是mongos每次返回事件之前都跟minPromisedSortKey对比：    
- 如果小于等于 minPromisedSortKey，说明可以安全返回。
- 如果大于 minPromisedSortKey，说明有些 shard “响应慢了“，需要等待。    

总的来说，mongos 的处理策略如下：    
1. 在向所有 shard 发出 aggregate 请求之后，每个 shard 都会把自己的 cursor id 和 PBRT 返回给 mongos。mongos 会等待所有 shard 响应之后，根据所有的 PBRT 得到 minPromisedSortKey。在此之前不向客户端返回数据。    
2. 在客户端不断 getMore 过程中，如果当前最小的 event（对比所有非空 docBuffer 的第一个 event）比 minPromisedSortKey 要大，就不返回数据，等待 minPromisedSortKey 的推进。    
3.每次收到来自 shard 的 PBRT，都会更新 minPromisedSortKey。    

```
bool AsyncResultsMerger::_readySortedTailable(WithLock lk) {
    if (_mergeQueue.empty()) {
        return false;
    }

    auto smallestRemote = _mergeQueue.top();
    auto smallestResult = _remotes[smallestRemote].docBuffer.front();
    auto keyWeWantToReturn =
        extractSortKey(*smallestResult.getResult(), _params.getCompareWholeSortKey());
    // We should always have a minPromisedSortKey from every shard in the sorted tailable case.
    auto minPromisedSortKey = _getMinPromisedSortKey(lk);
    invariant(!minPromisedSortKey.isEmpty());
    return compareSortKeys(keyWeWantToReturn, minPromisedSortKey, *_params.getSort()) <= 0;
}
```

这样 mongos 就可以向客户端吐出全局有序的数据流了。mongos 发送给客户端的每个响应中都带有一批 events 和 postBatchResumeToken 字段，postBatchResumeToken 就是 resume token，用作断点续传。这个字段的值：    
- 在不等待的时候，是已返回的最新 event 的 sort key 的值。
- 在等待的时候，是 minPromisedSortKey 。这样即使没有事件产生，客户端也能知道最新的 resume token。    

### DocumentSourceUpdateOnAddShard::getNext    
```
DocumentSource::GetNextResult DocumentSourceUpdateOnAddShard::getNext() {
    auto childResult = pSource->getNext();

    while (childResult.isAdvanced() && needsUpdate(childResult.getDocument())) {
        addNewShardCursors(childResult.getDocument());
        childResult = pSource->getNext();
    }
    return childResult;
}

// Returns true if the change stream document has an 'operationType' of 'newShardDetected'.
bool needsUpdate(const Document& childResult) {
    return childResult[DocumentSourceChangeStream::kOperationTypeField].getStringData() ==
        DocumentSourceChangeStream::kNewShardDetectedOpType;
}
```

当有 chunk mirgrate 到新的 shard 上，会产生kNewShardDetectedOpType event 。这个 stage 遇到这个类型的event，就会建立通往新 shard 的 cursor。

### DocumentSourceCloseCursor::getNext
```
DocumentSource::GetNextResult DocumentSourceCloseCursor::getNext() {
    pExpCtx->checkForInterrupt();

    // Close cursor if we have returned an invalidate entry.
    if (_shouldCloseCursor) {
        uasserted(ErrorCodes::CloseChangeStream, "Change stream has been invalidated");
    }

    auto nextInput = pSource->getNext();
    if (!nextInput.isAdvanced())
        return nextInput;

    auto doc = nextInput.getDocument();
    const auto& kOperationTypeField = DocumentSourceChangeStream::kOperationTypeField;
    auto operationType = doc[kOperationTypeField].getString();
    if (operationType == DocumentSourceChangeStream::kInvalidateOpType) {
        _shouldCloseCursor = true;
    }

    return nextInput;
}
```

当文件的 operationType 字段值为 invalidate 时，下次调用getNext，会抛出异常，关闭 cursor。

## 3.6 resume token   
resume token 用作 events 的排序和 change stream 的恢复。    
```
struct ResumeTokenData {
    enum FromInvalidate : bool {
        kFromInvalidate = true,
        kNotFromInvalidate = false,
    };

    enum TokenType : int {
        kHighWaterMarkToken = 0,  // Token refers to a point in time, not an event.
        kEventToken = 128,        // Token refers to an actual event in the stream.
    };

    Timestamp clusterTime;      // 如果是kEventToken，就等于oplog的cluster time
    int version = 1;            
    TokenType tokenType = TokenType::kEventToken;
    size_t txnOpIndex = 0;
    // Flag to indicate that this resume token is from an "invalidate" entry. This will not be set
    // on a token from a command that *would* invalidate a change stream, but rather the invalidate
    // notification itself.
    FromInvalidate fromInvalidate = FromInvalidate::kNotFromInvalidate;
    boost::optional<UUID> uuid;
    Value documentKey;    // _id + shard key
};

/*   {
*     _data: String, A hex encoding of the binary generated by keystring encoding the clusterTime,
*            version, txnOpIndex, UUID, then documentKey in that order.
*     _typeBits: BinData - The keystring typebuildMatchFilter bits used for deserialization.
*   }
*/
class ResumeToken {
public:
    constexpr static StringData kDataFieldName = "_data"_sd;
    constexpr static StringData kTypeBitsFieldName = "_typeBits"_sd;

    // 把ResumeTokenData转为hex string
    explicit ResumeToken(const ResumeTokenData& resumeValue);

    // 把hex string转为ResumeTokenData
    ResumeTokenData getData() const;
private:
    // This is the hex-encoded string encoding all the pieces of the resume token.
    std::string _hexKeyString;

    // Since we are using a KeyString encoding, we might lose some information about what the
    // original types of the serialized values were. For example, the integer 2 and the double 2.0
    // will generate the same KeyString. We keep the type bits around so we can deserialize without
    // losing information.
    Value _typeBits;   // 不参与比较，这个字段常常被忽略
};
```

要想排序，就要尽可能精确地描述 event，尽量让每个 event 都有一个唯一的值。能够做到全局有序，那么恢复也不是什么难事。      

ResumeTokenData 里面有这些字段：
- clusterTime，表示了集群中事件的偏序关系。不同的 shard 可能产生有相同 ts 的 event。events 的 ts 相同，说明他们的先后顺序不重要。
- documentKey ，不同的 documentKey 在同一时间一定属于不同的shard。有些 event 是没有 documentKey 的，比如 DDL 操作。
- txnOpIndex 是针对多文档事务添加的。applyOps oplog里面会包括多个子oplog。
- tokenType 和 fromInvalidate 是为断点续传设计的。 
- uuid，collection的uuid，直接沿用oplog的ui。
- version用做断点续传时兼容性判断，比如集群升级:
    - 3.6是第一个版本，clusterTime + documentKey + uuid.    
    - 4.0.0 ~ 4.0.6 是v0版本，clusterTime + applyOpsIndex + documentKey + uuid.    
    - v1版本就是这篇文章讲的。
    - 6.0 内核开始引入了 v2 版本，主要变化就是 documentKey 字段变成了eventIdentifier 字段，对于 CURD 操作还是当作 documentKey 使用，对于非 CURD 操作，用来记录operationDescription。开始支持更多的DDL操作。

最后总结一下这些时间戳：    
- oplog entry 的 ts.    
- change event 的 cluster time.    
- collection scan 的 latest oplog timestamp.     
- shard 上维护的 PBRT.    
- mongos 上的 minPromisedSortKey.    
- resume token，用作客户端断点续传。    

# 4. ChangeStream 的问题以及应对方案


# 5. 总结

# 6. 参考资料
1. https://github.com/mongodb/mongo/tree/v4.2
