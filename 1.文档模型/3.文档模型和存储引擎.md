# 1. MongoDB 内核架构
关于分布式(Sharding) 和副本集(Replication) 的内容放到后面的章节讨论。这里主要讨论单个 mongod 节点的内核架构。    
引用[官方文档](https://www.mongodb.com/blog/post/building-applications-with-mongodbs-pluggable-storage-engines-part-1)的架构图如下：    
<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/814fcfb9-c3b9-415f-a44e-454d937c6abd" width=700>
</p>

如图所示，用户使用 MongoDB query language 进行数据的增删查改操作，MongoDB 服务侧生成对应的执行计划，然后调用 Document Model 提供的接口进行数据处理，并将处理后的结果返回给用户端。如果涉及到数据变更，还会将新生成的数据序列化之后存储到底层的 KV 存储引擎。      

没有一个存储引擎能完美适配所有的业务场景。当业务在选择合适的数据库时，除了数据库的稳定性、易用性、流行度、基准测试性能等方面的考虑外，还会根据自身的业务特点并结合存储引擎的特性进行分析和压测。比如使用机械硬盘存储日志类的场景，可能会优先考虑 LSMTree 类型的存储引擎；对于使用大内存、NVMe-SSD 并存在大量随机读写的场景，可能会更倾向于使用 Btree 类的存储引擎。       
为了适配更多的业务场景，很多数据库产品都支持插件式存储引擎，比如 MySQL 就支持 MyIASM、InnoDB、RocksDB 等多种存储引擎。     
MongoDB 从 3.x 版本开始，也通过插件式存储引擎的形式，支持了越来越多的存储引擎。比如官方文档中公布的存储引擎就有：      
<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/6741cd21-22a8-436a-9349-2a9e9303282f" width=800>
</p>         

另外，MongoDB 在 3.x 版本也支持RocksDB 引擎。对于存储日志、账单类型的业务，以及使用机械硬盘的业务场景有比较大的性能优势。
使用方式可以参考 [MongoRocks](https://github.com/mongodb-partners/mongo-rocks) 的介绍。
但是需要说明的是 RocksDB 目前还不是 MongoDB 官方支持的存储引擎。

MongoDB 为了支持插件式存储引擎，定义了一个虚基类 [KVEngine](https://github.com/mongodb/mongo/blob/r4.0.28/src/mongo/db/storage/kv/kg_engine.h) 规定 KV 存储引擎接入 MongoDB 的标准，具体规范了建表、建索引、获取引擎元数据等操作的接口。另外，也定义了虚基类 [RecordStore](https://github.com/mongodb/mongo/blob/r4.0.28/src/mongo/db/storage/record_store.h) 定义了表的操作接口，以及虚基类 [SortedDataInterface](https://github.com/mongodb/mongo/blob/r4.0.28/src/mongo/db/storage/sorted_data_interface.h) 定义了索引的操作接口， [SnapshotManager](https://github.com/mongodb/mongo/blob/r4.0.28/src/mongo/db/storage/snapshot_manager.h) 用于 snapshot 管理。      
以 WiredTiger 引擎为例，通过 [WiredTigerKVEngine](https://github.com/mongodb/mongo/blob/r4.0.28/src/mongo/db/storage/wiredtiger/wiredtiger_kv_engine.h) 实现存储引擎的接入接口，通过 [WiredTigerRecordStore](https://github.com/mongodb/mongo/blob/r4.0.28/src/mongo/db/storage/wiredtiger/wiredtiger_record_store.h) 和  [WiredTigerIndex](https://github.com/mongodb/mongo/blob/r4.0.28/src/mongo/db/storage/wiredtiger/wiredtiger_index.h) 实现了表和索引的操作接口，通过  [WiredTigerSnapshotManager](https://github.com/mongodb/mongo/blob/r4.0.28/src/mongo/db/storage/wiredtiger/wiredtiger_snapshot_manager.h) 实现了 snapshot 的管理机制。       
上述相关代码存放在代码目录 "src/mongo/db/storage/wiredtiger/" 中，独立于 WiredTiger 引擎的代码。可以看作是 MongoServer 和底层存储引擎之间的接口层。这个接口层实现了 MongoDB 规定的接口，并调用底层存储引擎的 API 完成具体的数据操作。   
而 Wiredtiger 引擎本身有独立的代码仓库，放在 thirdparty 目录下，和 MongoDB 代码进行联合编译。   

同理，RocksDB 引擎也是独立的代码仓库，可以通过 MongoRocks 这个接口层接入到 MongoDB 中。在 MongoRocks 中也对应的通过 [RocksEngine](https://github.com/mongodb-partners/mongo-rocks/blob/master/src/rocks_engine.h) 实现了存储引擎的接入接口，通过 [RocksRecordStore](https://github.com/mongodb-partners/mongo-rocks/blob/master/src/rocks_record_store.h) 和 [RocksIndexBase](https://github.com/mongodb-partners/mongo-rocks/blob/master/src/rocks_index.h) 实现了表和索引的操作接口，通过 [RocksSnapshotManager](https://github.com/mongodb-partners/mongo-rocks/blob/master/src/rocks_snapshot_manager.h) 实现了 snapshot 的管理操作接口。
# 2. MongoDB 如何使用 WiredTiger 存储数据

从前面的分析可以发现，KV 存储引擎是不感知 BSON 文档模型的。
在 KV 引擎中存储的数据有：
1. recordId: int64 类型，作为表在 KV 引擎中存储时的 Key;
2. BSON 文档序列化后的字符串: String 或者 byte[] 类型，作为表在 KV  引擎中存储时的 Value；
3. KeyString: String或者 byte[] 类型，作为索引在 KV 引擎中存储时的 Key;
4. (KeyString的 )typeBits: String 或者 byte[] 类型，作为索引在 KV  引擎中存储时的 Value;

MongoDB 中的表和索引，对应到 Wirediger 存储引擎中都是 `wt table`. 由于 MongoDB 中表和索引存储的数据类型有所区别，因此在定义 `wt table`的schema 时也会不同。  
WiredTiger 目前支持的[存储类型](https://source.wiredtiger.com/3.2.1/schema.html#schema_format_types) 有：    
<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/1c7e3be2-9289-4d9d-94e3-044671d357d7" width=500>
</p>           
对于 MongoDB 表来说，存储的数据是 recordId -> BSON文档，因此对应的 `wt table` schema 为 "key_format=q, value_format=u".   
对于 MongoDB 索引来说，存储的数据时 KeyString-> typeBits, 因此对应的 `wt table` schema 为 "key_format=u, value_format=u".

除了上述 Key/Value 的类型之外，在定义 `wt table` 时还可以指定 叶子节点大小、数据块对齐大小、数据块分配和压缩算法等参数。详细的配置参数和说明，可以参考 [WT 官方文档](https://source.wiredtiger.com/3.2.1/struct_w_t___s_e_s_s_i_o_n.html#a358ca4141d59c345f401c58501276bbb)

MongoDB 在建表和建索引时具体指定的 schema 参数可以参考 [WiredTigerRecordStore::generateCreateString](https://github.com/mongodb/mongo/blob/r4.0.28/src/mongo/db/storage/wiredtiger/wiredtiger_record_store.cpp#L553) 和 [WiredTigerIndex::generateCreateString](https://github.com/mongodb/mongo/blob/r4.0.28/src/mongo/db/storage/wiredtiger/wiredtiger_index.cpp#L195)

>tips: 对于 MongoDB 用户来说，可以在连接上 MongoDB 节点之后，通过db.colltest.stats().wiredTiger.creationString命令来查看建表时的各项参数，举例如下：    
>access_pattern_hint=none,allocation_size=4KB,app_metadata=(formatVersion=1),assert=(commit_timestamp=none,read_timestamp=none),block_allocation=best,block_compressor=snappy,cache_resident=false,checksum=on,colgroups=,collator=,columns=,dictionary=0,encryption=(keyid=,name=),exclusive=false,extractor=,format=btree,huffman_key=,huffman_value=,ignore_in_memory_cache_size=false,immutable=false,internal_item_max=0,internal_key_max=0,internal_key_truncate=true,internal_page_max=4KB,key_format=q,key_gap=10,leaf_item_max=0,leaf_key_max=0,leaf_page_max=32KB,leaf_value_max=64MB,log=(enabled=false),lsm=(auto_throttle=true,bloom=true,bloom_bit_count=16,bloom_config=,bloom_hash_count=8,bloom_oldest=false,chunk_count_limit=0,chunk_max=5GB,chunk_size=10MB,merge_custom=(prefix=,start_generation=0,suffix=),merge_max=15,merge_min=0),memory_page_image_max=0,memory_page_max=10m,os_cache_dirty_max=0,os_cache_max=0,prefix_compression=false,prefix_compression_min=4,source=,split_deepen_min_child=0,split_deepen_per_child=0,split_pct=90,type=file,value_format=u



# 3. WiredTiger 存储引擎架构

参考[官方文档](https://source.wiredtiger.com/develop/arch-index.html)，WiredTiger 引擎的整体架构如下：   
<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/1e4f2f65-bead-4471-a802-88d458b328fd" width="500">
</p>

涉及到的一些重要组件和概念包括：
- [Schema](https://source.wiredtiger.com/develop/arch-schema.html): 引擎中存储的数据格式，其中定义了 Key 和 Value 的类型。
- [Metadata](https://source.wiredtiger.com/develop/arch-metadata.html): WiredTiger 引擎的元数据，定义了有哪些表以及对应的文件，Checkpoint 信息等，使用 WiredTiger.wt 这个特殊的表进行存储。
- [Cursor](https://source.wiredtiger.com/develop/arch-cursor.html): WiredTiger 对外的数据接口都通过 Cursor 执行，Cursor 指向了表中的某个位置，Cursor 给上层提供了按 Key 检索，迭代，读写数据等操作接口。
- [Dhandle](https://source.wiredtiger.com/develop/arch-dhandle.html):描述了一个资源的句柄，通俗理解为一个 'wt table'. 一般指向一个 Btree；
- [Btree](https://source.wiredtiger.com/develop/arch-btree.html): WiredTiger 中存储表的一种格式，其中 root page 和 internal page 只存储 key 和下一层 page 的指针，leaf page 存储用户写入的 KV 数据，Btree 是一个按 key 有序的数据结构。WiredTiger 中的 Btree 节点在内存中的表现形式与磁盘上的表现形式有所区别。
- [Transaction](https://source.wiredtiger.com/develop/arch-transaction.html): 事务接口。  
- [Snapshot](https://source.wiredtiger.com/develop/arch-snapshot.html): WiredTiger 使用 snapshot 进行多版本并发控制，snapshot 定义了哪些版本的数据对事务可见，哪些版本不可见。
- [Row/Column Store](https://source.wiredtiger.com/develop/arch-row-column.html): MongoDB 使用 WiredTiger 引擎的 row store 模式。
- [Cache](https://source.wiredtiger.com/develop/arch-cache.html): WiredTiger 引擎使用内存来缓存最近访问和修改的 page，当 cache 使用率超过设定的阈值时，会通过 eviction 和 checkpoint 操作进行清理。
- [Eviction](https://source.wiredtiger.com/develop/arch-eviction.html): 当 Cache 使用率或者脏页比率达到设定的阈值后，会通过 eviction 流程选择合适的 page 刷到磁盘中，并释放内存空间。
- [History Store](https://source.wiredtiger.com/develop/arch-hs.html): 在磁盘上维护数据的历史版本信息，这些历史版本信息可能还会被使用，比如一个长耗时的事务。在 4.0 版本中也叫 Look Aside Table，4.2 之后的版本统计叫 History Store, 使用 WiredTigerHS.wt 这个特殊的表进行持久化存储。个人理解有点类似 InnoDB 中的 undo log.
- [Block Manager](https://source.wiredtiger.com/develop/arch-block.html): 内存中的 page 对应到磁盘上就是一个 block，因此 block manager 就是用来管理磁盘文件的，提供了文件的读写，extend, truncate 等接口。
- [Logging](https://source.wiredtiger.com/develop/arch-logging.html): 和 WAL(Write ahead log)、journal、redo log 等名词都是一个意思，保证了事务的持久性。在出现异常宕机时，可以通过 checkpoint + WAL 的机制将数据库恢复到最近时间的一致性状态。
- [File System & OS interface](https://source.wiredtiger.com/develop/arch-fs-os.html): 提供了跨平台（Linux/Windows 等）的文件操作接口和系统操作接口。

# 4. WiredTiger 引擎中的数据组织
创建一个 4.0 版本的副本集实例并初始化配置，然后在数据库 db1 下创建表 coll1 并插入数据。可以在 `dbPath` 目录下看到如下目录和文件：   
- admin 目录：自带的系统库 admin, 其中包含了 system.users 表存储user 信息， system.roles表存储用户自建的 role 信息，system.version 存储当前使用的协议版本号。非常不建议用户在 admin 库下建表存储业务数据，由于 [SERVER-16092](https://jira.mongodb.org/browse/SERVER-16092) 的原因，admin 库下无法实现并发修改，所有对 admin 库的修改操作都只能串行执行。这个限制直到 5.0 版本才放开，参考 [SERVER-48878](https://jira.mongodb.org/browse/SERVER-48878) .
- config 目录：自带的系统库 config,  其中包含了 system.sessions 表存储 session 信息，transactions 表存储事务状态信息，对于分片集群来说，还会存储路由表信息。
- local 目录：自带的系统库 local， 其中 system.replset 表会存储副本集配置信息，oplog.rs 表会存储 oplog. 在设计上， local 数据库是每个 mongod 节点私有的，不会参与主从复制，因此也千万不要在 local 下存储业务数据。
- journal 目录：存储引擎的 WAL（Write Ahead Log）.
- diagnostic.data 目录：mongod 每秒会将节点的进行状态转储到该目录下的文件中，可以用于故障定位和问题分析。更多细节可以参考官方文档中关于[Full-Time Diagnostic Data Capture](https://github.com/mongodb/mongo/blob/master/src/mongo/db/ftdc/README.md)的描述
- db1 目录：用户自己创建的数据库。如果配置文件中将 storage.directoryPerDB 设置为 true, 则每次创建的数据库都会有独立的目录。该数据库下创建的表和索引都会存储在这个目录中，比如 collection-24-5744306567866307342.wt 是 coll1 的表文件，index-25-5744306567866307342.wt 是 coll1 表 _id 索引对应的索引文件。
- _mdb_catalog.wt 文件：记录了 MongoDB 中库表的属性信息，比如 db1.coll1 表对应的哪个表文件，包含了哪些索引以及对应的索引文件又是哪些等。假设在 db1 库下创建了 coll1 表，并创建了 "a" 字段的索引，则 _mdb_catalog.wt 中会记录如下信息：
  ```
  {
      "md": {
          "ns": "db1.coll1",
          "options": {
              "uuid": {
                  "$binary": {
                      "base64": "IW2WNwG9TbW0T3eUOO8ClA==",
                      "subType": "04"
                  }
              }
          },
          "indexes": [
              {
                  "spec": {
                      "v": {
                          "$numberInt": "2"
                      },
                      "key": {
                          "_id": {
                              "$numberInt": "1"
                          }
                      },
                      "name": "_id_",
                      "ns": "db1.coll1"
                  },
                  "ready": true,
                  "multikey": false,
                  "multikeyPaths": {
                      "_id": {
                          "$binary": {
                              "base64": "AA==",
                              "subType": "00"
                          }
                      }
                  },
                  "head": {
                      "$numberLong": "0"
                  },
                  "prefix": {
                      "$numberLong": "-1"
                  },
                  "backgroundSecondary": false,
                  "runTwoPhaseBuild": false,
                  "versionOfBuild": {
                      "$numberLong": "1"
                  }
              },
              {
                  "spec": {
                      "v": {
                          "$numberInt": "2"
                      },
                      "key": {
                          "a": "hashed"
                      },
                      "name": "a_hashed",
                      "ns": "db1.coll1"
                  },
                  "ready": true,
                  "multikey": false,
                  "head": {
                      "$numberLong": "0"
                  },
                  "prefix": {
                      "$numberLong": "-1"
                  },
                  "backgroundSecondary": false,
                  "runTwoPhaseBuild": false,
                  "versionOfBuild": {
                      "$numberLong": "1"
                  }
              }
          ],
          "prefix": {
              "$numberLong": "-1"
          }
      },
      "idxIdent": {
          "_id_": "db1/index-33--3130855521305933865",
          "a_hashed": "db1/index-34--3130855521305933865"
      },
      "ns": "db1.coll1",
      "ident": "db1/collection-32--3130855521305933865"
  }
  ```

- mongod.lock：锁文件，存储 mongod 进程的 pid.
- mongod.pid: 当前 mongod 进程的 pid.
- sizeStorer.wt: MongoDB 中每个表的文档条数和逻辑大小。通过单独存储表的文档条数和逻辑大小，可以大大加快 count 和 stats 命令的执行速度，避免全表扫描。
- storage.bson：存储配置，比如使用的哪个存储引擎，是否有开启 directoryPerDB 参数等。举例如下：
  ```
  [root@VM-24-2-opencloudos db]# bsondump storage.bson 
  {"storage":{"engine":"wiredTiger","options":{"directoryPerDB":true,"directoryForIndexes":false,"groupCollections":false}}}
  ```
- WiredTiger：文本文件，当前运行的 Wirediger 版本信息。举例如下：
  ```
  [root@VM-24-2-opencloudos db]# cat WiredTiger
  WiredTiger
  WiredTiger 3.3.0: (March 20, 2020)
  ```
- WiredTigerLAS.wt：WiredTiger 引擎中存储数据的历史版本信息，在 4.2 版本之后统一改为了 History Store. 详细的存储格式会放在 4.1 章节进行介绍。
- WiredTiger.lock：锁文件。
- WiredTiger.wt：WiredTiger 引擎的元数据表，存储了每个 Wirediger 表对应的配置属性、checkpoint 等信息。假设有一个 "access" 表，则 WiredTiger.wt 中会记录如下信息(对应的代码放在附录中)：
  |Key|Value|备注|
  |:--|:--|:--|
  |colgroup:access|app_metadata=,collator=,columns=,source="file:access.wt",type=file|说明资源的类型|
  |file:access.wt|access_pattern_hint=none,allocation_size=4KB,app_metadata=,<br>assert=(commit_timestamp=none,durable_timestamp=none,<br>read_timestamp=none),block_allocation=best,block_compressor=,<br>cache_resident=false,checksum=uncompressed,collator=<br>,columns=,dictionary=0,encryption=(keyid=,name=),format=btree,<br>huffman_key=,huffman_value=,id=2,ignore_in_memory_cache_size=false,<br>internal_item_max=0,internal_key_max=0,internal_key_truncate=true,<br>internal_page_max=4KB,key_format=S,key_gap=10,leaf_item_max=0,<br>leaf_key_max=0,leaf_page_max=32KB,leaf_value_max=0,log=(enabled=true),<br>memory_page_image_max=0,memory_page_max=5MB,os_cache_dirty_max=0,<br>os_cache_max=0,prefix_compression=false,prefix_compression_min=4,<br>split_deepen_min_child=0,split_deepen_per_child=0,split_pct=90,value_format=S,<br>version=(major=1,minor=1),checkpoint=(WiredTigerCheckpoint.1=<br>(addr="018181e464e3ea7d8281e41546bd168381e46bce62af808080e22fc0cfc0",<br>order=1,time=1727512119,size=8192,newest_durable_ts=0,oldest_start_ts=0,<br>oldest_start_txn=0,newest_stop_ts=-1,newest_stop_txn=-11,write_gen=2)),<br>checkpoint_backup_info=,checkpoint_lsn=(4294967295,2147483647)|描述了建表的参数，和表文件的 checkpoint 信息<br>其中 checkpoint 信息中值得描述的是 checkpoint.addr，这是用 hex 编码的字符串，里面使用变长整型存储了 root_page、alloc_list、avail_list、discard_list 和 fileSize 信息，相关流程可以参考 [__wt_meta_ckptlist_to_meta](https://github.com/mongodb/mongo/blob/r4.2.25/src/third_party/wiredtiger/src/meta/meta_ckpt.c#L740) 和 [__wt_block_checkpoint_load](https://github.com/mongodb/mongo/blob/r4.2.25/src/third_party/wiredtiger/src/block/block_ckpt.c#L39)。<br>而 checkpoint_lsn 则记录了对应的 journal 日志的 file+offset|
  |table:access|app_metadata=,colgroups=,collator=,columns=,key_format=S,value_format=S|说明表的 K、V 类型都是 String|
- WiredTiger.turtle：文本信息，存储 WiredTiger.wt 元数据表的 checkpoint 信息。举例如下：
```
WiredTiger version string
WiredTiger 3.2.2: (August 28, 2019)
WiredTiger version
major=3,minor=2,patch=2
file:WiredTiger.wt
access_pattern_hint=none,allocation_size=4KB,app_metadata=,assert=(commit_timestamp=none,durable_timestamp=none,read_timestamp=none),block_allocation=best,block_compressor=,cache_resident=false,checksum=uncompressed,collator=,columns=,dictionary=0,encryption=(keyid=,name=),format=btree,huffman_key=,huffman_value=,id=0,ignore_in_memory_cache_size=false,internal_item_max=0,internal_key_max=0,internal_key_truncate=true,internal_page_max=4KB,key_format=S,key_gap=10,leaf_item_max=0,leaf_key_max=0,leaf_page_max=32KB,leaf_value_max=0,log=(enabled=true),memory_page_image_max=0,memory_page_max=5MB,os_cache_dirty_max=0,os_cache_max=0,prefix_compression=false,prefix_compression_min=4,split_deepen_min_child=0,split_deepen_per_child=0,split_pct=90,value_format=S,version=(major=1,minor=1),checkpoint=(WiredTigerCheckpoint.95364=(addr="018081e41de13b488181e423dffcd88281e4f901b406808080e3020fc0e2dfc0",order=95364,time=1727511146,size=69632,newest_durable_ts=0,oldest_start_ts=0,oldest_start_txn=0,newest_stop_ts=-1,newest_stop_txn=-11,write_gen=404307)),checkpoint_backup_info=,checkpoint_lsn=(4294967295,2147483647)
```

---

了解了基本的文件组织架构之后，下面看看一个 WT 表是如何读写的，在内存和磁盘上的表现形式如何。   
4.0 版本的架构如下（高版本引擎在细节上会有些区别，但是整体架构一样）：

<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/495e1010-1644-4530-993f-89dab7e0d438" width=800>
</p>


对于每个表来说：
1. 在内存中是一个 btree 结构，其中只有 leaf page 存储具体数据。存储的是未压缩未加密的原始数据。   
2. 在磁盘上对应了一个 xxx.wt 文件，每个 page 在磁盘上的表现形式为一个 block，并默认按照 4KB 对齐。每个 block 可以由一个 extent(offset+length) 进行标志。磁盘上存储的是压缩加密后的数据，因此在加载到内存中时需要进行转换。磁盘文件中的 block 有不同的状态，通过 allocation + available + discard 共 3 个 skiplist 进行维护。   
3. 每个表都有对应的 checkpoint 信息，集中存放在 WiredTiger.wt 这个特殊的元数据表中，checkpoint 信息包含了 root page 的位置，对应的文件长度，block skiplist 信息。checkpoint 信息可以方便节点重启时快速找到一致性点。   

# 5. 细说变长页：为什么 WiredTiger 引擎更适合文档数据库
相比其他常见的数据库存储引擎，比如 MySQL 的 InnoDB 和 PostgreSQL 的 Heap 引擎，WiredTiger 在数据管理机制上有着自己鲜明的特点。     

## “写时复制” VS “原地更新”
以 PG 为代表的数据库，如果页内的数据发生了变化，在写回时，仍然写入到磁盘上之前的位置。
但是 MongoDB 中，这个页会写入到一个新空闲位置（寻找空闲位置的算法大多是 best-fit, 极少数 first-fit）。    

<架构图>

MongoDB 这样做的好处：    
- 避免半页写问题。不需要 WAL 中的 Full Page Write, 或者 MySQL 中的 double-write buffer 机制;
- 故障恢复时，既能快速回滚到写入前的状态（旧 block），又能快速恢复到写入后的状态（旧 block + wal）；
- 能很好地**支持变长页**；

这样做的代价：
- 空间膨胀。旧 block 会被标记为 "discard" 状态，但是还不能马上重用。只有等到下一个 checkpoint 做完之后，旧 block 才会从 "discard" 转换成 "available" 状态，真正变成可重用的逻辑删除状态。而 checkpoint 每隔 1 分钟 1次，**如果这段时间内有很多页面被修改之后刷到磁盘，那么就会有很多的临时空间膨胀。**

## 变长页 VS 定长页
传统的关系型数据库的主流是定长页。比如 MySQL 默认是 16KB，PG 默认是 8KB.    
定长页管理起来非常方便，但是存在一些痛点，比如：    
1. 不好处理压缩；
2. 不好处理大文档。以 PG 为例，默认超过 2KB 的数据会切片后存放到独立的 toast 表中，是的读写性能降低；    

而这些刚好是 MongoDB 使用场景的痛点：JSON 类数据一般都有很大的压缩空间；而且每条 JSON 一般都会比较大，100KB+ 的JSON 在实际业务场景中页屡见不鲜（NoSQL 一般也推荐高内聚低耦合的使用方式）。

MongoDB 采用了变长页的设计，允许页面根据自身存储数据的实际情况进行有限度的伸缩。     
可以观察 MongoDB 中的默认建表参数：    
<createString图>

和本次讨论关系较大的参数有：    
- allocation_size: 4KB. 分配数据块的单元，按照 4KB 的整数倍进行分配，最后写入到磁盘的页也要保证是 4KB 对齐。
- block_allocation: best. 分片空闲块的算法。每个文件上的空闲数据块，会有 2 个 skip-list 来维护，一个 skip-list 按照大小组织，对应了 best-fit 分配算法；另外一个按照 offset 组织，用于 first-fit 算法。 绝大部分场景都是 best-fit，极少场景(比如写 checkpoint 元数据)使用 first-fit.
- block_compressor: snappy. 默认开启 snappy 进行页面级压缩。
- internal_page_max: 4KB. 非叶子节点建议大小。在内存中会根据 非叶子页面大小*当前压缩比 > internal_page_max 决定是否分裂。当前的压缩比是一个实时维护的变量，如果由于各种原因导致最后写入的 非叶子页面大于 4KB，也能在对齐后正常写入。**也就是说，这里的 4KB 只是一个分裂的建议值，防止页面无限制扩张，而不是说写入的页面必须在 4KB 及以内。**
- leaf_page_max: 32KB。叶子节点建议大小。和 internal_page_max 的原理类似，不再赘述。
- leaf_value_max: 64MB。最大的 value 长度，如果超过这个值，需要将 value 放在单独的区域，而且内存 cache 中不缓存 value 的值。由于 MongoDB 的 BSON 最大是 16MB，因此不可能超过这个值。**这个配置，也在侧面反应了 MongoDB 的叶子节点可以很大。**    

变长页带来的弹性，能够很好的解决页面级压缩带来的页大小变化问题，以及超大 JSON 不好存储的问题。    
付出的代价是：    
1. 页面空间管理更加复杂。    
	将数据块划分成了 "allocate", "discard", "available" 3 种状态，并通过 skip-list 进行维护，每次 checkpoint 时也会作为元数据持久化到磁盘。再新的 checkpoint 持久化之后，会涉及到新旧 checkpoint 之间的 “skip-list 合并”。
2. 可能会出现碎片。    
	由于页大小可能动态改变，即使采用 best-fit 算法也可能造成碎片：

<碎片示意图>



# 6. 总结
1. MongoDB 通过分层的架构设计将文档模型和存储引擎隔离开，底层 KV 存储引擎不需要理解 BSON 文档模型的细节。  
2. 通过抽象存储引擎接口，实现了插件式存储引擎。通过支持 Memory, Mmap, WiredTiger，RocksDB 等多种存储引擎，使得 MongoDB 能够驾驭更多的业务场景。
3. WiredTiger 存储引擎通过 Btree 实现了高效的数据读写，通过 BlockManager 实现了高效率的磁盘使用，通过 CheckPoint + journal 机制保证了数据安全，通过 MVCC 提供了高效的并发访问。
4. WiredTiger 存储引擎通过变长页的设计，解决了传统定长页的两大痛点：页面压缩和超大文档。而这也是文档数据库非常需要的两大特性。  

# 参考文档
1. http://source.wiredtiger.com/2.0.1/tuning.html
2. https://developer.aliyun.com/article/66848
3. https://www.mongodb.com/blog/post/building-applications-with-mongodbs-pluggable-storage-engines-part-1
4. https://github.com/mongodb-partners/mongo-rocks
5. https://github.com/mongodb/mongo/blob/master/src/mongo/db/ftdc/README.md
6. https://jira.mongodb.org/browse/SERVER-16092
7. https://source.wiredtiger.com/develop/arch-index.html

# 附录
打印 Wiretiger.wt 信息的代码：
```
    /*! [access example meta query] */
    // do a checkpoint first
    error_check(session->checkpoint(session, NULL));
    error_check(session->open_cursor(session, "metadata:", NULL, NULL, &mcursor));
    //  uri is colgroup:access, file:access.wt, table:access
    mcursor->set_key(mcursor, "colgroup:access");
    mcursor->search(mcursor);
    error_check(mcursor->get_value(mcursor, &value));
    printf("Got colgroup:access record:  %s\n", value);
    mcursor->set_key(mcursor, "file:access.wt");
    mcursor->search(mcursor);
    error_check(mcursor->get_value(mcursor, &value));
    printf("Got file:access.wt record: %s\n", value);
    mcursor->set_key(mcursor, "table:access");
    mcursor->search(mcursor);
    error_check(mcursor->get_value(mcursor, &value));
    printf("Got table:access record: %s\n", value);
```
