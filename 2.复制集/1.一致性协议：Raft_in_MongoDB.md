# 1. 导语
MongoDB 使用多个节点组成副本集保证数据高可靠以及服务高可用。对于自动容错的分布式系统来说，如何保证HA、数据一致性、高吞吐、低延迟、协议易理解（易运维）是系统设计的关键。   
一致性协议（consensus algorithm， 共识算法）就是上述问题的解决方案。

# 2. Raft 和 MongoRaft
目前业界比较流行的一致性协议有 Paxos 和 Raft。[《共识协议的技术变迁》](https://mp.weixin.qq.com/s/UY9TPMcuf0O7xS0kuXTcVw)一文中对一致性算法的发展历程进行了非常通俗易懂的描述。   
Paxos 诞生时间比较早，并使用在 Chubby，ZooKeeper 等系统中，但是协议理解起来比较复杂，学习路径有些陡峭。   
Raft 的诞生实践稍晚，相对 Paxos来说非常好理解。从 [Raft 论文](https://raft.github.io/raft.pdf)也可以看出，从设计之初就考虑了协议的通俗易懂，以及如何指导工程落地。在 etcd，redis 等开源项目中都能看到它的身影。   
正如 Raft 论文中所说，Raft 本身还有很多值得优化的地方，比如日志复制的性能问题等。另外，在实际的成功落地过程中，也有很多边界情况需要考虑。所以，业界很多系统使用的 Raft 算法时都是根据实际情况进行优化后的变种。截止目前，在 [raft.github.io](https://raft.github.io) 中登记的使用了Raft算法的开源项目已经有几十个（比如 TiKV, etcd 等），如果算上没有登记的项目（比如 MongoDB） 应该有上百个。

MongoDB 使用的一致性协议也可以看作是 Raft 协议的一个变种，同样使用了强主模式来定序，使用日志复制到大多数节点来提交请求。但是 MongoDB 在落地过程中，相比于原生 Raft 协议还是有不少改进，比如更丰富的节点状态、Pull-Based 复制模型、链式复制、可调一致性、Logless Reconfig 等。    
正如 MongoDB 官方论文[《Fault-Tolerant Replication with Pull-Based Consensus in MongoDB》 ](https://www.usenix.org/conference/nsdi21/presentation/zhou)中所说，MongoDB 选取 Raft 而没有基于 Paxos 很大一定程度上也是基于工程落地方面的考虑。理论上来说，如果基于 Paxos 实现也是没有问题的。   
MongoDB 一致性协议的[发展历史](https://www.usenix.org/system/files/nsdi21_slides_zhou-siyuan.pdf)如下：   
<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/3b514626-b141-45ed-8499-f372c944489f" width=800>
</p>   

- MongoDB 1.0 是刀耕火种的时代，需要手动进行 failover.   
- MongoDB 1.6 引入了自动 failover 机制，但是是基于没有证明的私有协议。
- MongoDB 3.2 基于 Raft 重新改造了一致性协议，进行了 TLA+ 证明。同时在改造过程中保留了 MongoDB自身 的 Pull-Based 日志复制模型、链式复制等诸多优秀特性，使得MongoDB 的一致性协议在正确性、效率、性能等各方面都达到了非常高的标准。   

由于 MongoDB 官方已明确说明一致性协议基于 Raft，而且在 [《Design and Analysis of a Logless Dynamic Reconfiguration Protocol》](https://drops.dagstuhl.de/opus/volltexte/2022/15801/pdf/LIPIcs-OPODIS-2021-26.pdf)论文中更是将配置变更算法起名为 MongoRaftReconfig. 因此，为了描述方便，本文统一将 MongoDB 的一致性协议简称为 MongoRaft。

类比是非常好的学习方法。本文首先从原生 Raft 入手，从 Raft 论文简要了解其设计思想和流程。然后结合 MongoDB内核源码、官方论文以及github wiki，重点介绍 MongoDB 的一致性协议，并说明相比原生 Raft 所作的优化及效果。   
结合 Raft 论文对协议做的模块划分，以及我个人的理解。本文将分如下几个模块阐述：   
1. 节点状态及其变迁规则。
2. 日志复制和提交。
3. 如何选主。
4. 配置变更。
5. 持久化保证。
6. 一致性保证。

# 3. 深入分析 MongoRaft
## 3.1 节点角色
Raft 论文中描述了 3 种节点角色：   
- Leader： 主节点，接收读写请求，承担日志复制，请求提交，心跳探活等任务。
- Follower：从节点，接收日志复制和应用，作为一个完整的数据副本。  
- Candidate：Follower长时间没有感知到主节点时，可以变为 Candidate 并发起选举。   

<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/33003281-72a3-44b5-8b3e-e826c17490e1" width=400>
</p>

此外，论文中也提到了 Follower 节点可以设置 non-voting 属性，不计入大多数提交时的副本数（Learner）。一般在新加入节点时，可以使用这个设置避免数据提交卡顿。  

MongoDB 中包含的节点角色也有主从状态。 其中主节点（Primary）支持读写，从节点（Secondary）支持读（这是和 Raft 不同之处）。另外从节点又[细分](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/repl/member_state.h#L58-L70)为：  
- Secondary：正常状态下的从节点，支持用户读。  
- Rollback：回滚状态。常见的场景是主节点异常重启后变为从节点，需要将没有复制到其他节点的日志回滚掉。  
- Recovering：正在恢复的状态。节点启动后会短暂处于该状态，然后变成 Secondary。如果长期处于该状态，对应的场景是该节点的日志太旧，无法找到合适的节点同步日志，此时一般需要清空数据后重新做全量同步。  
- Startup：刚启动时的状态。  
- Startup2：初始化全量同步的状态。对应的场景是新加入一个空节点（或者手动将某个节点的数据清空），该节点需要通过 Initial Sync 流程全量同步数据。  
- Arbiter：投票节点，不存储数据。  
此外，如果节点异常，会显示 Unknown、Down、Removed(Other) 状态。

另外 MongoDB 中的每个节点可以配置多种属性：   
- Votes：是否有投票权。所谓的“多数派提交”和“多数派选举”中的“多数”，指的就是包含有 votes 属性的节点。一个 MongoDB 副本集中最多只有7 个包含 votes 属性的节点。通过将 votes 属性设置为 0 ，可以加更多的从节点，因此这个属性一般会用在各大云厂商的“只读实例”产品中。  
- Priority：节点被选为主的优先级。priority 设置的数值越高，越会被选举为主。通过将 priority 设置为0，可以避免主节点切换到某些节点。  
- Hidden：是否将从节点隐藏起来不对外提供服务。设置为隐藏后，客户端驱动无法通过 isMaster/Hello 探测命令感知这个节点，因此无法向其发读请求，但是通过 rs.conf() 和 rs.Status() 命令还是能看到其配置。Hidden 节点一般作为备份节点。由于主节点不能被隐藏，因此官方规定 hidden=true 的节点需要 priority=0。  
- BuildIndexes：从节点是否建索引。如果设置为 false， 则不会同步创建索引。理论上可以节省索引占用的存储空间以及维护索引的 CPU 和内存消耗。但是如果从节点被选举为主，此时由于索引缺失可能会导致一些问题，因此官方规定 buildIndexes=false 的节点需要 priority=0，而且不能对已经加入副本集的节点进行动态变更。  
- Tags：为节点打标签，一般用户从节点的请求分流。比如副本集中有 4 个节点，其中 2 个节点硬件配置高，可用于在线服务，则设置这 2 个节点的 tag 为 {"usage":"online"}；另外 2 个节点硬件配置较低，只用户离线特征分析，则设置这 2 个节点的 tag 为 {"usage":"offline"}。用户程序可以根据自身请求特点设置readPreference 说明希望将请求发往 "online" 还是 "offline" 节点。  
- ArbiterOnly：轻量级从节点，不同步和存储数据，只负责选主投票。一般用于解决偶数副本数的场景。比如副本集中只有 2 个节点，必须全部存活才能选出主。加入一个 arbiter 节点之后，副本个数变为 3，可以容忍一个节点失效。  
- SlaveDelay：强行设置某个从节点距离主节点的主从延迟。比如设置 60 秒，则该从节点的最新日志同步时间比主节点要延迟 60 秒或以上。一般用于业务数据快速回滚（比如误删）和测试场景。  

通过状态区分和灵活的配置，MongoDB 能够更好地支持多样化的业务场景。

## 3.2 日志复制
### 3.2.1 Raft 原理
Raft 采用推日志模型，由 leader 节点将日志并发推送到 follower 节点，并应用状态机。整体流程如下图所示：   
<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/6796bf31-33ab-45c2-af62-b194d8cc9ba6" width=400>
</p>
   
（1）Client 往 leader 节点发写请求。   
（2）Leader 节点在本地生成日志，并通过 RPC 将日志并发推送到其他 follower 节点。   
（3）确保当前大多数节点（包括自己）已经完成了日志复制，则将日志应用到自身的状态机（存储引擎）中。   
（4）给 Client 返回提交成功。   
注意 follower 节点的日志应用是异步的。在上述流程结束后，leader 节点会推进自身的 commitIndex，在下一次日志推送以及定期的心跳请求中会携带上 commitIndex 发送给 follower，使得 follower 也将这部分日志应用到状态机。   

日志中包含了 index, term，执行命令等信息，如下所示：   

<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/a8b3e351-9e92-4240-87d6-4ed335759409" width=400>
</p>

每个节点的日志顺序要保证完全一致且不能由空洞，不支持乱序提交。   

### 3.2.2 MongoRaft 原理
MongoDB 使用 oplog 完成主从节点之间的数据同步， oplog 采用递增的混合逻辑时钟作为“序号”（类似 Raft 中的 logIndex），并且会携带 term、库表名、操作类型和命令信息。   
以 4.2.24 内核版本为例，往 db1.coll1 表中插入一条文档 {a:1} 后生成的 oplog 如下：   
>{ "ts" : Timestamp(1698199568, 1), "t" : NumberLong(37), "h" : NumberLong(0), "v" : 2, "op" : "i", "ns" : "db1.coll1", "ui" : UUID("a3e5d8ac-f4b3-4a40-88c9-8b0b6c47b4c4"), "wall" : ISODate("2023-10-25T02:06:08.268Z"), "o" : { "_id" : ObjectId("6538780f07311975afc52751"), "a" : 1 } }

每条 Oplog 中都包含Timestamp 用于定序。Timestamp 由 int32 的秒级时间戳和 int32 的秒内计数器 2 部分组成，合起来可以用 int64 表示。该 Timestamp 由[主节点执行写请求时确定](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/ops/write_ops_exec.cpp#L294-L314)，从节点不作任何修改。
#### 日志同步模型
MongoRaft 中的日志同步采用了从节点拉日志的模型（Pull-Based)，和 Raft 的主节点推日志模型有着显著区别。    
以4.2.24内核版本为例，MongoDB节点启动后会加载配置，并最终调用[ReplicationCoordinatorExternalStateImpl::startSteadyStateReplication()](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/repl/replication_coordinator_external_state_impl.cpp#L207) 启动复制流程，其中会涉及如下线程：   
1. **BackgroundSync**：一直向源节点发起 find+getmore 请求，并将获取的 oplog 数组存放到 **oplogBuffer** （本质上是一个 OplogBufferBlockingQueue, 即 std::queue\<BSONObj\>，最大 [256MB](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/repl/oplog_buffer_blocking_queue.cpp#L40)）。   
2. **ReplBatcher**(参考 SyncTail::OpQueueBatcher) ：由 OplogApplier 启动的后台线程，这个线程不断消费 oplogBuffer 中的 oplog 日志，并传递到 **OpQueue**（本质上是一个 std::vector，也叫做一个 oplogBatch，最多 [5000 条](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/repl/repl_server_parameters.idl#L246)和 [100MB](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/repl/repl_server_parameters.idl#L258)）。在消费 oplogBuffer 的过程中，会对每一条 oplog 进行版本检查，并通过类型设置栅栏。比如对一个表进行了 ”文档插入“后又进行了“删表”，则“删表”操作不能和前面的“文档插入”放在一个 OpQueue 中。也就是要保证 OpQueue 是可以并发回放的。   
3. **OplogApplier**：管理  syncTail 对象，并在启动时调用 syncTail::oplogApplication 方法一直获取 **OpQueue** 中的 oplog 数组进行回放（**MultiApply**），回放流程首先按照{表，文档_id} 将 oplog 数组再哈希到多个数组中（参考 [fillWriterVectors](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/repl/sync_tail.cpp#L1163)），然后提交给 writerPool 线程池进行并发回放([_applyOps](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/repl/sync_tail.cpp#L1308))。   
4. **WriterPool 线程池**：真正执行 oplog 回放的线程，默认的最大线程个数为min([16](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/repl/repl_server_parameters.idl#L220) ，[机器的核数*2](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/repl/oplog_applier.cpp#L61) ），线程名字为 repl-writer-worker-*.   
5. **ApplyBatchFinalizerForJournal**:  oplogApplier 创建出来的守护线程，主要的作用是在应用完一批 oplog 并更新了 lastAppliedOpTime 之后，通知 syncSourceFeedback 主动给上游节点上报。另外还会等待这批 oplog 的请求刷完 journal 更新了 lastDurableOpTime 之后，通知 syncSourceFeedback 给上游节点上报。   
6. **SyncSourceFeedback**：给上游节点上报同步进度的守护线程。一般由 oplog 回放流程结束后通过条件变量触发，如果长时间没有oplog 应用，也会在 5秒后主动上报一次（起到探活的作用）。   

<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/ef2021bb-5c0a-4796-b69d-124d179b3d5e" width=1000>
</p>

从架构图中，可以看到 MongoDB 复制模型具有以下明显特征和优化点：   
- **主节点先应用状态机（写数据）并同时生成 oplog**，然后再将日志复制到其他节点。对于原生 Raft 来说，是先复制日志到其他节点，然后再提交。   
- 从节点**拉日志**。而原生 Raft 是主节点推日志到从节点，从节点被动接收。拉日志模型带来一个明显的好处就是可以形成链式复制结构，即从节点不一定非要到主节点上复制，使得复制链路更加灵活，容错性更高。另外对于一些跨地域的复制场景，链式复制能够明显降低网络带宽成本。比如 5 个节点的副本集，S1 是主节点， S1 和 S2 在北京，S3/S4/S5 在广州，则可能的复制链路为 S1->S2->S3->S4/S5，即 S4 和 S5 不需要跨地域复制。   
- 从节点**异步反馈复制进度**。而原生 Raft 是主节点在推送日志的 RPC 响应中感知日志的复制进度。而在 MongoDB 中，由于链式复制的存在，syncSourceFeedback 流程除了上报自己的进度之外，还会上报其他已知节点的进度。比如 3  节点的副本集，复制链路是 S1->S2->S3, S3 给 S2 上报进度，S2 给 S1 上报进度时也会带上 S3 的进度信息。如果不这样做，S1 只能通过心跳感知 S3 的复制进度，效率会大打折扣。另外，即使S2 一段时间没有日志推进，但是 S3 有日志推进，S3 给 S2 上报进度后，也会触发  S2 给 S1 进行一次上报。syncSourceFeedback 还有另外一个作用就是探活，比如 S1 和 S3 之间的网络有问题，心跳不通，但是 S1 能够通过 S2 的主动上报来感知 S3 的存活状态。   
- 使用**多线程**进行**流程解耦**。日志的拉取、回放和进度反馈都是不同的线程完成的，并通过内存队列和条件变量等数据结构进行多线程通信。解耦后各个子流程之间不会直接相互影响，尽可能提升拉日志、回放核心流程的效率。   
- **并发乱序回放oplog**。原生 Raft 规定 Follower 节点严格按照日志顺序进行回放，但是在 MongoDB 中日志是严格按照日志时间分 Batch进行拉取的，但是在一个 Batch 内部的回放是哈希打散之后分发给线程池并发完成。并发回放是必须的，因为 MongoDB 主节点提供并发写入，如果从节点只能严格按照时间顺序串行回放，性能肯定跟不上，导致主从延迟越来越大。但是并发又会带来一些问题，比如是否所有的操作都支持并发？按照什么规则进行拆分能保证正确性？并发操作是否会和唯一索引等约束条件产生冲突？并发带来的乱序是否会导致用户读到不一致的数据，MongoDB 是如何解决的呢？   
- **更灵活的复制状态**。原生 Raft 中 Follower 节点在日志持久化后才认为复制成功，但是在 MongoDB 中使用 lastAppliedOpTime 和 lastDurableOpTime 进行了区分。其中 lastAppliedOpTime 表示日志已经应用并在本地生成了 oplog，但是还没有异步写 jounal(WAL)，而 lastDurableOpTime 表示已经写  journal 的持久化状态。用户可以自定义写入级别，主节点将根据用户定义的写入级别依据 lastAppliedOpTime 或者 lastDurableOpTime 来更新提交状态。总结来说，MongoDB 将选择权交给用户，可以在请求延迟和数据持久性上做权衡。   

>注：在 [8.0 内核](https://www.mongodb.com/zh-cn/docs/manual/release-notes/8.0/#majority-write-concern) 对 majority 提交流程进行了一些变更：从节点在复制完 oplog 即反馈进度，而不是等待本地应用之后再反馈。这在一定程度上减少了 majority 提交的延迟。

#### 同步源节点的选择
如果用户关闭了副本集的链式复制功能，则所有节点只能到主节点同步。   
如果用户打开了链式复制功能（默认打开），则：   
- （默认）**自动选择**同步源节点。从节点根据自己的拓扑信息，选择一个 opTime 比自己新的，而且网络距离最近的节点作为同步源。   
- **手动选择**同步源节点。用户可以通过 replSetSyncFrom 命令，手动给一个从节点指定同步源。通过手动指定同步源，用户可以明确的控制整个复制链路。   

链式复制举例如下，B 从 A 同步，C和 D 从 B 同步。复制链路包括 拉oplog 流程（粗实线）和进度反馈流程（细实线），心跳流程是虚线部分。因此，最终副本集中各个节点的通信是**复制链路+心跳链路相结合**的形式。复制链路通信更频繁，是节点状态同步的核心路径（oplogFetch 和 updatePosition 命令的元数据中会携带节点的状态信息）而且由于链式的存在，能够很好容忍局部网络故障。心跳通信频率更低（默认 2 秒 1 次），能够作为节点状态同步非常好的补充，并且为网络故障时的同步源切换流程提供参考。两者相辅相成。

<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/95fb398d-7558-48bc-a3bc-4190c3abda62" width=400>
</p>

#### 并发回放如何保证正确性
**如何并发：按照什么规则进行 Hash?**   
MongoDB 内部采用了 2 级 Hash 的策略，参考[SyncTail::_fillWriterVectors](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/repl/sync_tail.cpp#L1163):   
1. 首先按照表（db.collection）进行 Hash，因为不同表之间的修改没有直接关联。   
2. 其次再按照文档的 _id 进行 Hash，因为不同文档之间的修改也没有直接关联。   

同 1 个文档（_id 相同）的 oplog 会Hash 到一个桶中，并保持原始的时间顺序。   
当然也有一些特殊情况不会进行 2 级 hash。比如固定集合（capped collection）要满足“先插入先删除”的特性，对不同文档的插入顺序有严格要求，因此不能按 _id Hash 后乱序插入，也[不能进行 groupInsert](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/repl/applier_helpers.cpp#L91) 的回放优化。

**DDL 操作如何处理？**   
Oplog 中除了常见的 CRUD 操作之外，还有例如删库删表等 DDL 操作。这些 DDL 操作无法和 CRUD 操作并行回放，例如对表 A 先插入一条数据，然后再删表。如果将这 2 条 oplog 并发回放的话，可能会出现先删表再插入数据，显然是不合适的。   
从节点的 ReplBatcher 线程调用 [OplogApplier::getNextApplierBatch](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/repl/oplog_applier.cpp#L244) 方法将 oplogBuffer 队列中的 oplog 传递到 opQueue 中，opQueue 可以认为是一个批量并发回放的 batch。getNextApplierBatch 方法会甄别每条 oplog 的类型，如果是 DDL 操作，会将这些操作单独作为一个 batch 回放，具体可以参考 [mustProcessStandalone](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/repl/oplog_applier.cpp#L211) 的处理逻辑。

**如何并发回放事务？**   
对于 4.0 内核版本的副本集事务，是比较好处理的。事务在提交时，会将所有操作通过一个数组打包好，并放在一条 oplog 中。这种方式虽然对事务大小有 [16MB 限制](https://www.mongodb.com/docs/manual/core/transactions-production-consideration/#oplog-size-limit)，但是 secondary 节点在回放时非常好处理。回放时只需要将事务 oplog 中的数组解开，得到具体的写操作后进行 hash 就能并发回放了。本质上和普通的写操作并无太大区别。   

但是，从 4.2 内核版本开始，出现了很大的变化。   
4.2 版本对副本集事务进行了扩展，并且支持了基于 2 阶段提交的分布式事务。对应到 oplog 的实现方式上，出现了 2 个重大区别：   
1. 事务的 oplog 不再局限于 1 条，可能存在**多条**。因此，4.2 版本的事务大小也不再受限于 BSON 的 16MB 限制。   
2. 分布式事务包含 prepare 和 commit/abort **2 个阶段**，都有各自的 oplog。   

Oplog 的变化给回放流程带来了新的挑战，主要有：
1. 如何保证**原子性**。假如 1 个事务有 4 条 oplog，secondary 节点在第 1 次拉取时拉到了 2 条 oplog（每次拉 oplog 的大小也是有限制的，可能刚好拉到前 2 条 oplog 时就达到了大小限制，后面 2 条只能下一次再拉），那么前面 2 条 oplog 能被回放吗，只回放事务的部分操作，岂不是破坏了事务的原子性？另外，第 2 次将事务的后 2 条 oplog 也拉过来了，如何找到这个事务前 2 条 oplog 呢？   
2. 如何进行 **2 阶段事务**的并发。为了回放的正确性，要保证同一事务的 prepare 在 commit/abort 之前执行。Prepare 中的操作也不能直接解析成普通的写操作进行回放，因为此时还不确定事务能否成功提交。另外，Commit/abort 操作也不能和普通的写操作并发回放。   

我们通过一个副本集事务的 oplog 回放流程，来探讨如何解决回放的原子性问题。   
假设一个很大的副本集事务依次生成了 4 条 oplog，时间戳分别是 t1、t2、t3、t4。Secondary 节点第 1 批拉取到了 t1 和t2, 第 2 批拉取到了t3 和 t4。如下图所示：   

<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/1bef164d-2d70-4baf-b27c-10e6231db278" width=800>
</p>

先看第 1 批的回放流程：   
1. Secondary节点先将拉取到的所有 oplog 都写到自己的 oplog 表中（异步执行，**包括 t1 和 t2**）。   
2. 执行 oplog 的 hash 流程：   
2.1. 处理到 t1, 发现是事务的一部分 oplog，即 partial = true。先不回放，而是将这条 oplog 放在[内存 cache](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/repl/sync_tail.cpp#L1199-L1212) 中。这里是为了方便后续回溯，而 第 1 步的oplog 写表异步任务可能还没完成，因此必须引入内存 cache。   
2.2. 处理到 t2，还是 partial = true, 也放到内存 cache 中。   
3. 等待第 1 步写 oplog 表的异步任务执行完，然后将第 1 批 oplog 全部回放完。在此过程中，t1 和 t2 中的操作并没有被执行，而且内存 cache 也被清空。    

接着再回放第 2 批：   
1. Secondary节点先将拉取到的所有 oplog 都写到自己的 oplog 表中（异步执行）。   
2. 执行 oplog 的 hash 流程：     
2.1. 处理到 t3，发现 partial = true, 放到内存 cache中。   
2.2. 处理到 t4，发现是事务的最后一条 oplog，要准备真正执行回放了。则通过 sessionId 找 cache 中的 t3，然后通过 t3->[prevOpTime](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/repl/transaction_oplog_application.cpp#L269)字段找到上一条 oplog 的时间戳是 t2, 并使用这个时间戳到[本地 oplog 表](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/transaction_history_iterator.cpp#L59-L61)中把 t2 的完整 oplog 找到。接着用同样的方法找 t2->prevOpTime，得到t1 的完整 oplog。最后发现 t1->prevOpTime = null，此时 oplog 的回溯流程结束。得到事务的完整 oplog： t1、t2、t3、t4。   
2.3. 在回溯 oplog 的过程中，也会将上述 4 条 oplog 中包含的操作都解析出来，放在一个数组中并且[排好序](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/repl/transaction_oplog_application.cpp#L290-L305)。然后将这个数组 hash 到多个并发线程中。   
3. 等待第 1 步写 oplog 表的异步任务执行完，然后执行这批 oplog 的操作。由于是在一个 batch 中回放完成的，因此能够保证事务回放的原子性。   

我们再通过一个例子，看看如何处理 2 阶段事务。   
假设事务包含 5 条 oplog，前 4 条是 prepare 阶段产生的，最后 1 条是 commit。如下图所示：   

<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/3b14c204-ca2e-4aef-ad22-ff5a63d1771a" width=800>
</p>

第 1 阶段的回放流程和上述副本集事务例子的第 1 阶段相同，不再赘述。   
第2 阶段流程和副本集事务有些区别：   
1. 处理到 t3，由于 partial = true, 先放到 cache 中。   
2. 由于 t4 是 prepare = true 的 oplog， 会独占一个 oplog batch，不和其他操作共享。处理到 t4 时，会从 oplog 表中把  t3、t2、t1 都回溯出来，然后根据这些 oplog 中的操作执行 [prepareTransaction](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/repl/transaction_oplog_application.cpp#L458)。注意这里是 prepareTransaction，而不是hash成普通的写操作执行，因为此时还不确定这些事务能否提交。   
3. 处理到 t5，这是 1 条 commit  oplog，会独占一个 oplog batch。Secondary 节点根据  commit oplog 中携带的 txnId sessionId 信息，找到之前 prepare 的上下文，然后[执行 commit](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/repl/transaction_oplog_application.cpp#L163)。   


**回放过程中的不一致数据能读到么？**   
假设在主节点上依次插入 3 条数据 {x:1} -> {y:2} -> {z:3}，在从节点上并发回放的先后顺序可能是 {z:3} -> {y:2} -> {x:1}。如果用户在从节点上读取数据，显然是无法满足一致性要求的。   
因此，MongoDB 通过一些机制避免了用户读取正在回放中的数据：   
- 在 3.6 及之前的版本，用户对从节点的读取操作需要先获得 PBWM (Parallel Batch Writer Mode)锁。由于 oplog 的批量回放也需要先获取 PBWM 锁，因此能够互斥。但是这种方式效率很低，导致用户到从节点的请求经常卡顿，而且也会影响从节点的同步速度。   
- 在 4.0 及之后的版本，用户到从节点的读取操作变为直接读一致性快照（不依赖 PBWM锁）。所谓快照，就是读取不超过 lastAppliedOpTime 之后修改的数据版本。而每次 oplog 批量回放完成之后，都[会主动更新 lastAppliedOpTime](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/repl/sync_tail.cpp#L865)。**这样保证了用户在从节点上能够尽快获取尽可能新的一致性数据。**   

示意图如下：   
<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/409df3b3-3b55-4800-b6dd-b8e4823d9234" width=800>
</p>


**乱序回放是否会打乱从节点的  oplog 顺序，导致主节点和从节点的 oplog 顺序不一致？**     
假设下图的场景：  
<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/177d4c62-66d1-492f-93c6-f0c513dbed03" width=600>
</p>
   
(a) S1 是主节点，最新写入日志 3、4、5。S2 是从 S1 同步，日志进度为 1、2。S3 从 S2 同步，日志进度为 1。   
(b) S2 从 S1 拉取日志 3、4、5，然后批量乱序回放，本地形成日志 5、4、3 。   
(c) S3 从 S2 拉取日志 2、5、4，然后批量乱序回放，本地形成日志 4、2、5。批量回放结束后，4、2、5 的修改变为对用户可见，**此时用户会读取到不包含 3 的不一致数据！**  

上述假设场景的根本问题是日志回放顺序和生成顺序绑定，导致的日志乱序。MongoDB 通过以下机制保证了乱序回放情况下的日志严格有序：
- **每条 oplog 都会携带时间戳**（int64），而且这个时间戳只能是主节点生成，从节点只能遵守不能修改。这样保证 oplog 在副本集中每个节点的时间顺序都是一样的。   
- **Oplog 在 MongoDB 中严格按照时间戳存储和查询**，并通过可见性判断机制（只有 oplog batch 回放完，[才对外可见](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/repl/sync_tail.cpp#L876)）来保证从节点拉取的 oplog 不会出现空洞。   
- 另外，**从节点上写 oplog 和回放 oplog 的流程是分开的**，不像主节点将写用户库表和oplog放在一起提交。在从节点上，线程池[先并发写 oplog](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/repl/sync_tail.cpp#L1369)，再[并发回放 oplog](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/repl/sync_tail.cpp#L1410)。   

上述机制保证了从节点上的日志回放顺序不影响日志生成顺序，副本集中每个节点的日志顺序完全相同。所以，MongoDB 中真正的复制流程如下所示：   
<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/9d257a15-5d08-405c-89ac-9787a15abb22" width=800>
</p>

(a) S1 是主节点，最新写入日志 3、4、5。S2 是从 S1 同步，日志进度为 1、2。S3 从 S2 同步，日志进度为 1。    
(b) S2 从 S1 拉取日志 3、4、5，虽然乱序回放时数据写入的先后顺序时 5->4->3， 但是日志的顺序仍然保持 3->4->5，和主节点一样。    
(c) S3 从 S2 拉取日志 2，3，4，乱序回放插入数据 4->2->3。回放完之后用户可以读取 S3 的数据为 1，2，3，4，虽然数据不是最新的，但是不会读到有空洞的非一致性数据。    


**如何处理乱序打破唯一性约束的问题？**   
乱序回放可能会破坏唯一索引的约束。以下图为例，在某个表上按照 "a" 字段创建了唯一索引。在主节点上对这表依次进行了 插入 {_id:1, a:1}、删除{_id:1, a:1} 和 插入{_id:2, a:1} 的操作。随后生成的 oplog 在从节点上按照 _id 哈希之后并发回放，则有可能 {_id:1, a:1} 和 {_id:2, a:1} 的插入操作先执行，此时就破坏了唯一索引的约束，导致错误。    
<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/f023bf96-0ec6-4362-bcba-c12472792cde" width=600>
</p>
   
为了解决索引唯一性的问题，MongoDB 在从节点 oplog 回放阶段**临时放开了唯一性约束**，具体可以参考 [IndexCatalogImpl::_indexFilteredRecords](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/catalog/index_catalog_impl.cpp#L1334)、[IndexCatalogImpl::_updateRecord](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/catalog/index_catalog_impl.cpp#L1395) 和[IndexCatalogImpl::_unindexKeys](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/catalog/index_catalog_impl.cpp#L1441) 中如何调用 [prepareInsertDeleteOptions](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/catalog/index_catalog_impl.cpp#L1649) 生成操作参数。在 prepareInsertDeleteOptions 的逻辑中，对于从节点的索引操作放开了约束，即使是唯一索引也允许 duplicateKey 的存在，即设置 dupsAllowed = true。这个设置会使得操作存储引擎时[跳过索引的唯一性检查](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/storage/wiredtiger/wiredtiger_index.cpp#L1519-L1549)。   
跳过唯一性检查，意味着 oplog 回放阶段的唯一索引可能存在多个相同 key 的情况，但是在回放完成后能仍然能满足唯一性。基于前面的分析，在 **oplog 回放期间这部分不一致数据是用户读不到的**，因此不会对正确性产生影响。

>备注：  
业界广泛使用的 DTS(数据传输服务，将一个 MongoDB 集群的数据同步到另一个 MongoDB 集群) 也会遇到索引的唯一性约束问题。  
一些云产商为了实现方便，在检测到表有唯一索引后，会将这个表的数据同步变为串行（不再按_id 并发），单表的同步性能会下降。  
也有一些云产商采用了一些冲突检测机制尽量保留并发，比如 MongoShake 在 oplog 日志中增加了 uk 字段来说明表有哪些唯一索引以及本次改动的 key。如果有 2 条 oplog 改动了相同的 key，则不能放在一个 batch 中回放。如果改的 key 不同，则还是能放在同一个 batch 中并发回放。  
总体来说，MongoDB 内核中主从同步的冲突处理策略是最易理解，也是性能最强的，当然这也得益于内核代码本身就能进行更加精细化的控制。  

#### 日志同步性能探究
**GetMore 拉日志的困境**       
<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/59ffbd58-6f8e-4936-bb97-e6512346a7e4" width=300>
</p>
  
虽然MongoDB 采用了将**日志拉取和回放操作分离**的设计，尽可能提升拉取日志的速度，但是本质上还是一个单连接模型，网络延迟仍然不能忽视。   
以上图为例，假设主节点和从节点之间的网络距离是 50 ms（跨地域部署的场景，图中虚线框所示），硬件配置无限高使得请求和响应的处理几乎不耗时，已知一批 oplog 的最大载荷是 16MB，则可以推算该场景下的极限同步速度：   
a) 0->50ms 时刻, 拉日志请求（find/getmore）由从节点发给主节点。   
b) 主节点准备好日志，封装好 response，极限情况下足够快，不耗时。   
c) 50->100ms 时刻, Response 由主节点发给从节点，极限情况下带宽无限。   
d) 从节点处理响应，将 oplog 放到内存队列，然后开始拉下一批，极限情况下不耗时。   
这样主从的极限同步速度为 16MB/100ms = 160MB/s。如果主节点上有大量写入，生成oplog日志的速度超过了 160MB/s，则主从延迟只会越拉越大。   

**MongoDB 的解决方案**
1. FlowControl 机制，主动限流   
从 4.2 内核版本，开始引入了 [Flow Control 机制](https://www.mongodb.com/docs/v4.4/tutorial/troubleshoot-replica-sets/#flow-control)，主节点在主从延迟较大时，会对写入请求进行限流，保证副本集的大多数提交时间点（majority committed point）不会超过设置的阈值（默认 10 秒），具体实现为主节点根据延迟情况计算每秒能够发出多少 ticket供写入，如果 ticket 消耗完毕，则写入请求会阻塞到下一秒有空余的 ticket 为止。具体的计算规则可以参考 [Github Wiki](https://github.com/mongodb/mongo/blob/r5.0.16/src/mongo/db/catalog/README.md#flow-control)，这里不作展开。   

2. 主节点流式推送日志，推拉结合   
从 4.4 版本开始，MongoDB 默认支持了 [Streaming Replication](https://www.mongodb.com/docs/v4.4/core/replica-set-sync/#streaming-replication), 如果节点 B 到节点 A 同步数据，会发起带 exhaust cursor 属性的拉日志请求，然后接收节点A 的日志推送(推拉结合)。后续节点 A 产生新的 oplog 日志后，可以直接推送给节点B，这样可以减少网络 round-trip 带来的延迟。提升了拉 oplog 日志的速度后，在以下方面会带来显著提升：   
    - 提升到 Secondary 节点读数据的新鲜度。   
    - 对于 {w:1} 只写主节点就确认成功的请求，能够降低主节点 crash 时数据回滚的风险。   
    - 对于{w:"majority"} 写多数节点的请求，能够有效降低处理延迟。   

以节点 A -> 节点 B 的同步为例。在 MongoDB 4.4 版本中，为了实现 streaming 复制流程，节点B的 OplogFetcher在通过 find/getmore 请求到节点A 上拉日志时，会[携带 exhaust 标记](https://github.com/mongodb/mongo/blob/r4.4.24/src/mongo/db/repl/oplog_fetcher.cpp#L638-L656)。后续处理流程如下：   
- 节点A 在返回一批oplog后，**如果发现处理的是 exhuast 请求，则[自动生成一个新请求](https://github.com/mongodb/mongo/blob/r4.4.24/src/mongo/transport/service_state_machine.cpp#L511-L517)（相当于在同一个连接上，自己给自己发了一个 getmore 命令）。然后将 ServiceStateMachine 的[状态置为 State::Process](https://github.com/mongodb/mongo/blob/r4.4.24/src/mongo/transport/service_state_machine.cpp#L440) 方便后续直接处理这个请求**，非 exhaust 场景下会[置为 State::Source](https://github.com/mongodb/mongo/blob/r4.4.24/src/mongo/transport/service_state_machine.cpp#L446) 接收节点 B 发下一个 getmore 命令。   
- 节点B  不断通过 [OplogFetcher::_getNextBatch()](https://github.com/mongodb/mongo/blob/r4.4.24/src/mongo/db/repl/oplog_fetcher.cpp#L663) 获取新日志，底层调用 [DBClientCursor::requestMore()](https://github.com/mongodb/mongo/blob/r4.4.24/src/mongo/client/dbclient_cursor.cpp#L261) **在判断请求是 exhaust 类型时，会直接[读取连接中的buffer数据](https://github.com/mongodb/mongo/blob/r4.4.24/src/mongo/client/dbclient_cursor.cpp#L305)**，如果不是 exhuast 类型则[重新发起 getmore 命令](https://github.com/mongodb/mongo/blob/r4.4.24/src/mongo/client/dbclient_cursor.cpp#L277-L284)去源节点拉日志。   

Streaming 模式的示意图如下：  
<p align="center">
<img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/940da4b6-1406-43cd-b614-bca819c55ed1" width=700>
</p>

**业界横向对比1：ParallelRaft  的并行复制**    
不论是MongoRaft 的拉日志模型还是推拉结合模型，都是使用了单连接有序传输日志。日志的提交方式都是顺序提交，且不能有空洞。同理，原生 Raft 协议由于其顺序提交的特性，也不能很好地驾驭多连接日志同步的场景。   
[PolarFS](https://www.vldb.org/pvldb/vol11/p1849-cao.pdf) 的 ParallelRaft 算法提供了一个新思路：通过多个连接传输日志，并且支持乱序确认和乱序提交。为了解决乱序提交带来的冲突检测，每条日志上都有 look-behind buffer 记录前 N 条日志修改的 LBA （逻辑地址，可以类比为 MongoDB 中的 _id），在日志存在少量空洞的情况下也能完成冲突检测并顺利提交。

**业界横向对比2：TiKV 的 MultiRaft**    
既然单个 raft group 存在的性能问题不好解决，为何不将数据划分为多个 region，每个 region 作为 1 个独立的 raft group只管理少量的数据呢？    
[TiKV](https://tikv.org/deep-dive/scalability/multi-raft/#:~:text=Here%20Multi-Raft%20only%20means%20we%20manage%20multiple%20Raft,Raft%20groups%20in%20terms%20of%20partitions%2C%20namely%2C%20Region.) 的 MultiRaft 就是采用了这种方式。通过将数据切分为多个 region(1段 key range)，每个 region 对应独立的 raft group，TiKV 能够轻易搞定海量数据。    
可能有人会将 MultiRaft 和 MongoDB 的分片集群进行对比。在 MongoDB 分片集群下，海量数据按照 shard key 映射到底层多组 shard servers，每一组 shard servers 也是一个 raft group。分片集群的复制也能达到 MultiRaft 的效果。    
但是 MongoDB 分片集群不同的地方在于，raft 运行的单元还是一组独立的 mongod 进程（里面可能包含多个 chunk，即多个 key range）。而 TiKV 中，raft 的执行单元是一个 region，粒度更小。Region 对应到 MongoDB 中应该是 chunk，即 1段 key range。    


### 3.2.3 小结
MongoRaft 和 Raft 虽然都是强主模式，通过大多数节点的日志复制来完成提交，但是在具体实现上却有很大区别。主要体现在：  
1. Raft 采用**推日志模型**，leader 通过 RPC 将日志推送给 follower。而 MongoRaft 采用**拉日志模型**，并通过 streaming 推拉结合的方式提升速度。拉日志模型带来一个明显的好处就是能更好的支持链式复制。  
2. Raft 先复制日志到大多数节点，leader 节点才应用日志。而 MongoRaft 中每个节点的日志复制和日志应用是”一体“的。  
3. Raft 中的日志采用递增的序号来标记顺序。MongoRaft 中的 oplog 采用混合时钟来标记顺序，可以更灵活应对各种业务需求，比如按时间戳读、回档到某个时间点等。  

另外，MongoDB 在内核实现中，使用了大量的架构优化来提升性能，比较重要的优化点有：  
1. 采用多线程 + 内存队列，将日志的拉取和回放流程**解耦**，使得整个复制过程**流水线化**，充分利用多核硬件。  
2. 采用最极致的并发策略，充分利用多线程提升日志持久化和回放的效率。在并发乱序回放过程中保证了读取数据的一致性，并且严格按顺序提交日志。  
3. 通过 flow control 限流机制和 streaming replication 机制尽可能提升日志传输的效率，避免主从延迟带来的风险。  



## 3.3 选主协议
Raft 和 MongoRaft 都是 “强主”协议，所有的写操作都在主节点上完成，并复制到从节点。因此，如何快速安全的选出主节点非常关键。简单来说，主节点的选举由副本集中大多数节点（包括candidate节点自己）投赞成票而来，然而实际生产环境中还需要解决一些异常场景，包括：   
1. 触发选主的条件是什么？
2. 如何避免网络分区导致同时出现多个主节点？
3. 如何避免在配置变更时出现多个主节点？
4. 如何在出现投票冲突时快速达成一致？
5. 如何主动进行主节点切换？
6. 如何保证新主节点一定包含已提交的数据，即不会出现已提交的数据由于切主而丢失？

下面我们带着这些问题，分析一下 Raft 和 MongoRaft 的处理流程。其中配置变更时的选主安全性问题，我们单独在配置变更章节描述。

### 3.3.1 Raft 原理
#### 何时触发   
Leader 定期给 follower 发送心跳。如果 follower 节点在一段时间内（Election timeout）没有接收到心跳，则状态变为 candidate 并发起选主。   
<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/c849c661-4c9a-4eef-ae0a-8bd31b6fbb69" width=300>
</p>

#### 选主流程   
每轮选主流程都在某一个 term （任期）下进行，term 对应了一个严格递增的整数，可以用来标识节点的状态是否过期。     
在一个 term 内，每个投票者最多只能投一次赞成票，因此最多只会选举出一个主。当然，也有可能因为超时或者选举冲突等原因，某个 term 内选不出主。    

Candidate 首先将 term 加 1，然后并发给副本集中的其他节点发 RequestVote 请求。Candidate 节点接下来可能会遇到 3 种场景：   
1. 副本集中大多数节点（包括 candidate 自己）投了赞成票，此时 candidate 成功变为 leader. 投票节点在每个 term 最多只会投出 1 张赞成票，而且为了防止出现已提交数据被回滚的情况，candidate 的日志必须足够新才能得到赞成票。   
2. 收到另外一个节点声明自己是 leader 的心跳信息。如果那个新 leader 的 term 大于等于自己当前的 term，则 candidate 变为 follower，否则忽略那个心跳。   
3. 没有收到大多数节点的赞成票，则 candidate 等待一段时间后 term 加1，重新发起选举。出现这种场景，可能是副本集中同时有多个 candidate，每个 candidate 都只得到了少数票。也有可能是副本集当前只有少数节点存活。   

对于上述第 3 种场景，raft 采用的解决方式是 candidate 等待随机一段时间后再重新发起选举，这样下次选举的冲突几率就非常低了。   
最初 raft 打算采用给 candidate 派优先级的方式来解决冲突（ranking system），优先级低的 candidate 如果发现有更高优先级的 candidate 存在，则出动退回到 follower 状态。这个方案看起来合理，但是实际测试中发现系统的可用性（无主时间）会受到较大影响，而且有很多极端场景导致的bug. 最终，还是选择了随机回退的方案。   
从 raft论文 给出的测试结果来看，随机回退能够有效避免选举冲突（split vote）的情况。   
<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/40a79a7b-7ee7-4e09-82d8-44a4cdc41beb" width=400>
</p>
   
在上面的半张图中，150-150ms 的场景是没有任何随机性的，可能选主需要长达 10 秒才收敛。如果增加 5 ms 的随机性，即选主超时在 150-155ms 中的随机值，平均 287 ms 即收敛，可以说效果非常明显了。   
在下面的半张图中，raft 分析了将 election timeout 设置为多长更合适的问题。一般来说设置的短一点，follower 能很快发现无主并发起选主，降低系统无主的时间，比如实验中设置 12-24ms 的 election timeout, 平均只需要 35ms 就能选出新主。   
但是设置的太短也有问题，如果出现一些网络延迟抖动，可能导致一些不必要的选主和切主。比如 3 副本分别在中国、北美和南美，网络 RTT 就在百 ms 级别，而且网络不稳定，如果设置 election timeout 为 200 ms, 可能会导致选主非常频繁。**因此，election timeout 的设置要依据网络距离，网络质量等因素进行综合评估。**   

#### 选主成功后的流程
如果 candidate 成功变为 leader，则立刻向其他节点发送心跳昭告自己的状态，并避免新的选举发生。

### 3.3.2 MongoRaft 原理
#### 何时触发
mongo 的选主支持主动和被动 2 种触发方式。
主动触发的方法有：   
- 主节点主动 stepDown。在主节点上主动执行 [replsetStepDown](https://www.mongodb.com/docs/v4.4/reference/command/replSetStepDown/#replsetstepdown) 命令，主节点会马上转为 secondary, 并很快选举出新主节点。相比重启主节点来触发选主，replsetStepDown 方式触发选主的[时间更短](https://www.mongodb.com/docs/v4.4/reference/command/replSetStepDown/#election-handoff)，从节点不需要等待 election timeout （默认 10 秒）就发起选主。   
- Priority takeover。如果某个节点发现自己配置了比主节点更高的 priority，则会在等待一段时间后（参考 getPriorityTakeoverDelay）发起选举。   

被动触发一般对应节点不通的场景：   
- 主节点一段时间内（election timeout, 默认 10 秒）感知不到大多数节点的存在，则主动 stepDown。  
- 从节点一段时间内（election timeout, 默认 10 秒）感知不到主节点的存在，则发起选举。  

Mongo 也使用了心跳机制来感知各个节点的状态，但是与 raft 不同的是，Mongo 中的任意 2 个节点都能相互发心跳请求：   

<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/607a6fd1-6969-4a41-a042-c1f2f0a8a9cf" width=500>
</p>

由于任意 2 个节点之间都能互发心跳，对于 N 个节点的副本集，平均每个心跳周期内，副本集内的总心跳请求有 N(N-1) 个，如果节点太多会出现爆发式增长。这也是 MongoDB 限制一个副本集最多 50 个节点的原因之一。   
这种心跳机制的好处是，副本集中每个节点都能知道全局的节点状态信息，因为心跳信息中包含了节点的 opTime 同步进度，replsetConfig 版本，主节点的状态和 term 等信息。    
基于上述机制，副本集中的从节点可以根据一定的规则选择合适的同步源节点，并形成链式复制结构（生成无环的 spanning tree）。   
另外，在 MongoDB 的复制模型中，拉取 oplog 的请求和更新oplog 复制位置的请求会携带包含节点状态的元数据。   
以下图为例，副本集中有 5 个节点，在某个时刻主节点 A 和 C, D, E 网络不通，但是 B 和所有节点都能正常通信：   

<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/f5450ee0-bcec-40ce-a376-cfad9a2c4bd4" width=600>
</p>

对于左侧 Raft 来说，C, D, E 在一段时间内接收不到主 A 的心跳，会发起选主。   
对于右侧的 MongoRaft 来说，C, D, E 可以从 B 节点同步，链式复制结构会让 A 通过 B 节点感知到 C、D、E的状态，而不用重新发起选主。   
不过需要注意的是，如果节点 B 是 arbiter 或者数据非常落后，则不会形成上述链式复制结构。   

#### 选主流程
选主流程分为 2 步：   
1. Dry-run election, 即试探性发起选举。其目的在于试探集群中有多少节点会支持自己，增加后续真正选举的成功率。发起 dry-run election 时不会增加自己的 term，所以不会造成 term 无意义的递增，而且也不会导致当前的 Primary 节点 stepDown.   
2. Real election，candidate将 term 加 1，然后真正发起选举（旧主如果收到这个 term 更大的 real election 请求，会主动 step down）。首先 candidate 会投票给自己，然后并发给集群中有投票权的节点发起投票请求。如果获得了大多数赞成票，则选举成功。   

从 voter 的视角来看，它在收到 candidate 发过来的 requestVotes 命令时，先判断 term 是否比自己的新，并更新自己的 term. 然后判断自己是否应该投赞成票，如果满足以下条件，会投反对票：  
- Candidate 的 term 更低。   
- 配置不匹配，replSetName 不匹配。   
- Candidate 的数据更旧（lastAppliedOptime更小）。   
- 对于这个 term，voter 已经投过一次票了（当然 dry-run 流程的不算数）。   
- Voter 是一个 arbiter, 而且它能感知到当前存活了另一个主，并且这个主的 priority 不比 candidate 低。这个策略主要是应对 Primary-Secondary-Arbiter 架构下，primary 和 secondary 不能通信，arbiter 能够和它们都正常通信的场景。如果 arbiter 没有这样的投票策略，可能同时会出现 2 个主。   

一旦节点给自己或者其他节点投票，则会将 “lastVote” 信息持久化到本地的 local.replset.election 表中，避免节点重启之后这些信息丢失，导致给同一个 term 多次投票的情况。   

#### 如何保证选主流程的安全
一个节点只给一个 term 投 一次票的机制，避免了一个 term 出现多个主节点的情况，但在某些时刻，集群中可能出现 2 个主的情况，分属于不同的 term。    
举例如下：   
<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/b13756c9-9c85-47e5-9c0c-406ad4acf66d" width=400>
</p>

(a). 所有节点运行在 term 1, 其中S1 为主节点。    
(b). S1和S5 出现网络隔离，S5 发起选举并获取 S3、S4、S5的投票成为了新主。S1、S2 运行在 term 1, S3、S4、S5运行在 term 2。    
(c). S1 和 S5 都能接收客户端的写请求，S5 的写请求能提交（到大多数节点）。S1 的写请求只能复制到 S1 和 S2，其他节点由于网络不通，因此会卡住不能提交。    
(d). S1 的日志复制到 S3、S4、S5, 但是term 太低并拒绝。    

对于 Raft 来说，S1 推送给 S3、S4、S5 的复制请求会由于自身 term 太低被拒绝。    
而对于 MongoRaft 来说，由于采用了从节点拉日志的方式，情况要更复杂一些。在上述例子中，S3 和 S4 的日志复制取决为当时的同步源节点是否切换为 S5，如果切换为 S5 则压根不会从 S1 节点复制日志。如果同步源还是 S1 ，则会在应用日志后，通过 updatePosition 命令给 S1 反馈日志同步情况，该命令中会携带 S3/S4 自身的 term 为 2。 S1 在收到这个命令后，明白自身的 term 太低，主动 stepDown，并不会将该写请求置为提交。   
MongoDB 论文中列举了不一样的例子：   
<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/d3583214-26e2-4bf1-a84e-cf24fe277f20" width=400>
</p>

(a). A、B运行在 term 2, A 作为主节点提交了日志 “2”；C、D、E 运行在 term 3， E 作为主节点提交了日志 “3”.   
(b). Raft 协议：A的日志只能复制到节点B。C、D、E 节点由于 term 更高会明确拒绝。   
(c). MongoRaft 协议：A 的日志可以复制到 C 和 D.   
(d). MongoRaft 协议：C 和 D 在 updatePosition 反馈时表明自己是 term 3，A 收到后主动 stepDown，不会提交日志 “2”。   
(e). MongoRaft 协议：E 节点的日志“3”复制到所有节点，并 overwrite A节点的日志“2”。   
整体来看，虽然由于复制模型导致处理过程不一样，但是 MongoRaft 和 Raft 殊途同归。   

引入 term 机制后确实避免了同时出现多主并同时提交日志的问题。但是也引入了另外一个问题：主节点是否能直接提交之前 term （older term）的日志？   
对于 Raft 和 MongoRaft 来说，答案都是否定的。   
首先引用 Raft 论文中的一个例子，看看提交主节点如果提交之前 term 的日志会有什么问题：   
<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/bf8eb6f9-4b64-4b96-a2fd-a73a39d83c60" width=400>
</p>

(a). S1 是主节点，并将最新的日志 "2" 同步到了 S1 和 S2，还没有提交。    
(b). S1 挂了，S5 被 S3、S4、S5 选举为新主，运行在 term 3，并写入了日志 “3” 但没有复制到其他节点。    
(c). S5 挂，S1 重启并成为新主。然后继续将之前的日志 “2” 复制到 S3, 此时已经复制到了大多数节点（已经复制到了S1、S2、S3），但还没有提交。    
(d). S1 没有提交日志 “2” 就挂掉，则 S5 可能重新被选为主，并在选主之后会将之前已经复制到大多数的日志“2” 抹掉（overwrite）。   
(e). S1 提交了当前 term 的日志“4” 才挂掉，则 S5 不可能被选为主。日志 “2”在日志“4”之前，才会保持提交状态。  

>需要特别说明的是，为什么提交了日志 “4”， S5 就不会被选举为新主，而之前“提交”了日志 “2”， S5 却能被选举为新主呢？   
>Raft 选举时，voter 给 candidate 是否投票，有一项主要的依据就是比较日志的新旧：如果 voter 的日志更新，就不会给 candidate 投票。而比较日志的新旧，首先就是要比较日志中的 term 大小。
在上述例子中，日志“2” 的 term小于日志“3” 的 term，日志 “3” 的 term 小于日志“4” 的 term。

从上述场景(d) 中可以看到，即使older term 的日志复制到了大多数节点，还是可能被抹掉。因此，Raft 中的主节点不会通过计算日志是否复制到了大多数节点来提交日志，而是**通过提交当前 term 的日志来间接提交之前 term 的日志**（场景(e))。   
Raft 论文中也提到，采用上述“一刀切”的机制还是方便协议的理解落地，降低复杂性。理论上来说，如果在上述步骤 (c) S1 挂掉之前，日志 “2” 复制到了所有节点（并 overwrite 了 S3 的日志“3”），其实也能肯定日志“2” 的提交状态。   

MongoDB 也采用了一样的机制，如果从节点反馈的 updatePosition 中的 term 较低，主节点不会真正提交这条日志。只有收到的反馈中 term 和主节点当前的 term 相同，才会真正提交这条日志。在某一条日志被提交之后，比其更早（opTime 更小、term 更小）的日志也间接提交了。   

**这也是为啥 MongoDB 在选举新主节点之后，会马上生成一条当前 term 的 oplog 日志，其实就是为了提交更早 term 的日志。**   

#### 选主成功后的流程
新主节点在选举成功之后会进入如下流程：   
1. 停止当前的同步逻辑，并将自己选举成功的消息通过心跳告知其他节点。   
2. 检查自己是否需要 catch-up. 比如副本集中有 5 个节点，数据从新到旧的排序为 A>B>C>D>E，B得到了 BCDE 的投票成为新主。A 节点上存在一些更新的日志，虽然没有提交，但是 B 节点也会同步过去。具体实现流程为步骤1 中的心跳响应中包含了各个节点的 lastAppliedOpTime 信息，如果有节点比自己新，则这个新主节点就会进入 catch-up 阶段尽可能多同步一些日志。在进入 catch-up 节点之前，新主节点会启动一个定时器，防止catch-up 时间不可控，集群长期无主影响可用性。如果 catch-up 超时也没有关系，因为这只是一个尽力而为的操作。   
3. Catch-up 不论成功与否，接下来会进入 drain-mode. 新主节点将拉过来的日志进行回放（apply）。   
4. 新主节点写一条 “new primary” 的 oplog，方便提交之前 term 的日志。   
5. 新主节点删除上述临时表、终止残留的事务。此时选主操作才真正完成，新主节点可以对外提供写服务。   

从上述流程可以看出，catch-up 阶段时选主成功后 MongoRaft 和 Raft 最大的区别。MongoRaft 通过这个机制可以尽量保留已经写入到主节点但是还没有提交（到大多数节点）的数据。主要是因为MongoDB 支持非 majority的写入方式（local commit），比如客户端为了性能考虑，可以在只写主成功后即返回。原则上来说，这些数据是可能被回滚的，MongoDB 也不保证这种写入方式的持久性。但 MongoDB 还是在设计上尽量避免发生回滚。

### 3.3.3 小结
纵观 Raft 和 MongoRaft 的选主流程，在心跳探活、term、大多数选举等机制上大致相同。但是 MongoRaft 相比 Raft，还是在不少方面做了改进，包括：   
- 探活机制方面，MongoRaft 任意 2 个节点之间都有心跳，容错性更高。另外，MongoRaft 支持灵活的链式复制架构生成 spanning tree 并传递节点状态信息。使得集群对网络的容错性也更高，避免发生不必要的选举。
- 选主流程上，MongoRaft 引入了 dry-run 机制，提升了真正选举时的成功率，避免不必要的term 递增以及由此造成的主节点 stepDown。
- 选主成功后，MongoRaft 引入了 catch-up 机制，尽可能地保留已经写到主节点但是还没有提交（到大多数节点）的数据，增强了数据持久性，减少回滚操作。


## 3.4 配置变更
在副本集运行过程中会遇到配置变更需求，比如新增加一个节点提升读能力，或者删除一个机器故障的节点等。    
一种比较原始的方法，是将配置写在配置文件中。如果需要修改配置，则停掉所有节点，统一修改配置之后再重启所有节点。但是这种简单粗暴的方法并不适用于线上系统，因为很大程度上影响了系统的可用性，并增加了误操作风险。    
因此，非常有必要支持不停服变更配置。    
下面先从 [Raft 论文](https://raft.github.io/raft.pdf)中描述的算法入手，了解配置变更的问题以及解决方案。然后分析 MongoRaft 的实现方法，以及相对 Raft 有哪些改进。

### 3.4.1 Raft 原理
为了做到不停服，就需要解决变更配置期间系统运行的安全性问题。我们知道新配置的同步是需要时间的，在这段时间内集群中新、老配置共存，可能引发同时存在多个主节点的问题。论文中列举了一个例子如下：    
<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/d3e75b5c-7a05-4182-bae1-a6d94faaea70" width=400>
</p>

起始时刻，集群中有 Server 1,2,3 共 3个节点按照 C(old) 配置正常运行，后来加入了 2 个节点 Server 4 和 Server 5。在中间某一时刻，Server 1, 2 还在使用 3 节点的旧配置，但是 Server 3,4,5 已经使用了 5 节点的新配置。如果 Server 1,2 与 Server 3,4,5 出现了网络隔离，则 Server 1,2 之间会选出一个主节点（满足总节点数为 3 的选举条件），Server 3,4,5 之间也会选出一个主节点（满足总节点数为 5 的选举条件）。 2 个主节点都能接收并提交写请求。

为了解决上述问题，Raft 采用 2 阶段流程来实现不停服配置变更，具体流程为：       
1. 生成 C(old, new) 日志，并提交到 C(old) 的大多数节点和 C(new) 的大多数节点。
2. 生成  C(new) 日志并提交到集群的大多数节点，如果有节点发现自己不在新配置中，则主动退出。     

<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/d9e019ce-845d-49db-8cd5-3e8fc3623eb4" width=400>
</p>
  
需要说明的是，C(old, new) 不只是简单的将 C(old) 和 C(new) 的节点做并集。C(old, new) 的真正的含义是：每项决议（选主和日志提交）都需要 C(old) 的大多数节点通过**而且** C(new) 的大多数节点通过。     
通过这种方式，避免了集群在同一时刻同时存在 C(old) 和 C(new) 并且各自独立进行决议的问题。但是又引入了 3 个新问题：
1. C(old) 和 C(old, new) 能否共存？    
如果 C(old,new) 还没有完成提交就出现了主节点宕机或者网络分区，此时可能会选举新主。不妨假设假设新主节点在 C(old) 配置下，则其需要获得 C(old) 中大多数节点的赞成票；假设新主节点在 C(old, new) 配置下，则其同时需要 C(old) 和 C(new) 中大多数节点的赞成票。**也就是说，无论如何都需要收到 C(old) 中大多数节点的赞成票**。由于C(old) 节点中大多数节点的赞成票只会投给一个节点（Election Safety），因此不会再同一时刻出现 2 个主节点。    
至于新主节点可能运行在  C(old) 配置下，也有可能运行在 C(old, new) 配置下，具体取决于故障之前 C(old, new) 的同步情况。    
2. C(old, new) 是否为稳定状态?      
如果 C(old, new) 状态被提交，说明 C(old, new) 配置已经同时提交到了 C(old) 和 C(new) 中的大多数节点。如果此时发生主节点宕机或者网络分区，新主节点肯定会运行在 C(old, new) 配置下，其选举要同时获得 C(old) 大多数节点的赞成票以及 C(new) 中大多数节点的赞成票。因此，不会出现 2 主节点。   
3. C(old, new) 到 C(new) 能否平滑过渡？       
C(old, new) 被提交之后，主节点接着会提交 C(new)。如果在提交 C(new) 的过程中出现了主节点宕机或者网络分区导致重新选主。则新主节点要么运行在 C(old, new) 下，要么运行在 C(old) 下，绝不可能运行在 C(old) 下，因为前面 C(old, new) 的成功提交已经保证了即使还有节点运行在 C(old) 下，也构不成大多数。      
不妨假设新主节点运行在 C(old, new) 下，则其需要同时获得 C(old) 和 C(new) 中大多数节点的赞成票；如果新主节点运行在 C(new) 下，则其需要获得 C(new) 中大多数节点的赞成票。也就是说不论怎么选举，新主节点都需要 C(new) 中大多数节点的赞成 。因此，不会出现 2 个主节点。      

除了新旧配置的安全过渡之外，Raft 论文中还描述了一些工程实践中常见的异常场景：       
- **新加入节点的延迟过大问题**。新加入的节点需要花较长的时间进行数据同步（catch-up），一般是全量加日志增量的方式。由于 Raft 中日志顺序提交的特性，配置变更日志和数据变更日志糅合在一起，会导致配置变更日志需要较长的时间才复制到新加节点上，整个配置变更的时间也随之变长。为了解决这个问题，Raft 建议按照 non-voting 模式添加新节点，这种模式的节点不会算在 majority 范围，因此不会阻塞配置变更的提交。     
- **主节点不在 C(new) 新配置中**。如果主节点发现自己不在 C(new) 中，就立刻 step down 并退出集群，则 C(new) 日志无法在集群中提交。这种情况下，意味着已经要被剔除的节点还要承担一段时间的集群管理任务。Raft 建议的做法是该主节点要等到 C(new) 在集群中提交之后再退出，当然这里提交时的大多数节点是不包含它自己的。
- **已删除的节点发起选主请求**。第一次看到这问题比较诧异，因为论文中明确描述了 C(new) 提交之后，不在 C(new) 中的节点主动停服并退出集群，为啥还有已删除节点发起选举的问题呢？后来想了一下，个人认为这里说的应该是配置提交过程中因为各种原因没有收到新配置的节点。这些节点因为已经被剔除，无法收到主节点的心跳，所以会尝试通过RequestVote RPC 选主，导致主节点降为 follower 状态。然后集群重新选主，选出的新主又不给这些已删除 的节点发心跳，然后这些已删除的节点又要闹着选主。Raft 建议的做法是从节点如果在 election timeout 内成功收到过主节点的心跳，或者主节点在 election timeout 内成功收到过大多数节点的心跳，则认为主节点正常。如果这段时间有节点闹着选主，一概拒绝之。

Raft 的配置变更流程整体来看比较完善，但是实际工程实践中还有一些场景有待优化。个人认为有 2 点：     
1. 没有充分说明主从延迟很大时的效率问题。     
Raft 将配置和数据变更都放在日志中提交，这样导致主从延迟大时，配置变更操作很可能卡住。比如线上 1主 2 从的副本集，2 个从节点延迟 1 个小时，我希望将 1 个从节点剔除后换个高性能节点，那这个变更操作可能会卡 1 个小时。显然是无法接受的。     
可能有人会想到，可以参考加节点的方法，先将 2 个从节点变成 non-voting 模式，不算在 majority，配置变更就能很快提交了。然而问题是，调整 non-voting 本身就是配置变更操作，这项操作首先就会卡住。
2. 对主节点强依赖。
Raft 论文中描述的配置变更算法都依赖主节点执行，然而实际场景中我们可能遇到无主节点的情况。比如 3 节点副本集，有 2 个节点的机器永久性损坏了，不可能自己选出主节点，此时该如何恢复呢？      
一种简单粗暴的方法，就是将仅剩的节点停掉，然后改配置，再作为单节点的方式重启。     

### 3.4.2 MongoRaft 原理
MongoRaft 的实现原理可以参考作者发布的[论文](https://drops.dagstuhl.de/opus/volltexte/2022/15801/pdf/LIPIcs-OPODIS-2021-26.pdf)，MongoDB 4.4 版本的代码实现可以参考源码中的 [README.md](https://github.com/mongodb/mongo/blob/r4.4.24/src/mongo/db/repl/README.md)。结合论文和代码实现，我认为 MongoRaft 和 Raft 在配置变更流程上有 3 点重大区别：   
1. **Logless**. 结合前面的分析我们看到 Raft 的配置变更流程还是要依赖日志同步，日志同步的快慢或多或少会对配置流程的时长会有影响。而 MongoRaft 采用了“无”日志的方式（也不是整个过程完全没日志，所以要加个引号），使用基于心跳的 Gossip-like 协议进行配置变更同步。在流程上将配置变更和数据同步分开，再保证数据安全的前提下尽量快速进行变更。    
2. **没有照搬 Raft 的 2阶段算法进行配置的安全过渡**。MongoRaft 使用了更加简单巧妙的方式。具体来说，新配置和旧配置必须保证 Quorum 节点有重叠，每次只能有一个 voting节点加入或者删除（对于 non-voting 成员，由于它们不属于 majority(quorum) 的计算范围，因此没有这项限制）。通过这个限制，可以使得协议实现更加简单，但是如果要添加或者删除很多节点，则可能需要发起多次配置。我们可以简单论证一下这种方法的安全性：当前配置C1 和新配置 C2 如果要同时选举产生不同的主节点，则必须通过获得 C1 的主节点需要获得 C1 中大多数节点的同意，C2 的主节点也需要获得 C2 中大多数节点的同意，但是现在 C1 中大多数节点和 C2 中大多数节点必定是有重叠的（QuorumOverlap），这些重叠的节点不会在同一个选举周期内给 2 个不一样的节点投赞成票。因此，通过反证法能够证明不可能同一时刻在 2 个配置中出现 2 个不一样的主节点。
3. **支持 unsafe(force) 模式**。真实业务场景中可能会出现节点损坏较多无法选主的情况，MongoRaft 支持在非主节点发起强制配置变更。Unsafe 模式会跳过很多安全性检查，有数据回滚、丢失的风险。因此，这种模式一般用来应急使用，而不是常规手段。

下面以 MongoDB 4.4 版本的实现进行详细分析。由于 unsafe 模式同 safe 相比主要是跳过了一些检查，因此这里只分析 safe 模式。

#### 配置构成
MongoDB 的配置方法和配置选项可以直接参考[官方文档](https://www.mongodb.com/docs/v4.4/reference/method/rs.reconfig/)，这里不再赘述。

#### 核心流程
配置信息持久化存储在每个 mongod 节点自己的 local.local.system.replset 表中。我们知道 local 库是每个 mongod 本地私有的，不会通过 oplog 同步到其他节点。因此，配置信息不是通过 oplog 同步，而是使用心跳进行同步。    
为了保证变更流程的安全性，除了上述每次添加和删除的节点个数限制之外，执行 reconfig 的主节点还要做 2 项检查（假设当前的配置是 C1,上一个配置是 C0, 即将要变更的配置是 C2）：    
1. 当前配置 C1，已经被复制到了副本集的大多数 voting 成员。
2. 上一个配置 C0 已经提交的数据（复制到大多数 C0 的 voting 成员），在当前配置C1 中也是提交状态，才能进行下一步的状态变更。

其中第 1 项检查保证了 C0 不再有形成大多数的机会。否则在 C1 到 C2 的变更过程中，C0 配置如果也选举了一个主节点，则整个副本集中将同时有  2 个主节点接受写入。    
第 2 项检查保证了已经提交的数据不会因为配置变更而回滚丢失，为了方便理解，参考下面的示意图：     

<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/9b10f846-0cf5-480a-9801-c8493846e9fc" width=700>
</p>
    
图中所有节点都是 voting 节点，最开始有一条数据在 A, C,E 上提交。节点 E 被删除后，副本集中只有 4 个节点，数据存在于 A 和 C 中，此时不满足大多数条件，但是数据还没有回滚丢失的风险。      
下一步，在没有等待数据同步到 B 时就又进行了一次配置变更，这次增加了一个节点 F。这时问题来了，如果 A 和 C 同时挂掉，会选举 B 或者 D 成为主节点继续接受写入，而 A 和 C 中的数据丢失了。即使 A 和C 重启了恢复正常，也会成为从节点，并将最近提交的数据回滚掉。    
可以看到，如果在第 2 次变更之前，先等待数据同步到 B ，使得数据变更满足大多数节点的提交状态，才能安全的进行下一步配置变更，示意图如下：      
<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/1769cb27-1435-461d-851c-b18873e10ff0" width=900>
</p>
    
检查完成，满足安全配置变更条件之后，主节点就执行真正的配置变更流程了，示意图如下：     
<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/cbb17664-6e75-407e-afba-be7e512fac95" width=900>
</p>
    
主要流程为：    
1. 主节点确保当前配置同步到了大多数 voting 节点，awaitConfigCommitment(currentConfig).     
2. 主节点检查新配置是否合法，以及新配置中节点的状态情况，configValidation & checkQuorumForReconfig.   
3. 主节点将新配置持久化到本地的 local 库中，并更新内存中维护的配置信息。storeLocalConfigDocument & setCurrentRSConfig.   
4. 主节点等待配置同步到大多数节点（提交成功），这里同步是采用心跳的方式，因此最终也是由心跳处理流程唤醒。   
5. 2 个从节点给主节点发（周期性）心跳请求，心跳请求中会携带从节点自己的配置版本号。主节点发现自己的配置版本更高，因此会在心跳响应中塞入新配置，传给从节点。   
6. 2 个从节点接受到更高版本的配置信息，应用到自身节点。即持久化到本地的 local 库，并更新内存中维护的配置信息。storeLocalConfigDocument & setCurrentRSConfig.   
7. 主节点（周期性）给从节点发送心跳请求，发现 2 个从节点都使用了最新配置。认为新配置已经复制到了大多数 voting 节点，已经提交。此时可以给客户端返回 success.    

Mongod 节点内部维护了一个状态机，确保同一时刻主节点上最多只有 1 个 reconfig 操作处于执行中，避免并发操作相互干扰。   
如果在变更流程中出现了节点故障，或者新配置一直没有提交到大多数 voting 节点，则 reconfig 命令可能会失败或者超时。然而在后台新配置可能还是会通过 Gossip-like 协议完成提交。所以客户端在重试之前可以先做一些检查。   

整体变更流程中，心跳发挥了非常重要的作用，包括前期的配置检查 checkQuorumForReconfig, 新配置同步，以及同步情况检查。[心跳请求](https://github.com/mongodb/mongo/blob/r4.4.13/src/mongo/db/repl/repl_set_heartbeat_args_v1.h#L48)和[心跳响应](https://github.com/mongodb/mongo/blob/r4.4.13/src/mongo/db/repl/repl_set_heartbeat_response.h#L50)包含的信息如下：   

<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/a322ba73-5dfa-43be-9b5e-1d015c65106c" width=400>
</p>

副本集内任意2 个节点之间都会有周期性心跳（默认 2 秒），每个节点也可以在一些流程中主动触发心跳，比如节点拓扑发生改变，或者上述变更流程中的 checkQuorumForReconfig 逻辑等。心跳除了探活之外，还会携带 term，configVersion 等信息。如果节点在收到心跳请求时，发现对方的配置版本比自己低，则会在心跳响应中附带上自己完整的配置信息。    

#### 测试分析
我们可以通过下面的测试，验证一下 MongoDB 在主从延迟很大的时候能否快速完成配置变更。   
测试环境为：4.4.13 副本集，3 个 voting 节点，为了方便测试观察，心跳周期设置为 10s（默认 2s）, 选主超时配置为 60s（默认 10s）, 2 个 从节点 slaveDelay 10min（默认没有 slaveDelay, 设置为 hidden 是为了防止主节点切换）. 后台 10 秒 1 次写请求。   
配置变更操作为将一个从节点的 slaveDelay 从 600s 改成 601s.   

首先我们验证正常情况下的配置变更耗时。在执行变更前当前配置已经运行了很长时间，处于提交状态。而且没有切主及 term 变更。   
测试得到的总耗时为 10.902s, 通过在代码中添加日志，看看具体耗时的分布情况：   
1. 第1阶段，确认当前配置复制到了大多数 voting 节点，并且当前 term 的第一条oplog 同步到了大多数，耗时忽略不计（小于 1ms）。   
2. 第 2 阶段，配置检查+大多数节点状态检查+本地配置应用，耗时 811ms。   
3. 第 3 阶段，确认新配置同步到了大多数 voting 节点，耗时 10.091s。   

可见，正常情况下，配置变更流程的耗时主要集中在心跳检查阶段。由于 MongoDB 默认的心跳周期是 2s, 所以正常情况下 2s 左右就能完成配置变更，即使主从延迟有 10分钟也没关系。

再来看看异常情况，主节点重启并继续成为主(term++), 由于 slaveDelay 很大，会在第一阶段阻塞很久。总耗时 10min.   
通过加日志发现，这里的耗时基本都在第 1 阶段，等待当前 term 的第一条日志复制到大多数节点。由于我们测试环境的 slaveDelay 配置为 10 分钟，因此这里也卡了 10 分钟。通过代码我们可以看到第 1 阶段确认 oplog 最小提交时间的判断条件为 std::max(_lastCommittedInPrevConfig, _firstOpTimeOfMyTerm); 

另外，通过测试发现， reconfig 流程也会生成 oplog，这也是为什么要对 logless 的“无”日志变更加上引号的原因：   

<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/31511f6f-671b-46f1-abcd-9c06275bcecb" width=850>
</p>
 
同样，出现了选主及 term 变更，也会生成一条 oplog, 也就是上述 _firstOpTimeOfMyTerm 的依据。   

### 3.4.3 小结
1. Raft 协议通过动态变更算法保证了配置变更流程中的可用性，并通过 2 阶段的方式保证安全。   
2. MongoRaft 在 Raft 基础上，借助心跳机制实现配置变更和数据复制分离，能够很大程度上缓解主从延迟大带来的配置变更耗时长的问题。并通过 QuorumCheck 的机制在保证安全性的同时简化了配置变更算法。   
3. MongoRaft 提供了 unsafe(force) 模式，能够在非主节点上实现强制变更，能够在极端情况下快速完成服务恢复。   


## 3.5 持久性保证
Raft 使用 log 来进行复制，并写入磁盘来保证数据持久性。随着写请求的持续，log 的大小会持续增加。因此，需要一种清理 log 的机制，并且不影响数据持久性。

### 3.5.1 Raft 原理
#### Snapshot
Raft 论文的 log compaction 章节说明了 raft 的机制：各个节点在特定条件下（时间周期和 log 大小），会将状态机中的已提交的数据做 snapshot 到磁盘，然后将之前的日志清除（raft 论文中说的是committed entries。Etcd采用的是 [appliedIndex](https://github.com/etcd-io/etcd/blob/release-3.4/contrib/raftexample/raft.go#L378C1-L378C1)，根据 raft 的执行机制，appliedIndex及之前的数据都是 committed）。        
以下图中的节点状态为例，在做 snapshot 时一共进行了 7 次操作，包含 7 条日志。其中序号为 1-5 的日志复制到了大多数节点并提交， 6-7 日志暂时未提交。进行 snapshot 后的结果为：   
- Last included index: 5    
- Last included term: 3    
- State machine state: 包含 1-5 的日志操作记录，即 x = 0, y = 9    

做完 snapshot 之后， 1-5 的日志可以清除，保留 6-7 的日志。   

<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/d389b0ec-4198-4b7c-92cf-1bb90e639754" width=400>
</p>

#### 新增节点
如果副本集中新增了一个空白节点，可以先将 snapshot 传输到空白节点上，然后再将 log 推送过去。也就是常见的全量+增量的方式。

#### 回滚
如果 leader在持久化日志之后，还没有推送到其他节点就异常退出，然后副本集中选出了新 leader 。则挂掉的节点重启后会成为 follower，并且将没有提交的冲突日志回滚掉。   
由于 Snapshot 中都是已提交的数据，因此不需要回滚状态机，只会涉及日志回滚。而日志回滚是很简单的，只需要将冲突的部分切除掉即可。   

### 3.5.2 MongoRaft 原理
MongoRaft 采取了和 Raft 类似的策略，但是具体实现上要更加复杂一些。    
首先是概念上的差异：    
- Checkpoint: 对应了 Raft 中的 snapshot，即某个时刻的全量数据的持久性快照。和 Raft 一样，MongoDB 的 checkpoint也遵循 Copy-On-Write 。
- Oplog：对应了 Raft 中的 log，但是在 MongoDB 中 oplog 也是一个表，并非等同于 journal 和 WAL（Write Ahead Log）的概念。
- Journal（其他系统可能叫 WAL）：持久化日志，存储在特定的文件中，通过 fsync 的方式写入来保证持久性。

Raft 中的 log 概念，对应了 MongoRaft 中的 oplog + journal。   
MongoRaft 在接受写请求时，会更新状态机（库表和索引）、插入 oplog、写 oplog 的 journal。
**用户的库表和索引是不需要写 journal 的，它们可以通过 oplog 保证持久性，而 journal 又保证了 oplog 的持久性。事实上，只有 local 库的部分表需要写 journal， 而local 库中的 oplog 表就是其中之一。具体可以参考 [WiredTigerUtil::useTableLogging](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/storage/wiredtiger/wiredtiger_util.cpp#L567) 的实现。    

另外需要说明的是，MongoDB 通过 Wiredtiger 提交事务时，不负责将 journal 持久化(fsync)到磁盘中。具体可以参考 [wiredtiger_open](https://source.wiredtiger.com/3.2.1/group__wt.html#gacbe8d118f978f5bfc8ccb4c77c9e8813) 和 [commit_transaction](https://source.wiredtiger.com/3.2.1/struct_w_t___s_e_s_s_i_o_n.html#a712226eca5ade5bd123026c624468fa2) 接口的使用说明：   

<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/b68b6b33-eb77-44b2-a08c-0a761f829fa7" width=650>
</p>

Journal 的持久化通过调用另外的 [log_flush](https://source.wiredtiger.com/3.2.1/struct_w_t___s_e_s_s_i_o_n.html#a1843292630960309129dcfe00e1a3817) 接口完成，具体在 MongoDB 内核流程中，有多种触发方式：    
- 周期性触发。后台 WTJournalFlusher 线程每隔 [commitIntervalMs](https://www.mongodb.com/docs/v4.2/reference/configuration-options/#storage.journal.commitIntervalMs) 触发，默认是 100 ms，可配置。   
- 主动触发。包括依赖持久化的内部流程（比如前面介绍的 oplogTruncateAfterPoint），以及指定 { j : true} 的用户请求等。事实上，每次事务执行结束（普通写请求在内部也是事务，保证写表、索引、oplog 的原子性），都会触发守护线程异步刷 journal 并及时更新oplog 可见性。因此，MongoDB 刷  journal 也是比较频繁的。   

将 journal 的写流程和 fsync 持久化流程分离，为用户带来了更多的灵活性，用户可以根据业务需求在 处理延时 和 持久性 2 个方面进行权衡。
MongoDB 在内核实现上通过 lastAppliedOpTime 和 lastDurableOpTime 进行了区分。    

MongoDB 依赖 Wiredtiger 存储引擎接口来生成 checkpoint。在 Wiredtiger 中，1个表的 checkpoint 包含如下信息：   
- 数据块组织信息：包括 3 个跳表分别维护了已分配（allocated list）、可分配（available list）和已删除（discard list）的数据块信息。
- Root page 的位置。
- 文件偏移。用于 checkpoint 恢复。
- Write generation, 主要用于 salvage 流程。    

示意图如下：   
<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/be243e09-0092-42d4-ac12-c0d15ed8a991" width= 700>
</p>

Wiredtiger 在运行时，每个表有一个 “live checkpoint”, 是可读写的。除此之外，任何已持久化的 checkpoint 都是只读的，用户不能直接修改。    
如果当前的 "live checkpoint" 删除了之前  checkpoint 中的数据块，只能将其放入自己的 discard list 进行标记删除，对应的数据块不能被擦除或者马上重用。如果需要分配新的数据块持久化数据，只能从 available list拿， available list 当前空间不足时可以扩展文件加入新的数据块。    
Wiredtiger 采用了命名 checkpoint 的方式，MongoDB 每分钟定期创建的 checkpoint 采用的是默认名字 WiredTigerCheckpoint。    
每次创建时，会将之前的同名  checkpoint 删除。而 checkpoint 的删除会将其 allocated list 和 discard list 合并到下一个checkpoint，并对下一个 checkpoint 中合并后的 list 进行检查。如果有数据块同时出现在 allocated 和 discard list，则将其从对应的 list 中删除，并移动到 live checkpoint 的 available list 中进行重用。     

个人认为 Wiredtiger 的 checkpoint 机制有 2 点非常关键：    
1. Copy-On-Write 机制。在更新和删除已持久化 checkpoint 中的数据块时，不直接对该数据块进行修改，而是通过 discard list 将原数据块标记删除，数据写入新数据块。然后通过checkpoint 删除时的合并策略回收空间，同时也保证了 checkpoint 的只读特性。   
2. 每个 checkpoint 对应了固定大小的文件偏移。这个设计使得基于 checkpoint 的恢复流程非常快：从元数据找到 checkpoint 的长度信息，然后执行文件  truncate 操作即可。   

除了上述概念上的差异，和 Raft 相比，MongoRaft 中的状态机应用和日志生成是在一个原子操作中完成的。Primary 节点在更新状态机的同时生成了 oplog ，然后才复制到其他 secondary 节点。显然 **primary节点生成日志的时刻，这条日志是没有提交到大多数节点的，而此时 primary 节点的状态机已经更新了。**    
作为对比，Raft 先将日志提交到大多数节点，然后才更新 leader 的状态机。    
在 MongoDB 4.0 版本之前，primary 节点做 checkpoint 时会带入这些未提交到大多数节点的数据。一旦出现异常重启导致回滚，会同时涉及日志和状态机的回滚。需要明确的是：   
- **日志回滚是容易的**。只需要计算好需要回滚的区间，然后清除即可。
- **状态机回滚是复杂、耗时的**。我们并不能根据 oplog 反推出之前的状态，比如删除了 1 个表，oplog只会记录删除表的动作，这个表删除之前包含哪些数据并不能根据 oplog 推断出来。对于这种场景，回滚操作需要从最新的同步源节点重新查找数据（[refetch](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/repl/bgsync.cpp#L724)），数据量大的时候非常耗时。

MongoDB 从 4.0 版本开始，引入了 RTT（Recover To Timestamp） 算法，实现了快速物理回滚。其核心是效仿 Raft 引入了 Stable Checkpoint。至此，MongoDB 中主要包含 2 种 Checkpoint：   
- Stable Checkpoint：将提交到大多数节点的时间点为 commit point，stable checkpoint 中只包含状态机中 commit point 及之前版本的数据，但是 oplog 表以及 local 库中其他会写 journal 的表例外。在回滚时，先快速将状态机变为 stable checkpoint 状态（具体来说就是一些文件的 truncate 操作），然后回放 oplog 即可。    
- Unstable Checkpoint: 之前版本的 checkpoint。回滚复杂，可能需要进行 refetch 操作。    

为了实现 Stable Checkpoint，需要依赖 Wiredtiger 存储引擎按照时间戳管理多版本的能力，而且需要感知 commit point 时间戳。为此，MongoDB 节点在每次 commit point 有推进时，会[调用 Wiredtiger 的 set_timestamp 接口](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/storage/wiredtiger/wiredtiger_kv_engine.cpp#L1868)告知当前的时间戳，并且在 Checkpoint 后台线程中[使用 use_timestamp 方式](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/storage/wiredtiger/wiredtiger_kv_engine.cpp#L378)来生成 Stable Checkpoint。   

基于 Stable Checkpoint 的示意图如下： 
  
<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/2f4e4b0c-e61e-4de2-a944-fa770ab75f9a" width=800>
</p>

对 primary节点依次写入了 5 条数据，其中 1-4 复制到了 secondary-1 节点， 1-3 复制到了 secondary-2 节点。则 primary 节点的 mcp(majority committed point) = 4，stable = 4。 secondary-1 节点通过心跳和拉日志请求中的 [replsetMetadata](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/rpc/metadata/repl_set_metadata.h#L68) 推进 mcp = 4, stable = 4。而 secondary-2 节点mcp = 4，无奈自己才复制到日志 3，所以 stable = 3 (选取算法参考[ReplicationCoordinatorImpl::_chooseStableOpTimeFromCandidates](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/repl/replication_coordinator_impl.cpp#L3700) 的实现 )。   
如果此时各个节点做 stable checkpoint, 则：   
- Primary 节点会将用户表中 1-4 的数据持久化到 checkpoint，而 oplog 表由于会写 journal，不受 stable optime 的影响，会持久化 1-5。   
- Secondary-1 节点将用户表1-4 和 oplog表1-4 都持久化到 checkpoint。   
- Secondary-2 节点将用户表 1-3 和 oplog 表 1-3 持久化到 checkpoint。   


如果 primary 节点异常退出，则 secondary-2 会成为新主并接收写入，原 primary 节点重启后会进入回滚流程。    
回滚流程的示意图如下：   
<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/938ee5b5-2a70-4b22-8dc6-e1e61c955856" width=500>
</p>

回滚节点的大致执行步骤为：   
1. 确定 stable checkpoint 的时间点： stable optime（[调用](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/storage/wiredtiger/wiredtiger_kv_engine.cpp#L2254)存储引擎的接口获取）。   
2. 对比自己的 oplog 和主节点的 oplog，找到“分叉点”，也就是代码中的 [common point](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/repl/rollback_impl.cpp#L914-L973)，并检查其合法性：common point >= stable optime。  
3. 执行数据库回滚：[调用](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/storage/wiredtiger/wiredtiger_kv_engine.cpp#L2027) Wiredtiger 引擎的 [rollback_to_stable](https://source.wiredtiger.com/3.2.1/struct_w_t___c_o_n_n_e_c_t_i_o_n.html#a93dbc74accb426582b3c5c2f69e04b28) 接口将数据库回滚到 stable checkpoint状态。在此之前，会将[回滚数据写到本地文件](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/repl/rollback_impl.cpp#L480-L488)中，也可以配置 [createRollbackDataFiles](https://www.mongodb.com/docs/v4.2/reference/parameters/#param.createRollbackDataFiles) 参数关闭此功能。   
4. [清理](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/repl/replication_recovery.cpp#L388-L396) common point 后的 oplog。   
5. [回放](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/repl/replication_recovery.cpp#L423-L428) [stable, common point] 之间的 oplog，这部分 oplog 存储在本地。   
6. 结束回滚流程，进行正常的主从同步。  

### 3.5.3 小结
Raft 和 MongoRaft 都采用物理快照+日志保证持久性和 log compaction。但是具体实现上存在以下明显的区别：    
1. 概念上存在区别。Raft 中的 log 对应了 MongoRaft 中的 oplog+journal，其中 oplog 是系统表，journal是日志文件。个人认为，将 oplog 和 journal 分离至少有 2 个好处：   
a). 方便对日志进行检索操作。比如基于 oplog 的查找过滤，changeStream 等高级特性 。   
b). 方便集成各种插件式存储引擎。其中 journal 和存储引擎深度绑定，比如 Wiredtiger 和 RocksDB 的journal 就不相同，满足单机持久化需求，不参与主从复制；而oplog 不和存储引擎绑定，参与主从复制。对 MongoDB 代码来说，不用感知不同存储引擎的 journal 区别。   
2. MongoDB 节点的状态机变更和日志复制是在同一阶段完成的，状态机中包含未提交的数据。但是从 4.0 版本开始，也引入了 stable checkpoint 机制，在不影响现有复制流程的前提下，数据回滚效率大大提升。   
3. MongoDB 将事务提交时写 journal 和 持久化journal 分开控制，内部通过 lastAppliedOpTime 和 lastDurableOpTime 进行区分。用户可以自行在延时和持久性方面做权衡。   

## 3.6 一致性
这里要讨论的一致性不是数据库事务 ACID 中的一致性，而是分布式存储系统中的一致性。具体分为 2 个维度：      
- 数据维度（Data Centric）。结合经典的 CAP 及其扩展后的 PACELC 理论，探讨 C(Consistency)、A(Availability)、P(Partition)，以及 L(Lantency)、R(Recency) 如何在 Raft 和 MongoRaft 中落地。   
- 客户端维度（Client Centric）。探讨因果一致性（单调读、单调写、写后读、读后写）和[线性一致性](https://github.com/Vonng/ddia/blob/master/ch9.md)。

### 3.6.1 Raft 原理
从数据维度来看，写请求必须要复制到大多数节点才算提交成功。因此，Raft 是强一致性协议。   
从客户端维度来看，读写请求必须都在 leader 节点上执行（clients of Raft send all of their requests to the leader），而且 leader 状态机上的数据都是已经提交的数据（when the entry has been safely replicated , the leader applies the entry to its state machine and returns the result of that execution to the client）。因此，如果不考虑网络分区切主的话，Raft 能够很好地满足单调读、单调写、写后读，读后写这些因果一致性。   

分布式系统一致性的目标，还是让多个节点像单个节点一样工作，客户端感知不到分布式系统的存在。这就不得不提线性一致性。   
在线性一致性保证下，一旦数据完成写入，所有客户端就能读到新版本的数据，旧版本的数据不会在之后的读操作中如幽灵般重现。显然，在不考虑网络分区切主，以及不读 follower 节点的情况下，Raft 也是能轻易满足的。   
然而现实中的分布式系统，是可能出现网络分区的。如果出现网络分区选出了新 leader，可能会出现：   
- 客户端读到新 leader（多数派），但是新 leader 上的状态机还没来得及 apply 已提交的日志。    
- 客户端读到旧 leader（少数派），旧 leader 上没有最新 commit 的日志，状态机也没有更新。   

总结来说，就是读取的节点的 appliedIndex < committedIndex。显然，这是不能满足线性一致性的。   
另外，很多实际的业务场景都会做读写分离，通过读 follower 节点来扩展读能力。所以，读 follower 节点仍然保证线性一致性也是一个切实的需求。   

解决上述问题的核心思路，就是避免读取时节点的 appliedIndex < committedIndex。参考 Raft 论文和一些数据库的解决办法，将常见的解决方案总结如下：   
|方案|应对的场景|基本思路|
|:--|:--|:--|
|Log Read|网络分区|为读操作也生成 1 条no-op log，走一遍  Raft 的提交流程，宣示 leader 的主权 。确保当前是多数派 leader 后再返回读结果。<br>这样可以防止出现网络分区后，仍然在旧 leader上读数据的情况。<br>这种方案比较直观。但是读操作的流程较长，性能会比较差。另外通过日志提交将读操作线性化后，读并发会受到影响。<br>**MongoDB 的线性一致性设计采用了这个思想。**|
|Read Index|网络分区|Leader 收到读请求后先记录 readIndex = committeIndex，然后**通过心跳宣示主权**，确认自己不是少数派。然后确保 appliedIndex >= readIndex 后再返回读结果。<br>Raft 论文原文：1) Leader commit a blank no-op entry into the log at the start of its term.2) Leader exchange heartbeat messages with a majority of the cluster before responding to read-only requests. |
|Lease Read|网络分区|对 Read Index 的一种优化，不要每次读请求都触发 heartbeat，而是**基于 heartbeat 来形成租约机制**。在租约的有效期内，各个节点不要选新主。<br>但是租约机制一般都有比较明显的缺点：**非常依赖时钟来保证租约的准确性**。<br>Raft 论文原文：Alternatively, the leader could rely on the heartbeat mechanism to provide a form of lease, but this would rely on timing for safety (it assumes bounded clock skew).|
|Follower Read|读 follower|Follower 节点收到读请求之后，先向 leader 节点询问当前的 committedIndex。然后**等待日志复制到 committedIndex 并且 apply 到状态机**之后，再处理读请求。<br>问题：怎么保证自己不是少数派的 follower 呢，然后又询问了少数派 leader？是否需要发心跳给大多数节点确认一下自己的 term 是否最新？<br>缺点：每次都去问 leader，网络开销会较大。<br><br>[PolarDB](https://mp.weixin.qq.com/s/yxRsxvWrqumeCahH528mdA) 借鉴了这个方案的思想，但是进行了 3 点性能改进：<br>1. **线性Lamport时间戳，降低询问 leader 的次数**。<br>Follower每次拿到leader的时间戳（committedIndex）时，都会把这个时间戳保存在本地，并且会记录获取到该时间戳的时间，如果某个请求的到达时间早于本地缓存时间戳的获取时间，则该请求可以直接使用该时间戳。<br>2. **分层的细粒度的修改跟踪，避免非必要的等待**。<br>维护三层修改信息：全局的最新修改的时间戳（committedIndex），表级的时间戳和页面（page）级别的时间戳。如果要读取的表最近没有修改，则可以直接执行读操作，而不需要等待全局的日志同步。<br>3. **基于RDMA的日志传输，降低网络延迟和 CPU 消耗**。|


### 3.6.2 MongoRaft 原理
MongoRaft 基于 Raft 进行了扩展，支持可调一致性。具体来说，支持以下 3 个维度的配置：    
1. [Write Concern](https://www.mongodb.com/docs/v4.2/reference/write-concern/)：写请求执行到什么程度再给客户端返回结果。包含 3 个配置：“w” 表示写请求同步到多少个节点，可以指定非负整数，也可以指定 "majority" 表示大多数节点，或者按照节点的 tag 进行自定义；"j" 表示是否要等到节点刷完 journal 才返回，可以指定 true 或者 false；"wtimeout" 表示等待复制的超时。

<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/73a30e74-889a-4406-94b4-3f3e686c2345" width=700>
</p>   

2. [Read Preference](https://www.mongodb.com/docs/v4.2/core/read-preference/)：到哪个节点执行读请求。可以指定 primary, secondary, primaryPreferred, secondaryPreferred, nearest 多种模式，还可以结合节点的 tag、主从延迟情况信息进行灵活选择。

<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/b8085800-06df-4e1b-8760-f81ad35f4cf9" width=300>
</p>

3. [Read Concern](https://www.mongodb.com/docs/v4.2/reference/read-concern/)：读取节点上哪个版本的数据。支持 available, local, majority, snapshot, linearizable 多种级别。在 MongoDB 复制模型中，主节点先应用状态机，再将 oplog 复制到其他节点。每个节点的状态机中都可能包含未提交（majority commit）的数据，这类数据只在本地作了提交（local commit）。如果和传统数据库中的事务隔离级别进行类比，{readConcern: local} 可以类比为 read uncommitted, {readConcern: majority} 可以类比为 read committed。    
MongoDB 通过混合时钟对底层数据进行了多版本（MVCC）管理，**因此如何按照 readConcern 读数据的问题，说白了就是如何按时间戳读数据**。    
Snapshot 和 linearizable 相对特殊。Snapshot 是读取固定时间戳版本的数据，不会随着持续的数据写入和 majority commit point 推进而发生变化，最初只用在事务中，从 [5.0 内核](https://www.mongodb.com/docs/v5.0/reference/read-concern-snapshot/)开始给普通读请求使用。Linearizable 保证线性一致性，只能在 primary 节点执行，primary 节点会等待读请求之前的写操作全部 majority 提交，再执行该读请求。    
不少人将 {readConcern: majority} 和 Cassandra 等系统中的 quorum read 机制混淆。Quorum read 是**读取大多数节点**，然后返回最新版本的记录，是存在读放大的。而 {readConcern: majority} 是根据 readPreference **读取 1个节点**，返回其 majority 提交的记录，不存在读放大。    

<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/dd67d047-6e32-4c82-8fb1-324613cebf94" width=700>
</p>

对于 writeConcern、readPreference、readConcern 的实现原理，后续会有独立的章节进行分析。    
值得注意的是，上述都是请求级别（operation level）或者事务级别（transaction level）的配置。也就是说，用户在使用同一个 MongoDB 集群时，也能根据自身业务特点随时进行调整。比如一个在线编辑业务，可以对”自动保存“请求的writeConcern 设置为 {w : 1}, 而将 ”提交“ 请求设置为 {w: "majority"}.    
如果设置 {writeConcern:"majority", j:true}, {readConcern:"majority"} 和 {readPreference: "primary"}， 则MongoDB的一致性保障和 Raft 相同。因此，可以认为 **MongoRaft 的一致性是 Raft的超集**。   

当我们在设计和使用分布式系统时，不可避免要做权衡（tradeoff）。比如 CAP 理论中，在保证分区容忍性 P 的前提下，无法同时满足 C 和 A，需要根据业务特点进行取舍。    
在真实的业务场景中，除了 CAP 之外，我们经常还要关注吞吐能力、请求延迟、数据新鲜度等维度是否满足需求。理想情况下系统的表现应该是一个”六边形战士“，面面俱到。但是现实情况是，我们必须理解自己的业务系统需要强化哪些能力，弱化甚至放弃哪些能力。    

<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/046af1ee-6814-43fd-8bea-955a29314d0c" width=400>
</p>

- **吞吐**。MongoDB 可以添加 secondary 节点来提升读吞吐能力。但是需要容忍 secondary 节点的数据存在延迟的问题，无法保障数据新鲜度。一般应对的是分析型业务场景，比如对历史数据进行分析等。或者业务自身有容忍或者鉴别过期数据的能力。   
- **延迟**。通过设置较低级别的  write concern 来较低写请求处理延迟，通过设置 {readPreference : "nearest"} 将读请求打散到多个节点能够降低读请求延迟。但是需要牺牲一定的一致性和数据新鲜度。   
- **新鲜度**。读 primary 节点，并且设置 readConcern 为 local，可以读取到最新版本的数据。但是读到的数据可能只是 local commit， 而不是 majority commit，因此可能会由于  HA 被回滚，需要牺牲一致性。用户也可以通过设置 readConcern 为 linearizable 来同时保证新鲜度和一致性，但是请求延迟会大大增加，而且由于只能在 primary 节点完成，吞吐量会大幅下降。   
- **可用性**。将 write concern 设置为较低级别，比如 1，可以提升写操作的成功率，在 secondary 节点宕机和延迟较大的情况下保证可用性。将副本集中 vote 权限只给少量节点，同样也能在节点宕机的情况下保证选主成功和写操作成功。但是上述策略都在一定程度上牺牲了一致性。    
- **一致性**。通过将 write concern 设置为较高级别，可以强化数据一致性。但是在 secondary 宕机和延迟较大的情况下，可能导致写操作超时失败，因此会牺牲可用性。    
- **分区容忍性**。分布式系统都需要保证这项能力，不存在”可调“空间。   

官方论文[《Tunable Consistency in MongoDB》](https://www.vldb.org/pvldb/vol12/p2071-schultz.pdf)对运行在 Atlas 平台的 14,820 个 4.0.6 内核版本的请求特征进行了统计分析。发现广大用户对 writeConcern 和 readConcern 的使用方式如下：   

<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/a5164fee-583b-4b9c-b7c9-f0c41f9c9762" width=400>
</p>

从官方的统计中，我们能够得到以下启示：   
1. 可调一致性给业务带来了更大的灵活性。从表中可以看到每一种 readConcern 和 writeConcern 都有被使用。
2. 绝大多数都是使用的默认配置。我认为这不说明系统默认配置刚好就能满足绝大多数场景，而是很多用户并不清楚这项配置，也不知道如何在业务代码中进行自定义设置。我之前遇到过一些业务由于没有正确配置 writeConcern 导致有数据丢失的问题，可见业务在使用时有必要确认系统默认配置并且明确其带来的风险。另外，MongoDB 在不同内核版本下的默认配置可能是不一样的，比如 5.0 版本 writeConcern 的默认配置是 { w:”majority“}， 而 4.0 版本的 writeConcern 默认配置是 {w : 1}。因此，我的建议是**不要依赖系统默认配置，尽量在业务代码中进行明确配置，并知晓配置的风险**。


从客户端一致性的维度，MongoDB 提供可调的因果一致性：    
- Read own writes，写后读。本次读请求操作的是执行完上一个写请求的状态机。
- Monotonic reads，单调读。本次读操作得到的数据版本，不会比上一次读操作的版本更低。
- Monotonic writes，单调写。本次写请求操作的是执行完上一个写请求的状态机。
- Writes follow reads，读后写。本次写请求操作的状态机包含上一次读取操作的版本。    

如果因果一致性必须包含持久性，则必须 {readConcern: "majority"} 且 {writeConcern: "majority"} 才能满足要求：

<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/94fa0054-e5a0-4363-ba24-1fe9439727ae" width=700>
</p>

我们可以稍作推理来验证上述结论。以 read own writes 为例，设置 writeConcern 为 {w:1} 无法保证数据被 majority 提交，存在会被回滚的风险，因此无法保证下个操作一定能读到。设置 readConcern 为 local，可能读到存在回滚风险的数据（比如网络分区时，读到了少数派节点），也不满足需求。更多分析可以参考[官方文档](https://www.mongodb.com/docs/v4.2/core/causal-consistency-read-write-concerns/)。   
在因果一致性不需要完全满足持久性的情况下，可以放松上述要求。

MongoDB 作为一个商业数据库，性能是其架构设计时的重要考虑因素。因此，MongoDB 没有采用全局因果一致性方案(full dependency tracking)，因为维护和分析全局的依赖关系非常耗性能。
MongoDB 通过[混合逻辑时钟](https://dl.acm.org/doi/pdf/10.1145/3299869.3314049)(Hybrid Logical Clock, HLC) 来维护每个 client 内部多个请求的顺序。**HLC 由 <int32: unix second> 和 <int32: counter> 共同组成**，也叫 clusterTime 在集群中传播。Primary 节点在执行写请求时推进 HLC，并将其写入到 oplog，也会跟随数据写入到存储引擎，作为 MVCC 版本控制的依据。    
Primary 节点作为 HLC 的推动者，集群中的其他节点（client, mongos, config server, shard secondary）作为 HLC 的传播者，通过 gossip 机制实现全局同步。   

下图说明 Client 如何通过 clusterTime 在 secondary 节点上完成"read own write":    

<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/91397078-d153-4106-9891-7f8b93d895ba" width=500>
</p>

1. Client 给 primary 节点发送写请求。
2. Primary 执行请求，并推进 opTime 到 T2，并记录 oplog 进行异步复制。
3. Primary 将 opTime T2 作为返回结果的元数据传递到 Client.
4. Client 推进 opTime 到 T2.
5. Client 给 secondary 节点发起读请求，并携带 {afterClusterTime: T2}，表示要读取 T2 及之后版本的数据。
6. Secondary 判断自己的同步进度有没有到 T2, 如果没有则等待。
7. 如果满足条件，则执行读请求，并返回文档。

HLC 主要还是解决如何定序，管理数据版本的问题。在此之前，业界已经存在一些经典的解决方案，比较知名的有 Lamport 的逻辑时钟，Google Spanner 的时钟服务等。MongoDB 最终还是采用了 HLC 方案，可以从以下几个问题进行分析。    
1. **为什么不采用逻辑时钟，而加入了物理时间（wall clock）?**   
真实的业务场景往往是伴随物理时间的，比如按时间点读取数据，按时间点回档数据库等，能精确到 xx年xx月xx 日xx时xx分xx秒。如果只用逻辑时间，很多业务场景都无法满足。
2. **为什么不使用 Spanner 的 TrueTime 时钟服务？**   
MongoDB 官方认为会增加读写请求的延迟（每次执行请求之前都要调用 TrueTime 的 API），而且增加部署难度（额外的服务和硬件）。
3. **为什么不直接用更精细的纳秒时钟，而是采用 秒级时钟 + 秒内counter 的方式？**   
引入物理时钟之后，面临的问题是如何保证时钟一直会持续递增？一般部署 MongoDB 的机器都会采用 NTP 服务进行时钟对齐，那物理时间就有回拨的风险。引入 counter 能够保证即使机器时钟回拨，也不倒退 HLC的物理时钟，而是推进 counter 来保证递增。**换句话说，机器时间只是 HLC 的参考，但不是绝对权威**，具体可以参考 [LogicalClock::reserveTicks](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/logical_clock.cpp#L98) 代码的实现。但是一般来说，NTP 能够保证机器时钟不会有巨幅波动，所以可认为 HLC 的物理时间 == 机器时间。
4. **HLC如何保证安全性？**   
如果由于黑客攻击，将集群中的 HLC 推进到了 Unix 时间的最大值，则集群将无法继续处理写请求。因此，MongoDB 对集群中传递的 HLC 增加了 Hash 签名校验，密钥只有集群内部知道，而且定期更新。另外代码中也对 HLC的推进作了限制，默认每次推进的时间不能超过 1年。
Hash 校验保证了安全性，但是牺牲了计算资源。MongoDB 为了提升性能进行了2点针对性地优化：a. 区分[特权认证](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/logical_time_validator.cpp#L200)的连接，这些被信任连接的请求可以免去 Hash 校验；b. 充分利用 HLC [只能向前推进](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/logical_time_validator.cpp#L161)的特性，通过[缓存](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/time_proof_service.h#L44)避免重复的 Hash 计算。
5. **HLC 如何传播？**   
采用 gossip 机制，所有读写请求涉及的节点（Client 和 MongoDB中的各个节点）都会参与传播。但是只有  primary 节点有权限推进 HLC。

### 3.6.3 小结
1. Raft 提供强一致性协议。所有读写请求都集中在 leader 节点处理，都是操作的 majority committed 数据，能够满足因果一致性。   
2. MongoRaft 支持 writeConcern、readPreference、readConcern 动态配置，提供了可调一致性，并且这些配置下放到了请求级别。用户可以根据自身在吞吐、延迟、一致性、数据新鲜度、可用性方面的业务需求，进行灵活配置。   
3. MongoRaft 采用了混合逻辑时钟进行全局时钟同步，支持客户端级别的因果一致性。   

# 4. 总结

MongoRaft 基于 Raft，但不止于 Raft。本文从节点角色状态、日志复制、选主流程、配置变更、持久性、一致性方面深入对比了 2 者的不同点，并着重突出了 MongoRaft 在设计上的优势和实现细节。总结如下：   

|协议子模块|Raft|MongoRaft|
|:--|:--|:--|
|节点角色/状态|leader、candidate、follower、（learner）|角色：primary、 secondary、 **rollback、  recovering、startup、 startup2， arbiter**<br>节点配置：votes 、 **priority、 hidden、buildIndexes、 tags、 arbiterOnly、 slaveDelay**|
|日志复制|推日志模型（leader->follower）<br>不支持链式复制<br>按日志顺序提交<br>采用递增 ID 标记日志<br>先提交日志到大多数节点，再 apply 到状态机| **拉日志模型（secondary主动拉），4.4 版本开始推拉结合（streaming replication）**<br>**支持链式复制**<br>按日志顺序提交，但是一个 batch 内的 oplog 并发乱序回放<br>**引入混合逻辑时钟标记日志**，其中的物理时间能更好的支撑业务需求<br>**每个节点的日志同步和日志应用是”同时完成“的，按 batch 最大程度地并发 apply**|
|选主流程|探活机制： 心跳<br>按 term 区分不同届的选举，不直接提交上一个term 的日志<br>“大多数”选举机制<br>|探活机制：心跳（**任意2节点互发**)+（链式）**复制链路**（spanning tree），网络容错性更高<br>按term 区分不同届的选举，不直接提交上一个 term 的日志<br>“大多数”选举机制<br>支持**主动发起选举**（主动 stepDown, priority takeover）<br>**引入 dry-run 机制**，提升真正选举时的成功率，避免不必要的 term递增和主节点 stepDown<br>**引入 catch-up 机制**，尽可能挽救写到主节点（local commit）但没有提交（majority commit）的数据|
|配置变更|支持动态配置<br>依赖主节点操作<br>2 阶段算法保证安全性|支持动态配置<br>**logless**，利用心跳将配置变更和数据复制链路分离，能够很大程度上缓解主从延迟大时配置变更耗时长的问题<br>**支持 force(unsafe) 模式，不强依赖主节点**<br>2 阶段算法的变种保证安全性|
|持久性|snapshot + log|checkpoint + log<br>checkpoint分 2 种：**unstable**（包含 local commit 数据） 和 stable（只包含 majority commit 数据）<br>log 包含 2 部分：**oplog 表 + 存储引擎 journal日志**，用户可控制 journal刷盘策略|
|一致性|强一致性协议<br>日志复制到大多数节点才算提交<br>读leader，读 majority commit 数据<br>支持线性一致性（通过额外的机制保证）|**可调一致性**，用户可以在吞吐、延迟、可用性、一致性、数据新鲜度等方面根据自己的需求灵活使用。<br>支持多个配置选项：<br> - writeConcern: 写请求复制到多少个节点再返回，是否要等待 journal刷盘<br> - readPreference: 读哪些节点，支持读 secondary 节点<br>- readConcern: 读哪个版本的数据，包括 available 、local、majority、snapshot、linearizable<br>引入**混合逻辑时钟**，支持客户端级别的因果一致性、线性一致性|

MongoRaft 吸收 Raft 的优点，并结合自身特点进行改造后，进行了工程化落地。更难能可贵的是，MongoRaft 目前广泛被使用在全球各种真实的互联网业务中，是久经考验、被证明完备的一致性协议。   

# 5. 参考文档
1. Design and Analysis of a Logless Dynamic Reconfiguration Protocol， https://drops.dagstuhl.de/opus/volltexte/2022/15801/pdf/LIPIcs-OPODIS-2021-26.pdf
2. In Search of an Understandable Consensus Algorithm (Extended Version)，https://raft.github.io/raft.pdf
3. https://raft.github.io/
4. https://github.com/mongodb/mongo/blob/r4.4.24/src/mongo/db/repl/README.md
5. 共识协议的技术变迁，https://mp.weixin.qq.com/s/UY9TPMcuf0O7xS0kuXTcVw
6. Fault-Tolerant Replication with Pull-Based Consensus in MongoDB PPT： https://www.usenix.org/conference/nsdi21/presentation/zhou
7. Fault-Tolerant Replication with Pull-Based Consensus in MongoDB论文：https://www.usenix.org/system/files/nsdi21_slides_zhou-siyuan.pdf
8. MongoDB Replica Set Members: https://www.mongodb.com/docs/manual/core/replica-set-members/
9. MongoShake——基于MongoDB的跨数据中心的数据复制平台：https://developer.aliyun.com/article/603329
10. PolarFS：https://www.vldb.org/pvldb/vol11/p1849-cao.pdf
11. MongoDB streaming replication：https://www.mongodb.com/docs/v4.4/core/replica-set-sync/#streaming-replication
12. https://github.com/mongodb/mongo/blob/r5.0.16/src/mongo/db/repl/README.md#oplog-fetcher-implementation-details
13. MongoDB flow control 官方文档：https://www.mongodb.com/docs/v4.4/tutorial/troubleshoot-replica-sets/#flow-control
14. MongoDB flow control wiki：https://github.com/mongodb/mongo/blob/r5.0.16/src/mongo/db/catalog/README.md#flow-control
15. WiredTiger Checkpoints: https://github.com/wiredtiger/wiredtiger/wiki/Checkpoints
16. Read Isolation, Consistency, and Recency: https://www.mongodb.com/docs/v4.2/core/read-isolation-consistency-recency/
17. Tunable Consistency in MongoDB： https://www.vldb.org/pvldb/vol12/p2071-schultz.pdf
18. MongoDB 中文社区 | MongoDB 一致性模型设计与实现：https://mongoing.com/archives/77853
19. Implementation of Cluster-wide Logical Clock and Causal Consistency in MongoDB : https://dl.acm.org/doi/pdf/10.1145/3299869.3314049
20. Causal Consistency and Read and Write Concerns: https://www.mongodb.com/docs/v4.2/core/causal-consistency-read-write-concerns/
21. DDIA 一致性和共识：https://github.com/Vonng/ddia/blob/master/ch9.md
22. Etcd 代码：https://github.com/etcd-io/etcd/blob/release-3.4/contrib/raftexample/raft.go
23. 深度解析 Raft 分布式一致性协议： https://juejin.cn/post/6907151199141625870
24. VLDB论文解读｜PolarDB MySQL高性能强一致集群核心技术详解： https://mp.weixin.qq.com/s/yxRsxvWrqumeCahH528mdA
25. TiKV | Multi-raft: https://tikv.org/deep-dive/scalability/multi-raft/#:~:text=Here%20Multi-Raft%20only%20means%20we%20manage%20multiple%20Raft,Raft%20groups%20in%20terms%20of%20partitions%2C%20namely%2C%20Region.


