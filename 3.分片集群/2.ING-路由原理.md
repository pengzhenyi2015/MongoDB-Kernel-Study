# 导语
分片集群的核心就在于如何在多个分片中高效地组织数据，而这一切的核心就在于路由的设计。

接下来的分析主要基于 4.2 版本内核代码，并通过附带介绍高版本特性的方式，探讨关于路由设计的一些细节问题：
1. 路由的结构：路由表中的 range ，shard 和版本，节点缓存中的 Map 结构。
2. 路由算法：Hash 和 Range.
3. 节点的路由更新算法和版本管理。
4. 路由分裂 split.
5. 高版本内核对路由更新算法的改进。

另外，关于 chunk 迁移的 balancer 流程，由于非常复杂，后面单独作为一个章节进行分析。

# 1. 路由结构
## 1.1 库路由和表路由
不同的分布式数据库系统，对表类型的支持不尽相同。    
有些分布式数据库，只支持按 shardKey 进行分片，如果不显示指定 shardKey, 则默认使用主键或者第1列作为 shardKey.    
有些分布式数据库，既支持分片表，也支持非分片表。比如 [TDSQL](https://doc.fincloud.tencent.cn/tcloud/Database/TDSQL/705974/72883/59424) 支持单表（Noshard表），应用在一些数据量很小的场景。   

对于 MongoDB 分片集群来说，分片表和非分片表都支持，特点如下：    
- 分片表：需要使用 shardCollection 命令并指定 shardKey, 进行显式创建。支持 range 和 hash 两种分片算法。适用于大数据量，有横向扩展需求的场景；    
- 非分片表：默认隐式创建，无需指定 shardKey. 适用于数据量较小的场景。使用起来不会有 shardKey 相关的限制（比如 shardKey 无法更新），因此更加灵活；        

对于非分片表来说，是按照 **库（DB）级别** 组织的。每次创建一个 DB 时，会选择一个 shard 作为该 DB 的 primary shard, 该 DB 下的所有非分片表都会存储在这个 shard 上。如下图所示：      

<p align="center">
<img width="591" height="281" alt="image" src="https://github.com/user-attachments/assets/20747aaa-5a7d-4803-b3bd-c4c9124c1d55" />
</p>

MongoDB 的非分片表在设计上和 TDSQL 有些不一样，TDSQL 默认会将非分片表存储在第一个 set 上（也就是 MongoDB 中的第一个 shard 上）。带来的好处是路由非常简单，但是如果非分片表比较多，那么第一个 set 的压力会很大。     
而 MongoDB 的非分片表，存储在对应 DB 的 primary shard 上。如果非分片表比较多，但是分属于不同的 DB，而这些 DB 的 primary shard 比较分散，那么整体的负载也会比较均匀。        
不过这样设计的代价就是：需要设计一个 primary shard 选择算法保证均匀分布，另外需要一层 DB 级别的路由。     

Primary shard 的选择算法参考 [ShardingCatalogManager::_selectShardForNewDatabase](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/s/config/sharding_catalog_manager_shard_operations.cpp#L861), 核心逻辑是对每个 shard 执行 listDatabases 命令，得到其 totalSize。然后选择一个当前totalSize 最小（即空闲空间最多）的 shard，作为当前库的 primaryShard.       

库级别的路由，存储在 configSvr 的 config.databases 系统表中，举例如下：
```
mongos> db.getSiblingDB("config").databases.findOne()
{
        "_id" : "db1",     # DB 名称
        "primary" : "shard2",  # 该 DB 的 primary shard
        "partitioned" : true,
        "version" : {
                "uuid" : UUID("b00661ff-764d-4715-8361-830c9ff92072"),
                "lastMod" : 1
        }
}
```
表级别的路由，通过 config.collections 和 config.chunks 2个系统表维护。    
其中 config.collections 主要维护了分片表的 shardKey 和 分片算法（range or hash）等信息。举例如下：
```
mongos> db.getSiblingDB("config").collections.findOne()
{
        "_id" : "config.system.sessions",
        "lastmodEpoch" : ObjectId("67f8c93b0ac6a9eaaa094574"),
        "lastmod" : ISODate("1970-02-19T17:02:47.296Z"),
        "dropped" : false,
        "key" : {
                "_id" : 1    # shardKey 和 分片算法， 1 表示 range，"hashed" 表示 hash 算法
        },
        "unique" : false,  # shardKey 是否有唯一约束
        "uuid" : UUID("5cb70bae-4af1-49f5-978f-5931f15a9199")
}
```
而 config.chunks 主要维护了分片表在各个 shard 上的分布情况。举例如下：    

```
mongos> db.getSiblingDB("config").chunks.findOne({"ns": "db1.hash1"})
{
        "_id" : "db1.hash1-_id_MinKey",
        "ns" : "db1.hash1",    # 表名
        "min" : {
                "_id" : { "$minKey" : 1 }   # chunkRange 的最小值
        },
        "max" : {
                "_id" : NumberLong("-4611686018427387902")  # chunkRange 的最大值
        },
        "shard" : "shard1",  # 所属 shard
        "lastmod" : Timestamp(1, 0),  # 版本信息
        "lastmodEpoch" : ObjectId("68a28492d2b8e43aff985799"),
        "history" : [
                {
                        "validAfter" : Timestamp(1755481234, 6),
                        "shard" : "shard1"
                }
        ]
}
```

## 1.2 路由如何指引读写操作
以读请求为例，mongos 节点在处理请求时：    
- 如果是非分片表，则直接将请求[转发到 DB 的 primaryShard](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/s/cluster_commands_helpers.cpp#L550).    
- 如果是分片表：
  - 如果包含了 shardKey，则[根据 shardKey 查找路由表](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/s/chunk_manager.cpp#L160)，得到对应的 shardId 集合，进行转发；
  - 如果包含了 shardKey，且是一个范围查询，则[根据 shardKey 的范围，查找路由表](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/s/chunk_manager.cpp#L676)，得到对应的 shardId 集合，进行转发。一个比较特殊的例子，就是请求中没有携带 shardKey，此时会对所有 shard 进行广播；

mongos 节点会读取路由表中的信息，并在内存中进行缓存和更新。关于内存中缓存路由的数据结构，不同的内核版本会有一些差异。       
在 4.0 版本中，路由在内存中的组织形式，是一个 [std::map<std::string, std::shared_ptr\<ChunkInfo\>>](https://github.com/mongodb/mongo/blob/r4.0.3/src/mongo/s/chunk_manager.h#L53).    
其中 key 代表了chunk 的 max 信息，用于通过 upper_bound 方法快速定位到 shardKey 所属的 ChunkInfo，其中 ChunkInfo 包含了 chunkRange, shardId, version, history 等基本信息。

在高内核版本，比如 r4.2.25，内存 map 结构不再是简单的 `max -> ChunkInfo`， 而是 `max -> vector<ChunkInfo>`:
```
// Vector of chunks ordered by max key in ascending order.
using ChunkVector = std::vector<std::shared_ptr<ChunkInfo>>;
using ChunkVectorMap = std::map<std::string, std::shared_ptr<ChunkVector>>;
```   

之所以有这样的优化，是因为对于超大表来说，chunk数会非常多，导致 map 很大。超大表的 split，balancer 等频率也会更高，导致的路由变更会比较频繁。如果还是使用一个大 map 来存储，那么在路由变更时，会频繁锁住整个 map，导致性能抖动比较频繁。     
而改用 `vector<ChunkInfo>` 之后，一次小的 chunk 路由变更，不再需要锁住整个 map，而是只对一小部分的 chunk 访问有影响。因此，能很大程度上提升服务的稳定性。   
由于 ChunkVector 内部是有序的，因此可以通过二分查找快速定位到对应的 ChunkInfo。查找性能相比之前的版本也不受影响。            
关于上述优化的详细信息，可以参考 MongoDB 官方的 JIRA [SERVER-71627](https://jira.mongodb.org/browse/SERVER-71627).


# 2. 分片算法 Range vs Hash
MongoDB 目前支持 range 和 hash 2 种[分片算法](https://www.mongodb.com/docs/manual/sharding/#sharding-strategy)。      

Range 算法会将数据按照 shardKey 的范围进行划分，每个 chunk 对应一个 range。举例来说，如果使用 _id 作为 shardKey, 那么每个 chunk 会包含一个连续的 _id 值区间。    

<p align="center">
<img width="760" height="250" alt="image" src="https://github.com/user-attachments/assets/453464c2-dd88-4912-aad7-513599d5cff9" />
</p>

Hash 算法会将数据按照 shardKey 的 hash 值进行划分，每个 chunk 对应一个 hash 值区间。举例来说，如果使用 _id 作为 shardKey, 那么每个 chunk 会包含一个连续的 hash 值区间。      

<p align="center">
<img width="700" height="250" alt="image" src="https://github.com/user-attachments/assets/a44fed28-9f36-4b42-9158-fba32183e222" />
</p>

下面根据使用场景的不同，对比 2 种算法的优缺点：    
|特性|Range|Hash|
|---|---|---|
|范围操作|对于**小范围查询有明显优势**，可以单播或者多播到部分 shard <br> 不过这也意味着，如果数据是**按照 shardKey 单调递增或者递减进行写入的，那么性能也会很差**|需要广播到所有 shard，而且需要再额外创建一个 shardKey 的 btree 索引来加速范围查询|
|sharKey 唯一性|天然支持|默认会创建 shardKey 的 hash 索引，而 hash 索引不能够保证唯一性。<br>因此需要额外创建 shardKey 的 btree 唯一索引，会增加索引的维护开销|
|[预分片](https://www.mongodb.com/zh-cn/docs/v7.0/reference/method/sh.shardCollection/)|不支持，最初只有 1 个 chunk，随着数据量的增加，会分裂成多个 chunk，并进行内部迁移|支持，可以提前创建多个 chunk，有效避免分裂和迁移的次数，性能更加平稳|

# 3. Chunk 分裂和版本管理


# 4. 惰性路由同步


# 5. 总结


# 6. 参考文献
1. TDSQL的表类型：https://doc.fincloud.tencent.cn/tcloud/Database/TDSQL/705974/72883/59424
2. MongoDB 中路由缓存的优化：https://jira.mongodb.org/browse/SERVER-71627、
3. https://github.com/mongodb/mongo/tree/r4.2.25
4. https://github.com/mongodb/mongo/blob/r4.0.3
5. https://www.mongodb.com/docs/manual/sharding/#sharding-strategy
6. https://www.mongodb.com/zh-cn/docs/v7.0/reference/method/sh.shardCollection/




