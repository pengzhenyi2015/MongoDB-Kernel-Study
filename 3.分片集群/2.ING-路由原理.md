# 导语
分片集群的核心就在于如何在多个分片中高效地组织数据，而这一切的核心就在于路由的设计。

接下来的分析主要基于 4.2 版本内核代码，并通过附带介绍高版本特性的方式，探讨关于路由设计的一些细节问题：
1. 路由的结构：路由表中的 range ，shard 和版本，节点缓存中的 Map 结构。
2. 路由算法：Hash 和 Range.
3. 节点的路由更新算法和版本管理。
4. 路由分裂 split.
5. 高版本内核对路由更新算法的改进。

另外，关于 chunk 迁移的 balancer 流程，由于非常复杂，后面单独作为一个章节进行分析。

# 1. 路由结构
## 1.1 库路由和表路由
不同的分布式数据库系统，对表类型的支持不尽相同。    
有些分布式数据库，只支持按 shardKey 进行分片，如果不显示指定 shardKey, 则默认使用主键或者第1列作为 shardKey.    
有些分布式数据库，既支持分片表，也支持非分片表。比如 [TDSQL](https://doc.fincloud.tencent.cn/tcloud/Database/TDSQL/705974/72883/59424) 支持单表（Noshard表），应用在一些数据量很小的场景。   

对于 MongoDB 分片集群来说，分片表和非分片表都支持，特点如下：    
- 分片表：需要使用 shardCollection 命令并指定 shardKey, 进行显式创建。支持 range 和 hash 两种分片算法。适用于大数据量，有横向扩展需求的场景；    
- 非分片表：默认隐式创建，无需指定 shardKey. 适用于数据量较小的场景。使用起来不会有 shardKey 相关的限制（比如 shardKey 无法更新），因此更加灵活；        

对于非分片表来说，是按照 **库（DB）级别** 组织的。每次创建一个 DB 时，会选择一个 shard 作为该 DB 的 primary shard, 该 DB 下的所有非分片表都会存储在这个 shard 上。如下图所示：      

<增加图片>

MongoDB 的非分片表在设计上和 TDSQL 有些不一样，TDSQL 默认会将非分片表存储在第一个 set 上（也就是 MongoDB 中的第一个 shard 上）。带来的好处是路由非常简单，但是如果非分片表比较多，那么第一个 set 的压力会很大。     
而 MongoDB 的非分片表，存储在对应 DB 的 primary shard 上。如果非分片表比较多，但是分属于不同的 DB，而这些 DB 的 primary shard 比较分散，那么整体的负载也会比较均匀。        
不过这样设计的代价就是：需要设计一个 primary shard 选择算法保证均匀分布，另外需要一层 DB 级别的路由。     

Primary shard 的选择算法参考 [ShardingCatalogManager::_selectShardForNewDatabase](), 核心逻辑是对每个 shard 执行 listDatabases 命令，得到其 totalSize。然后选择一个当前totalSize 最小（即空闲空间最多）的 shard，作为当前库的 primaryShard.       

库级别的路由，存储在 configSvr 的 config.databases 系统表中，举例如下：
```
mongos> db.getSiblingDB("config").databases.findOne()
{
        "_id" : "db1",     # DB 名称
        "primary" : "shard2",  # 该 DB 的 primary shard
        "partitioned" : true,
        "version" : {
                "uuid" : UUID("b00661ff-764d-4715-8361-830c9ff92072"),
                "lastMod" : 1
        }
}
```
表级别的路由，通过 config.collections 和 config.chunks 2个系统表维护。    
其中 config.collections 主要维护了分片表的 shardKey 和 分片算法（range or hash）等信息。举例如下：
```
mongos> db.getSiblingDB("config").collections.findOne()
{
        "_id" : "config.system.sessions",
        "lastmodEpoch" : ObjectId("67f8c93b0ac6a9eaaa094574"),
        "lastmod" : ISODate("1970-02-19T17:02:47.296Z"),
        "dropped" : false,
        "key" : {
                "_id" : 1    # shardKey 和 分片算法， 1 表示 range，"hashed" 表示 hash 算法
        },
        "unique" : false,  # shardKey 是否有唯一约束
        "uuid" : UUID("5cb70bae-4af1-49f5-978f-5931f15a9199")
}
```
而 config.chunks 主要维护了分片表在各个 shard 上的分布情况。举例如下：    

```
mongos> db.getSiblingDB("config").chunks.findOne({"ns": "db1.hash1"})
{
        "_id" : "db1.hash1-_id_MinKey",
        "ns" : "db1.hash1",    # 表名
        "min" : {
                "_id" : { "$minKey" : 1 }   # chunkRange 的最小值
        },
        "max" : {
                "_id" : NumberLong("-4611686018427387902")  # chunkRange 的最大值
        },
        "shard" : "shard1",  # 所属 shard
        "lastmod" : Timestamp(1, 0),  # 版本信息
        "lastmodEpoch" : ObjectId("68a28492d2b8e43aff985799"),
        "history" : [
                {
                        "validAfter" : Timestamp(1755481234, 6),
                        "shard" : "shard1"
                }
        ]
}
```

## 1.2 路由如何指引读写操作
以读请求为例，mongos 节点在处理请求时：    
- 如果是非分片表，则直接将请求[转发到 DB 的 primaryShard]().    
- 如果是分片表：
  - 请求中没有包含 shardKey，则进行广播；
  - 如果包含了 shardKey，则根据 shardKey 查找路由表，得到对应的 shardId 集合，进行转发；

mongos 节点会读取路由表中的信息，并在内存中进行缓存和更新。关于内存中缓存路由的数据结构，不同的内核版本会有一些差异。       
在 4.0 版本中，路由在内存中的组织形式，是一个 [std::map<std::string, std::shared_ptr<ChunkInfo>>]().    
其中 key 代表了chunk 的 max 信息，用于通过 upper_bound 方法快速定位到 shardKey 所属的 ChunkInfo，其中 ChunkInfo 包含了 chunkRange, shardId, version, history 等基本信息。

在高内核版本，比如 r4.2.25，内存 map 结构不在是简单的 `max -> ChunkInfo`， 而是 `max -> vector<ChunkInfo>`:
```
// Vector of chunks ordered by max key in ascending order.
using ChunkVector = std::vector<std::shared_ptr<ChunkInfo>>;
using ChunkVectorMap = std::map<std::string, std::shared_ptr<ChunkVector>>;
```   

之所以有这样的优化，是因为对于超大表来说，chunk数会非常多，导致 map 很大。超大表的 split，balancer 等频率也会更高，导致的路由变更会比较频繁。如果还是使用一个大 map 来存储，那么在路由变更时，会频繁锁住整个 map，导致性能抖动比较频繁。     
而改用 `vector<ChunkInfo>` 之后，一次小的 chunk 路由变更，不再需要锁住整个 map，而是只对一小部分的 chunk 访问有影响。因此，能很大程度上提升服务的稳定性。   
由于 ChunkVector 内部是有序的，因此可以通过二分查找快速定位到对应的 ChunkInfo。查找性能相比之前的版本也不受影响。            
关于上述优化的详细信息，可以参考 MongoDB 的 JIRA [SERVER-71627](https://jira.mongodb.org/browse/SERVER-71627).


# 2. 分片算法 Range vs Hash


# 3. Chunk 分裂和版本管理


# 4. 惰性路由同步


# 5. 总结


# 6. 参考文献
1. TDSQL的表类型：https://doc.fincloud.tencent.cn/tcloud/Database/TDSQL/705974/72883/59424
2. MongoDB 中路由缓存的优化：https://jira.mongodb.org/browse/SERVER-71627
