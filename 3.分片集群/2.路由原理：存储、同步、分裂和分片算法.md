# 导语
分片集群的核心就在于如何在多个分片中高效地组织数据，而这一切的核心就在于路由的设计。

接下来的分析主要基于 4.2 版本内核代码，并通过附带介绍高版本特性的方式，探讨关于路由设计的一些细节问题：
1. 路由的结构：路由表中的 range ，shard 和版本，节点缓存中的 Map 结构。
2. 路由算法：Hash 和 Range.
3. 节点的路由更新算法和版本管理。
4. 路由分裂 split.

另外，关于 chunk 迁移的 balancer 流程，由于比较复杂，后面单独作为一个章节进行分析。

# 1. 路由结构
## 1.1 库路由和表路由
不同的分布式数据库系统，对表类型的支持不尽相同。    
有些分布式数据库，只支持按 shardKey 进行分片，如果不显示指定 shardKey, 则默认使用主键或者第1列作为 shardKey.    
有些分布式数据库，既支持分片表，也支持非分片表。比如 [TDSQL](https://doc.fincloud.tencent.cn/tcloud/Database/TDSQL/705974/72883/59424) 支持单表（Noshard表），应用在一些数据量很小的场景。   

对于 MongoDB 分片集群来说，分片表和非分片表都支持，特点如下：    
- 分片表：需要使用 shardCollection 命令并指定 shardKey, 进行显式创建。支持 range 和 hash 两种分片算法。适用于大数据量，有横向扩展需求的场景；    
- 非分片表：默认隐式创建，无需指定 shardKey. 适用于数据量较小的场景。使用起来不会有 shardKey 相关的限制（比如 shardKey 无法更新，无法撇开 shardKey 去创建全局唯一索引，早期版本无法对分片表执行 lookup等），因此更加灵活；        

对于非分片表来说，是按照 **库（DB）级别** 组织的。每次创建一个 DB 时，会选择一个 shard 作为该 DB 的 primary shard, 该 DB 下的所有非分片表都会存储在这个 shard 上。如下图所示：      

<p align="center">
<img width="591" height="281" alt="image" src="https://github.com/user-attachments/assets/20747aaa-5a7d-4803-b3bd-c4c9124c1d55" />
</p>

MongoDB 的非分片表在设计上和 TDSQL 有些不一样，TDSQL 默认会将非分片表存储在第一个 set 上（也就是 MongoDB 中的第一个 shard 上）。带来的好处是路由非常简单，但是如果非分片表比较多，那么第一个 set 的压力会很大。     
而 MongoDB 的非分片表，存储在对应 DB 的 primary shard 上。如果非分片表比较多，但是分属于不同的 DB，而这些 DB 的 primary shard 比较分散，那么整体的负载也会比较均匀。        
不过这样设计的代价就是：需要设计一个 primary shard 选择算法保证均匀分布，另外需要一层 DB 级别的路由。     

Primary shard 的选择算法参考 [ShardingCatalogManager::_selectShardForNewDatabase](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/s/config/sharding_catalog_manager_shard_operations.cpp#L861), 核心逻辑是对每个 shard 执行 listDatabases 命令，得到其 totalSize。然后选择一个当前totalSize 最小（即空闲空间最多）的 shard，作为当前库的 primaryShard.       

库级别的路由，存储在 configSvr 的 config.databases 系统表中，举例如下：
```
mongos> db.getSiblingDB("config").databases.findOne()
{
        "_id" : "db1",     # DB 名称
        "primary" : "shard2",  # 该 DB 的 primary shard
        "partitioned" : true,
        "version" : {
                "uuid" : UUID("b00661ff-764d-4715-8361-830c9ff92072"),
                "lastMod" : 1
        }
}
```
表级别的路由，通过 config.collections 和 config.chunks 2个系统表维护。    
其中 config.collections 主要维护了分片表的 shardKey 和 分片算法（range or hash）等信息。举例如下：
```
mongos> db.getSiblingDB("config").collections.findOne()
{
        "_id" : "config.system.sessions",
        "lastmodEpoch" : ObjectId("67f8c93b0ac6a9eaaa094574"),
        "lastmod" : ISODate("1970-02-19T17:02:47.296Z"),
        "dropped" : false,
        "key" : {
                "_id" : 1    # shardKey 和 分片算法， 1 表示 range，"hashed" 表示 hash 算法
        },
        "unique" : false,  # shardKey 是否有唯一约束
        "uuid" : UUID("5cb70bae-4af1-49f5-978f-5931f15a9199")
}
```
而 config.chunks 主要维护了分片表在各个 shard 上的分布情况。举例如下：    

```
mongos> db.getSiblingDB("config").chunks.findOne({"ns": "db1.hash1"})
{
        "_id" : "db1.hash1-_id_MinKey",
        "ns" : "db1.hash1",    # 表名
        "min" : {
                "_id" : { "$minKey" : 1 }   # chunkRange 的最小值
        },
        "max" : {
                "_id" : NumberLong("-4611686018427387902")  # chunkRange 的最大值
        },
        "shard" : "shard1",  # 所属 shard
        "lastmod" : Timestamp(1, 0),  # 版本信息
        # 这里并不是说使用混合时钟来标识版本，而是说版本信息只是套用了 Timestamp 这个类型的壳
        # 版本信息包含 uint32 的 majorVersion 和 uint32 的 minorVersion
        # 刚好和 TimeStamp 的 uint32 的 Unix时间戳和 uint32 的 count 完美重合
        "lastmodEpoch" : ObjectId("68a28492d2b8e43aff985799"),
        "history" : [
                {
                        "validAfter" : Timestamp(1755481234, 6),
                        "shard" : "shard1"
                }
        ]
}
```

## 1.2 路由如何指引读写操作
以读请求为例，mongos 节点在处理请求时：    
- 如果是非分片表，则直接将请求[转发到 DB 的 primaryShard](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/s/cluster_commands_helpers.cpp#L550).    
- 如果是分片表：
  - 如果包含了 shardKey，则[根据 shardKey 查找路由表](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/s/chunk_manager.cpp#L160)，得到对应的 shardId 集合，进行转发；
  - 如果包含了 shardKey，且是一个范围查询，则[根据 shardKey 的范围，查找路由表](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/s/chunk_manager.cpp#L676)，得到对应的 shardId 集合，进行转发。一个比较特殊的例子，就是请求中没有携带 shardKey，此时会对所有 shard 进行广播；

mongos 节点会读取路由表中的信息，并在内存中进行缓存和更新。关于内存中缓存路由的数据结构，不同的内核版本会有一些差异。       
在 4.0 版本中，路由在内存中的组织形式，是一个 [std::map<std::string, std::shared_ptr\<ChunkInfo\>>](https://github.com/mongodb/mongo/blob/r4.0.3/src/mongo/s/chunk_manager.h#L53).    
其中 key 代表了chunk 的 max 信息，用于通过 upper_bound 方法快速定位到 shardKey 所属的 ChunkInfo，其中 ChunkInfo 包含了 chunkRange, shardId, version, history 等基本信息。

在高内核版本，比如 r4.2.25，内存 map 结构不再是简单的 `max -> ChunkInfo`， 而是 `max -> vector<ChunkInfo>`:
```
// Vector of chunks ordered by max key in ascending order.
using ChunkVector = std::vector<std::shared_ptr<ChunkInfo>>;
using ChunkVectorMap = std::map<std::string, std::shared_ptr<ChunkVector>>;
```   

之所以有这样的优化，是因为对于超大表来说，chunk数会非常多，导致 map 很大。超大表的 split，balancer 等频率也会更高，导致的路由变更会比较频繁。如果还是使用一个大 map 来存储，那么在路由变更时，会导致性能抖动比较频繁。     
而改用 `vector<ChunkInfo>` 之后，路由变更的效率更高，能很大程度上提升服务的稳定性。   
由于 ChunkVector 内部是有序的，因此可以通过二分查找快速定位到对应的 ChunkInfo。查找性能相比之前的版本也不受影响。            
关于上述优化的详细信息，可以参考 MongoDB 官方的 JIRA [SERVER-71627](https://jira.mongodb.org/browse/SERVER-71627).


# 2. 分片算法 Range vs Hash
MongoDB 目前支持 range 和 hash 2 种[分片算法](https://www.mongodb.com/docs/manual/sharding/#sharding-strategy)。      

Range 算法会将数据按照 shardKey 的范围进行划分，每个 chunk 对应一个 range。举例来说，如果使用 _id 作为 shardKey, 那么每个 chunk 会包含一个连续的 _id 值区间。    

<p align="center">
<img width="760" height="250" alt="image" src="https://github.com/user-attachments/assets/453464c2-dd88-4912-aad7-513599d5cff9" />
</p>

Hash 算法会将数据按照 shardKey 的 hash 值（md5算法）进行划分，每个 chunk 对应一个 hash 值区间（int64取值空间）。举例来说，如果使用 _id 作为 shardKey, 那么每个 chunk 会包含一个连续的 hash 值区间。      

<p align="center">
<img width="700" height="250" alt="image" src="https://github.com/user-attachments/assets/a44fed28-9f36-4b42-9158-fba32183e222" />
</p>

下面根据使用场景的不同，对比 2 种算法的优缺点：    
|特性|Range|Hash|
|---|---|---|
|范围操作|对于**小范围查询有明显优势**，可以单播或者多播到部分 shard <br> 不过这也意味着，如果数据是**按照 shardKey 单调递增或者递减进行写入的，那么性能也会很差**|需要广播到所有 shard，而且需要再额外创建一个 shardKey 的 btree 索引来加速范围查询|
|sharKey 唯一性|天然支持|默认会创建 shardKey 的 hash 索引，而 hash 索引不能够保证唯一性。<br>因此需要额外创建 shardKey 的 btree 唯一索引，会增加索引的维护开销|
|[预分片](https://www.mongodb.com/zh-cn/docs/v7.0/reference/method/sh.shardCollection/)|不支持，最初只有 1 个 chunk，随着数据量的增加，会分裂成多个 chunk，并进行内部迁移|支持，可以提前创建多个 chunk，有效避免分裂和迁移的次数，性能更加平稳|

# 3. 路由版本

## 3.1 DB version

[DBVersion](https://github.com/mongodb/mongo/blob/r8.0.9/src/mongo/s/database_version.h) 的表现形式为 DBV\<U, T, Mod\>, 每个元素的含义为：
- **U** (uuid) : 一个全局唯一的 UUID，在 DB 被删除后又创建同名 DB 时，这个值也会变化。
- **T** (timestamp) : 5.0 内核版本开始引入的时间戳，作为 UUID 的补充，提供时间标识。
- **M** (last modified) : 一个自增整数，当使用 movePrimary 切换了 primaryShard 时，这个值递增。

## 3.2 Collection/Shard version
[ShardVersion](https://github.com/mongodb/mongo/blob/r8.0.9/src/mongo/s/shard_version.h) 的表现形式为 SV\<E, T, M, m, I\>, 每个元素的含义为：

1. **E** (epoch) : 表的全局 ID;
2. **T** (timestamp) : 5.0 内核版本开始引入的时间戳，作为 epoch 的补充。
3. **M** (major version) : 数据分布的 major 版本，随着 chunk migration 动作而增加。
4. **m** (minor version) : 数据分布的 minor 版本，随着 chunk 的 split/merge 而增加。
5. **I** (index version) : global index 的版本信息，如果一个分片表增加或者删除了 global index, 这个值会增加。

> 关于 global index, 截至目前（2025.10），在 MongoDB 官方文档中还并未找到指引文档。    
> 从代码提交历史来看，6.x 版本已经开始引入了，推测目前还没有正式推出这个特性。关于其实现原理，可以参考[官方文档](https://github.com/mongodb/mongo/blob/r8.0.9/src/mongo/db/s/global_index/README.md)

集群中每个节点维护的路由信息类型，从小到大为 chunkVersion --> shardVersion --> collectionVersion, 各自的作用不尽相同：
- **chunkVersion** : 每个 chunk 独立的版本信息，chunkVersion 会在 configSvr 中的路由表进行持久化存储。
- **shardVersion** : 每个 shard 的版本信息，取值为该 shard 上所属 chunkVersions 中的最大版本号，会**作为 mongos 和 shard 节点之间路由校验的依据**。这个值不会持久化存储，而是每个节点根据 chunkVersions 自己推理出来。
- **collectionVersion** : 每个 collection 的版本信息，取值为所有 shardVersions 的最大值（显然也是所有 chunkVersions 的最大值）,**会作为 chunkVersion 增加的基准，也会作为增量加载路由的依据**。这个值不会持久化存储，而是每个节点根据 shardVesions 自己推理出来。

在 4.x 内核版本中，ShardVersion 中包含的信息为 SV\<E, M, m\>, 相比之下少了 timestamp 和 index version 信息。下面的讲述主要基于 r4.2.25 内核版本。

# 4. Chunk 分裂
路由版本的变更，主要来源于 chunk 的分裂和迁移。其中 chunk 分裂会导致 minorVersion 增加；而 chunk 迁移会影响分布，导致 majorVersion 增加。      
由于 chunk 迁移涉及的内容比较多，因此后面会单独开一个章节来讲述。这里主要讲述 chunk 分裂的逻辑。   

为了了解 chunk 分裂的来龙去脉，接下来我们分 3 个问题进行展开：    
1. 分裂的触发条件和时机；
2. 如何执行分裂；
3. 分裂后如何更新路由信息，这个问题也是我们关注的重点；

**分裂时机**    

分裂操作由 shardSvr 触发并完成。    
ShardSvr 节点启动时，会注册一个 [ShardServerOpObserver](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/s/shard_server_op_observer.h#L41) 观察者，在分片表接受写入操作时会调用对应观察者的 onInserts 和 onUpdate 函数。   
观察者会根据写入的数据，找到其对应的 chunk 信息，并将数据大小累计到对应的 [ChunkWritesTracker](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/s/chunk_writes_tracker.h#L37) 中。如果 ChunkWritesTracker 中累计的数据大小超过了 maxChunkSize 的 20%，则尝试触发分裂。

**执行分裂**

分裂的动作只能由 shardSvr 的 [primary 节点](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/s/chunk_splitter.cpp#L272)执行。    
Primary 节点在进行一些 chunk 信息检查（比如 chunk 是否还属于本 shard）和参数配置检查（是否关闭了autoSplit 等）之后，会使用 shardKey 的索引进行[逻辑分割](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/s/auto_split_vector.cpp#L143)（确定分割点）：    
1. 利用表的 size 和 count 信息，计算每条文档的平均大小 avgObjSize.
2. 然后计算每个 chunk 预估有多少条文档，计算公式为：`maxDocsPerChunk = maxChunkSizeBytes / avgDocSize`.
3. 使用 shardKey 索引，对 `[chunkMin, chunkMax)` 区间执行 indexScan, 并确定分割点数组。
4. 根据分割点数组进行微调，比如：
    - a. 如果分割点数组为 0，说明此时 chunk 大小还很小，不适合分裂。此时会在返回之前将 ChunkWritesTracker 中的累计值清零，防止频繁进入分裂判断的逻辑。
    - b. 避免创建过小的 chunk.
    - c. 如果要分裂的 chunk 位于表的第一个 chunk 或者最后一个 chunk，则将分割点向极值靠拢。这个特性主要时针对 range 分片时，按照 shardKey 的顺序进行写入的场景。

**更新路由信息**

确定好了分割点数组之后，shardSvr 会通过 `_configsvrCommitChunkSplit` 命令，将分割点数组和 chunk 信息发送给 configSvr, 由 configSvr 进行路由信息的更新。     
等 configSvr 更新完毕后，shardSvr 会更新自己的路由信息。    

`_configsvrCommitChunkSplit` 在 configSvr 上的[执行流程](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/s/config/sharding_catalog_manager_chunk_operations.cpp#L292)，是本次路由更新的核心：   
1. 获取 _kChunkOpLock 的独占锁，使本次路由操作和其他的 chunk split/merge/move 操作互斥。
2. 读取本地的 config.chunks 表，或者当前分片表的 collectionVersion 和 shardVersion. 由于 config.chunks 表有基于 ns 和 lastmod 的索引，因此读取成本很低。    
3. 基于当前的 collectionVersion，逐个将 minorVersion 递增，然后生成分裂后的 chunk 数组的 version 信息。
5. 将原始 chunk 信息，和待更新的新 chunk 信息，打包成一个原子请求并执行。

# 5. 惰性（lazy）路由同步
在理想情况下，一旦出现路由信息变化时，“马上”同步到所有节点。但是，在现实情况下，由于网络延迟、节点负载等原因，并不能做到“马上”同步。    
这就意味着，在某个时刻，不同节点看到的路由信息可能并不一致。这样可能会导致非常严重的问题，比如 mongos 将要写入的数据发送给了一个并不该拥有它的 shard, 或者将读请求发给了一个数据已经迁移出去的 shard, 导致读不到应有的数据。

为了解决上述问题，MongoDB 引入了 **基于版本校验的惰性同步** 机制。这种机制遵循如下原则：
1. **configSvr 节点始终存储最新的路由信息，具有最高权威**。mongos 和 shardSvr 都是从它同步最新的路由信息。
2. **数据迁移操作（moveChunk,movePrimary）的源端 shardSvr 在操作结束时确保具有最新的路由信息**，避免出现 mongos 和 shardSvr 的路由版本都是过时的旧版本，而数据的读写请求仍能从 mongos 正常发给旧 shardSvr 的情况。
3. mongos 和 shardSvr 之间传递的请求会携带 dbVersion 和 shardVersion 信息, 由 shardSvr 侧执行版本比较。
4. 版本低的角色会触发（增量）路由同步，从 configSvr 拉取最新的路由信息，并更新自身维护的 shardVersion, collectionVersion, dbVersion 等元数据。
5. 路由更新完毕后，mongos 会根据请求类型执行重试。

下面以读请求为例，分别讨论 shardVersion 和 dbVersion 对应的 mongos 和 shardSvr 的路由更新场景。

## 5.1 shardVersion 更新
### 5.1.1 mongos 侧的 shardVersion 更新
如果 mongos 侧缓存的路由版本比较低，对应的流程如下：
1. mongos 侧， 根据自己缓存的路由信息找到请求要转发到哪些 shardSvr, 然后在请求中[携带自己的 shardVersion 信息](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/s/query/cluster_find.cpp#L192)发给这些 shardSvr.    
2. shardSvr 侧，在请求执行阶段，需要获取表的元数据，此时会调用 [CollectionShardingState::checkShardVersionOrThrow](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/s/collection_sharding_state.cpp#L197), 发现请求中携带的 shardVersion 版本更低，于是在给 mongos 的返回包中携带 `staleConfig` 信息。
3. mongos 收到 StaleConfig 报错后， 调用 [CatalogCache::_getCollectionRoutingInfoAt](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/s/catalog_cache.cpp#L204) 刷新自己的路由：
   - 3.1. 根据自己的路由版本信息，进行增量拉取 （去 configSvr 上执行查询命令， 过滤和排序条件参考 [createConfigDiffQuery](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/s/config_server_catalog_cache_loader.cpp#L87)）。
   - 3.2. 将拉取过来的 chunkInfo 打平，并排序后，和现有的路由进行合并，并在合并中不断更新自己的 shardVersion 和 collectionVersion（这里参考 [refreshCollectionRoutingInfo](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/s/catalog_cache.cpp#L75)， 里面会有 flatten 打平动作， 以及 [ChunkMap::_makeUpdated](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/s/chunk_manager.cpp#L305) 里面要执行的 merge 动作。 更新 shardVersion 参考 [ChunkMap::_updateShardVersionFromUpdateChunk](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/s/chunk_manager.cpp#L291)，在更新 shardVersion 的同时，也会同步更新 collectionVersion（取所有 chunkVersions 的最大值））。
4. mongos 使用刷新后的路由，重试请求。

<p align="center">
<img width="741" height="633" alt="image" src="https://github.com/user-attachments/assets/064b3ef6-4ce0-46d3-80a4-6648f17be3a3" />
</p>

---

路由缓存更新算法有 2 个特点：**增量拉取**和**集中式拉取**。

**增量拉取**

其他节点从 configSvr 拉取路由，采用的是**增量拉取**的方式（除了第 1 次是全量）。增量拉取非常关键，因为对于超大表来说，chunks 数会非常多，导致路由表非常大，频繁的全量拉取是不可取的。    
> 以 chunkSize 为 64MB 为例，对于一个 64TB 的大表，路由记录会达到 100 万条或者更多。    
     
增量拉取的过滤条件为： {ns: \<要拉取的库表\>, lastmod: {\$gte: \<本地缓存的 collectionVersion\>}}.       
注意这里采用的是 `$gte`，而不是 `$gt`, 也是有特殊考虑的。    
正常来说，使用 `$gt` 就能获取所有的路由变更，但是无法处理返回结果为空时，具体对应下面哪种情形：    
- 表被删除，然后重建了同名表，导致返回结果为空。    
- 当前的路由信息已经是最新的，因此返回结果为空。    

**集中式拉取**

如果有很多请求同时触发了路由更新，并且都去 configSvr 进行增量拉取，那么会给 configSvr 造成很大的性能压力，而且 mongos 本地也会进行很多次路由更新，造成性能抖动。    
这是一个比较典型的缓存击穿问题。    

mongos 侧解决上述问题的做法是，只使用一个后台线程去完成增量拉取和路由合并更新的动作，其他线程[等待 Notifiction 被唤醒](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/s/catalog_cache.cpp#L204)：    
1. 第 1 个线程 A，抢到互斥锁，生成一个 Notification（本质是一个 std::condition_var）。给线程池提交增量拉取和路由合并更新的任务。然后释放锁后等待 Notification.    
2. 其他线程陆续能够拿到互斥锁，发现能获取到一个 Notification，因此不会去执行后续的拉取和更新逻辑，而是等待 Notification 被唤醒。    
3. 线程池的任务执行完毕后，唤醒所有等待 Notification 的线程（包括线程 A）, 然后将 Notification 置空。    
4. 所有线程被唤醒后，可以直接使用新路由执行操作。   

### 5.1.2 shardSvr 侧的 shardVersion 更新

如果 shardSvr 侧的 shardVersion 比较低，则会触发自己去拉路由。    

如何和 mongos 侧的逻辑相比，相同点是都从 configSvr 进行增量拉取，并且路由的合并更新逻辑都一致。    
ShardSvr 侧的不同点在于：    
1. 更新路由缓存之后的后续逻辑不同：shardSvr 侧需要写本地的系统表。
2. Primary 和 Secondary 节点的同步逻辑不同：primary 能主动去 configSvr 拉路由；但是 secondary 节点依赖 primary 进行路由同步，不能直接从 configSvr 拉取。

为了说明第 1 点，我们需要先明确，每个 shardSvr 中都有相应的系统表，缓存 configSvr 中的路由信息等元数据，列举如下：
|configSvr 中的权威系统表|shardSvr 中的缓存系统表|作用|
|:--|:--|:--|
|config.chunks|多个表，每个表对应一个分片表的路由信息，命名规则为:<br>config.cache.chunks.<分片表名>|每个分片表的路由信息|
|config.collections|config.cache.collections|每个分片表的 shardKey、分片算法、唯一属性 等信息|
|config.databases|config.cache.databases|每个 DB 的名字、primaryShard、版本等信息|

除了作为信息缓存之外，shardSvr 中的缓存表还有一些额外信息。比如 `refreshing` 标识位，标记当前这个分片表是否正在进行路由刷新；`lastRefreshedCollectionVersion` 标记当前这个分片表的路由信息最后一次刷新时间等。    

另外，shardSvr 侧的这些系统表还有在 primary 和 seconday 之间同步路由的作用。因为任何对这个系统表的更改，都会记录 oplog，然后通过 oplog 同步到其他节点。        

为了详细说明这一点，下面梳理一下 secondary 进行路由刷新的步骤：   
1. Secondary 节点: 发现自己的路由版本比较低，于是给 primary 发 [forceRoutingTableRefresh](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/s/shard_server_catalog_cache_loader.cpp#L287) 命令，触发 primary 进行增量拉取。    
2. Primary 节点：收到 forceRoutingTableRefresh 命令后，进行强制路由刷新：    
   - 2.1. 首先从 configSvr 增量拉取路由变更，并进行本地合并，相关流程可以参考 mongos 节点同步路由的流程。
   - 2.2. 然后执行 [persistCollectionAndChangedChunks](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/s/shard_server_catalog_cache_loader.cpp#L87) 流程，将路由变成持久化到本地系统表中：
     - 2.2.1. 调用 [updateShardCollectionsEntry](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/s/shard_metadata_util.cpp#L205) 更新 config.cache.collections 表，将 refreshing 标记位设置为 true.
     - 2.2.2. 调用 [updateShardChunks](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/s/shard_metadata_util.cpp#L331) 更新 config.cache.chunks.\<NS\> 表，持久化路由信息（删除过时的路由条目，插入新增的路由条目）。
     - 2.2.3. 调用 [unsetPersistedRefreshFlags](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/s/shard_metadata_util.cpp#L91) 更新 config.cache.collections 表，将 refreshing 标记位设置为 false, 并更新 lastRefreshedCollectionVersion 为最新的 collectionVersion。
   - 2.3. 给 secondary 返回结果，并在结果中附加上自己的 lastAppliedOpTime.
3. Secondary 节点：在 forceRoutingTableRefresh 命令返回之前，会通过 oplog 同步到 config.cache.collections 和 config.cache.chunks.\<NS\> 表的更改，并在[感知到 refreshing 标记变为 false](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/s/shard_server_op_observer.cpp#L281) 时，清理自身内存中的本地路由缓存。
4. Secondary 节点：收到 forceRoutingTableRefresh 命令的返回结果，并确保 oplog 同步到 lastAppliedOpTime 之后，然后[从本地系统表](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/s/shard_server_catalog_cache_loader.cpp#L1119)中加载最新的路由，优先使用 `{lastmod: {$gte:<currentCollectionVersion>}}` 方式进行增量加载。       

## 5.2 dbVersion 更新
dbVesion 的更新流程，和 shardVersion 的更新流程类似，只是在维护的元数据格式、操作的系统表、刷新命令方面有一些区别。    
因此，下面不再赘述详细的更新流程，而是对比在系统表和刷新命令方面和 shardVersion 的区别：
|shardVersion|dbVersion|dbVersion 中的具体作用|
|:--|:--|:--|
|config.cache.collections|config.cache.databases|每个shardSvr 上缓存的 DB 信息，对应在 configSvr 上的 config.databases 表|
|config.cache.chunks.\<NS\>|无|-|
|forceRoutingTableRefresh|_flushDatabaseCacheUpdates|触发 primary 节点进行路由刷新|

# 5. 总结


# 6. 参考文献
1. TDSQL的表类型：https://doc.fincloud.tencent.cn/tcloud/Database/TDSQL/705974/72883/59424
2. MongoDB 中路由缓存的优化：https://jira.mongodb.org/browse/SERVER-71627
3. https://github.com/mongodb/mongo/tree/r4.2.25
4. https://github.com/mongodb/mongo/blob/r4.0.3
5. https://www.mongodb.com/docs/manual/sharding/#sharding-strategy
6. https://www.mongodb.com/zh-cn/docs/v7.0/reference/method/sh.shardCollection/
7. https://github.com/mongodb/mongo/blob/r8.0.9/src/mongo/db/s/global_index/README.md


