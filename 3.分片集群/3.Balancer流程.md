# 导语

MongoDB 分片架构的魅力就在于，能够灵活的添加和删除分片（横向扩缩容），并允许数据数据在多个分片之间智能流动，从而实现负载均衡。    
而且上述特性，并不需要过多的人工干预，只需要用户使用少量的命令触发即可：    
- addShard 添加分片，即横向扩容；
- removeShard 删除分片，即横向缩容；
- balancer开关及对应的参数配置，自动负载均衡；
- movePrimary 迁移 DB 的主分片（非分片表）；

触发上述动作后，内核会自动完成数据的迁移工作。主要对应了 3 个核心模块：
- Balancer 控制流程：什么时候发起均衡，如何智能的选择迁移的 chunk 和 shard；
- moveChunk 迁移流程：如何将一个 chunk 从源端迁移到目标端；
- movePrimary 迁移流程：如何将一个 DB 的主分片从一个 shard 迁移到另一个 shard；

下面分 3 个小节分别进行阐述。   

# 1.Balancer 控制流程
Balancer 流程由 ConfigServer 主节点发起。ConfigServer 随机选择分片表，并根据路由表中 chunks 分布情况，选择要迁移的 chunk、源端和目标端 shard 进行迁移。    

<p align="center">
<img width="500" height="180" src="https://github.com/user-attachments/assets/80b5787b-a554-4d61-a42d-552bf847f886" />
</p>

**接下来的介绍基于 4.2 版本的内核代码。**    
在 6.0 内核版本之后，分片集群的 split 和 balancer 机制发生了很大的变化，主要有：    
- Balancer是基于存储的数据量，不再是基于 chunks 数量；
- Split 操作在需要迁移时才做，而不是基于 chunk 最大大小进行自动分裂；

<p align="center">
<img width="1405" height="363" src="https://github.com/user-attachments/assets/97bcaa0d-a40f-4bc6-a1a6-f043f31ceba4" />
</p>

## 1.1 主流程
ConfigServer 节点在被选举为主节点时，会启动 balancer 核心流程。    
参考 [Balancer::_mainThread](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/s/balancer/balancer.cpp#L298)，核心流程为一个无限循环（shutdown 会退出），每个循环的流程为：   

1. 从 config.settings 表中刷新 balancer 的配置（包括是否开启开关，配置的时间窗口等）；

2. 如果关闭了balancer，或者不在窗口期内，则 sleep 10 秒后进入下一个循环；

3. 对所有的表的路由信息进行校验，进行必要的 split，如下条件满足时会给对应的 shard 节点发送 splitChunk 命令：    
    - a. 如果是 config.system.sessions 表，而且当前的 chunks 个数少于 1024 个，则分裂至 1024 个。（为啥会有这个看似奇怪的逻辑，个人认为和 sessions 表的 _id 范围分区策略有关，使用范围分区策略时没法指定初始chunks 大小，所有在内核代码中通过这套逻辑硬编码了类似 hash 分区的初始 chunks 大小策略。而为什么使用 _id 的 range 分区而不是 hash 分区，个人认为是 _id 采用的 UUID 完全随机，不存在数据倾斜，这种情况下使用 range 分区的性能更好）;        
    - b. 如果是普通表，则检查是否有 chunk 跨越了多个 zone， 如果有的话则进行 split. （比如有一个 chunk [10,20), 其中 [10,15）属于 zoneA, [15,20) 属于 zoneB, 而 zoneA 和 zoneB 刚好又存在同一个 shard 上，此时需要进行主动split）;     

4. 选择适合迁移的 chunks：    
    - a. 首先获取所有的分片表，并随机打散（std::shuffle），初始化availableShards， 包含了所有可用的 shardId ;    
    - b. 对每个分片表，循环执行如下判断：    
        - i. 如果这个表显式关闭了 balancer，则跳过；    
        - ii. 对每个 zone，循环执行：     
            - 1. 如果有 shard [处于 draining](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/s/balancer/balancer_policy.cpp#L639) 状态，则优先迁移这个 shard 的 chunks. 如果用户使用 `removeShard` 命令尝试删除一个 shard，则会在 `config.shards` 系统表中，会将这个 shard 设置为 draining. 
            - 2. 计算 zone 内的 shard 总量和 chunk 总量，得到每个 shard 的[理想 chunk 个数](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/s/balancer/balancer_policy.cpp#L639)；    
            - 3. 获取 availableShards 内的负载最大和最小的 shards，负载最大的是 fromShard, 最小的是 toShard。如果 fromShard 的 chunk个数大于理想chunk个数，而且 toShard 的 chunk个数小于理想chunk个数，则选择 chunk 进行迁移，[选择算法](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/s/balancer/balancer_policy.cpp#L727)是：遍历 fromShard 中的 chunk 列表，选择一个非 jumboChunk 的正常 chunk 纳入候选的迁移列表；    
            - 4. 将 fromShardId 和 toShardId 移出 availableShards 列表。 这样下一轮不会再选到这 2 个 shard，可以看作是一种并发控制：**每一轮 balancer，每个 shard 只参与一个 chunk 的迁移。如果一个集群有 N 个 shard，那么同一时刻迁移的 chunk 任务最多为 N/2.**    

5. 根据上一步选出的 chunk，将 moveChunk 命令发送给对应的 fromShard，执行 chunk 迁移；    
   
6. 统计本轮 balancer 的次数，并记录到 config.actionlog 中；   
   
7. 如果本轮有 chunks 迁移，则 sleep 1秒后进入下一个循环；否则，sleep 10秒进入下一个循环；    

## 1.2 控制接口
Balancer 模块对外暴露的接口主要有：    
- Balancer 总开关。通过 sh.stopBalancer() 和 sh.startBalancer() 命令可以启停总开关。默认开启；    
- 表级别开关。可以通过 sh.disableBalancing("库名.表名") 和 sh.enableBalancing("库名.表名") 命令对指定表进行启停。默认开启；    
- 时间窗口配置。指定“小时：分钟 - 小时：分钟”格式的时间窗口进行迁移。默认全天开启；    
- 迁移的 write concern: secondaryThrottle. 默认是[写主即成功](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/s/chunk_move_write_concern_options.cpp#L50-L82)；    
- 删除策略：waitForDelete. [默认 false](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/s/balancer_configuration.h#L120)；    

调整时间窗口的示例：    
```
db.settings.updateOne(
   { _id: "balancer" },
   { $set: { activeWindow : { start : "<start-time>", stop : "<stop-time>" } } },
   { upsert: true }
)
```
时间窗口也可以跨天，比如从晚上 21:00 到第二天凌晨 7:00，如下：    
```
use config
db.settings.updateOne(
   { _id: "balancer" },
   { $set: { activeWindow : { start : "21:00", stop : "7:00" } } },
   { upsert: true }
)
```

具体判断算法可以参考 BalancerSettingsType::isTimeInBalancingWindow:         
```
bool BalancerSettingsType::isTimeInBalancingWindow(const boost::posix_time::ptime& now) const {
    invariant(!_activeWindowStart == !_activeWindowStop);

    if (!_activeWindowStart) {
        return true;
    }

    LOG(1).stream() << "inBalancingWindow: "
                    << " now: " << now << " startTime: " << *_activeWindowStart
                    << " stopTime: " << *_activeWindowStop;

    if (*_activeWindowStop > *_activeWindowStart) {   // 没跨天的情形
        if ((now >= *_activeWindowStart) && (now <= *_activeWindowStop)) {
            return true;
        }
    } else if (*_activeWindowStart > *_activeWindowStop) {   // 跨天的情形
        if ((now >= *_activeWindowStart) || (now <= *_activeWindowStop)) {
            return true;
        }
    } else {
        MONGO_UNREACHABLE;
    }

    return false;
}
```

调整 secondaryThrottle 的示例：    
```
use config
db.settings.updateOne(
   { "_id" : "balancer" },
   { $set : { "_secondaryThrottle" : { "w": "majority" }  } },
   { upsert : true }
)
```

调整 waitForDelete 的示例：    
```
use config
db.settings.updateOne(
   { "_id" : "balancer" },
   { $set : { "_waitForDelete" : true } },
   { upsert : true }
)
```

# 2. MoveChunk 数据迁移流程

ConfigServer 的 balancer 流程确定了要迁移的 chunk 以及对应的 source(from/donor) shard 和 destination(to/recipient) shard 之后，会执行如下流程完成迁移：    
1. ConfigServer 生成 moveChunk 命令，并发送给 source shard 的主节点（在真正完成迁移之前，这个 chunk 的请求还是会发送到 source 端。Source 端也会管理接下来的整体迁移流程）；
2. Source主节点发送 _recvChunkStart 命令给 destination，通知它拉数据（参考 [StartChunkCloneRequest](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/s/start_chunk_clone_request.cpp#L58)）；
3. Destination 端创建相关的表和索引（如果之前没有创建过的话）；
4. Destination 端不断拉取 chunk 的数据，并进行同步；
5. 完成了全量同步之后，Destination 端执行增量同步逻辑，确保数据完全一致；
6. 两边的数据完全一致之后，Source 端负责向 ConfigServer 提交更新后的路由信息。此后关于这个 chunk 的读写请求会发送到 destination shard；
7. Source 端对 chunk的数据进行同步或者异步删除；

上述流程听起来平平无奇，但是实际的工程实现中，可能要解决很多难题，比如：
1. 在迁移期间，源端数据的修改存储在哪里？
2. 目标端如何拉取全量数据和增量数据？
3. 拉取增量数据期间是否需要停止写入，需要停止多久，是否在迁移完成之后自动重试？
4. 路由的修改如何保证原子性？
5. 孤儿文档如何清理？

## 2.1 主体流程
configSvr 在确定了要迁移的 chunk ，以及源和目标 shard 之后，会生成 moveChunk 命令发送到源shard上。此后的操作由源 shard 主控，目标 shard 配合。在完成迁移和路由更新之后，源 shard 最后会给 configSvr 返回结果。在迁移期间，configSvr 并不参与实际的控制逻辑（接受源 shard 发送的路由更新提交请求除外）。    

在源 shard 的主导下，会执行如下主体流程 （为了方便描述，下面将 **源 shard 简化为 A，目标 shard 简化为 B**）：    

1. A ：初始化 [MigrationSourceManager](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/s/migration_source_manager.cpp#L133), 进行整体逻辑控制；调用 [startClone](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/s/migration_source_manager.cpp#L220) => [_storeCurrentLocs](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/s/migration_chunk_cloner_source_legacy.cpp#L826) 进行 shardKey索引扫描，将这个 chunk 中文档的 recordId 信息存储到 [std::set\<RecordId\> 结构](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/s/migration_chunk_cloner_source_legacy.h#L334) 中。相当于先统计一下全量数据的 recordId.
2. A-> B：发送 [_recvChunkStart 命令](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/s/migration_chunk_cloner_source_legacy.cpp#L319)。 B 接受到命令后会启动 [MigrationDestinationManager](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/s/migration_destination_manager_legacy_commands.cpp#L113) 进行目标端的控制逻辑，首先确保要接受的 chunk 没有在本地存在孤儿文档(尝试发起清理任务，并等待任务结束)，然后启动一个后台线程进行主动数据拉取（参考 [_migrateThread](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/s/migration_destination_manager.cpp#L724) 和 [_migrateDriver](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/s/migration_destination_manager.cpp#L748)）。A 节点在发送命令之后，会不断再发送 [_recvChunkStatus](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/s/migration_chunk_cloner_source_legacy.cpp#L345) 询问 B 节点的同步状态（每次命令之间 sleep 的间隔从 1ms，最多到 1024ms， [指数级](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/s/migration_chunk_cloner_source_legacy.cpp#L355)增长），确认是否可以进入 catchUp 阶段（这个阶段代表着 B 节点完成了全量和已知增量的同步），等待进入这个状态的超时时间是 [6 小时](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/s/migration_source_manager.cpp#L79)。
3. B -> A: 由后台线程发送 [_migrateClone](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/s/migration_destination_manager.cpp#L885)命令。A 收到命令后，将全量数据作为命令的响应返回给 B，B收到后完成本地插入。如果 chunk的数据量比较大（超过 16MB），这个过程可能会持续多轮。同时为了保证同步的高效率，采用了一个线程拉取，另外一个线程插入的形式，线程之间通过内存队列进行同步（具体参考 [cloneDocumentsFromDonor](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/s/migration_destination_manager.cpp#L383)）。关于全量同步部分，其余需要明确的有：    
   - a. A 节点扫描得到全量数据？第一想法是按照 chunkRange 的 minkey 和 maxkey，使用 shardKey 的索引，进行一次 indexScan。实际的做法是，在迁移任务启动时，使用 indexScan 扫描得到这些数据的 recordId，并缓存在一个 std::set\<recordId\> 结构中。在收到 _migrateClone 命令之后，使用 recordId 进行 fetch 操作得到对应的 BSON 文档。
   - b. B 节点使用什么级别保证持久性？第一想法是 writeConcernMajority 进行大多数提交。实际的做法是，依赖 secondaryThrottle 参数，这个参数默认的配置是写主即成功。从原理上来讲，只需要保证在提交路由时，之前写入的数据能够完成大多数提交即可，没有必要每条数据都是写大多数。
   - c. 为什么是 B 发送命令过来拉取，而不是在源端 A 进行推送呢？我认为和迁移架构的设计有关系，整个架构的设计就是 源端和目标端都要参与，而不是源端控制一切。至于为什么不是源端控制一切，目前没有找到官方的注释说明。但是个人认为，源端控制一切的架构可能存在一些问题，比如目标端切主了，这个时候源端是否要感知，怎么判断之前写入的数据有没有被回滚，是否要取消迁移流程，还是要继续执行呢？处理起来可能会比较麻烦。
   - d. 迁移一旦开始之后，2 个 shard 都有数据的副本，此时如果有一个全表扫描的请求，是否会读到 2 份数据？实际上不会，因为请求会有一层 shardFilterStage 进行过滤，参考 [ShardFiltererImpl::documentBelongsToMe](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/exec/shard_filterer_impl.cpp#L45). 另外，2 个 shard 都会有 oplog，但是目标 shard 的 oplog 会带上 `fromMigrate:true` 的标记，避免对 changeStream 等依赖 oplog 的功能造成干扰。

4. B -> A: 由后台线程发送 [_tansferMods](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/s/migration_destination_manager.cpp#L920) 命令。A 收到命令后，会将增量数据作为命令的响应返回给 B，B 收到后会进行必要的检查（即增量数据是否属于这个 chunk），然后进行回放。对于delete变更，会对应地执行delete动作；对于 insert/update 变更，则是执行 upsert 动作。关于增量同步部分，需要明确以下几点：    
   - a. A 节点如何记录增量数据？第一想法可能是追踪 oplog，过滤和chunk 有关的修改。但是这种方法显然是低效的，首先， oplog 会很多，绝大部分都是和这个 chunk 无关的；其次，oplog 中没有文档的全量信息，甚至很大可能没有 shardKey (因为 shardKey 不能直接被修改)，所以提取出相关的文档非常麻烦。第二想法可能是专门搞一个数据段，或者文件之类的物理空间来存储修改后的文档（Copy-On-Write），但是问题在于无法预知需要多大的存储空间，而且 A 节点的查询处理会比较麻烦，而且还要设计到迁移成功后的清理，以及迁移失败时的数据合并等。
   - b. 实际上，A 节点会在内存中分别使用 [std::list\<BSONObj\>](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/s/migration_chunk_cloner_source_legacy.h#L361) 存储被 delete 的文档的 _id，使用 [std::list\<BSONObj\>](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/s/migration_chunk_cloner_source_legacy.h#L354) 共同存储被 update 和 insert 的 _id（update/insert 可以同等对待，因为给 B 节点返回数据时，都是使用 _id 查询当时的全量文档再返回）。为了保证每次的修改都被记录下来，[OpObserverShardingImpl:shardObserveTransactionPrepareOrUnpreparedCommit](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/s/op_observer_sharding_impl.cpp#L168) 会注册一个 [LogTransactionOperationsForShardingHandler](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/s/migration_chunk_cloner_source_legacy.h#L61) 事件，每次 [WiredTigerRecoveryUnit::_commit](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/storage/wiredtiger/wiredtiger_recovery_unit.cpp#L197) 进行事务提交时会调用该事件， 里面会通过 [_addToTransferModsQueue](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/s/migration_chunk_cloner_source_legacy.cpp#L609) 函数记录 delete/update/insert 的 _id 信息。
   - c. 为什么 delete/insert/update 的数据要记录 BSONObj 形式的 _id，而不是 recordId? 理论上来讲，直接通过 recordId 查询更高效，但是从 [LogTransactionOperationsForShardingHandler::commit](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/s/migration_chunk_cloner_source_legacy.cpp#L171) 的实现来看，从 [ReplOperation](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/repl/oplog_entry.h#L55) 参数中能够直接获取的是 _id. 个人认为这是一方面原因。另外，对于 delete 来说，还是需要将 _id 传送到 B 节点进行删除，而不是直接传递 recordId。因为副本集中不同节点可以通过 _id 找到相同的文档，但是通过 recordId 则不行（由于并发回放等机制，同一条文档在不同节点之间的 recordId 很有可能是不一样的）。
   - d. 为什么 delete 和 insert/update 要分成 2 类分别记录，有什么必要性？是否会引起时序的问题？目前能想到的必要性就是在功能上进行了区分：delete 操作可以直接将std::list 中的 _id 返回给 B 节点，而 insert/update 需要根据 _id 读取全量文档后再返回。至于可能引起的时序问题，通过一个巧妙规则来解决：[先执行 delete 操作](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/s/migration_chunk_cloner_source_legacy.cpp#L732)，然后再执行 insert/update 操作。举例如下：
     - i. 如果A节点的时序是 删除{_id:1} => 插入 {_id:1,data:"xxx"}, 则 B 节点执行 先删后插显然也没有问题；
     - ii. 如果A节点的时序是 插入{_id:1,data:"xxx"} => 删除 {_id:1}，则 A 会给 B 传输 {_id:1} 的删除操作，而再给 B 返回 插入动作时，由于再 A 上回表已经查不到 {_id:1} 的数据，因此会跳过。最终 B 只会收到 {_id:1} 的删除动作，而由于 B 上没有 {_id:1} 的数据，因此这个删除动作也会被忽略。因此，B 节点的数据还是和 A 一致，没有问题。
     - iii. 相关的代码注释摘抄如下：
        ```
        // The "snapshot" for delete and update list must be taken under a single lock. This is to
        // ensure that we will preserve the causal order of writes. Always consume the delete
        // buffer first, before the update buffer. If the delete is causally before the update to
        // the same doc, then there's no problem since we consume the delete buffer first. If the
        // delete is causally after, we will not be able to see the document when we attempt to
        // fetch it, so it's also ok.
        ```
   - e. B 什么时候结束？如果 _transferMods 没有再返回数据，则增量阶段结束，B 将自己设置为 "steady" 状态。此时，A 节点可能突然又接收了一些写入，这也没关系，后面提到的 catchup 步骤就是专门处理这些写请求的。
5. A -> B：前面第2步提到了， A 节点一直在给 B 节点发送 _recvChunkStatus 询问状态。到目前为止，B 节点终于变为 "steady" 了，那么 A 节点也知道自己应该进入 catchup 状态了。进入 catchup 阶段的典型特征就是：设置这个表只读 (5.0内核版本还引入了 [maxCatchUpPercentageBeforeBlockingWrites参数](https://www.mongodb.com/zh-cn/docs/manual/release-notes/5.0/#maxcatchuppercentagebeforeblockingwrites-server-parameter)进一步控制停写的时机, 用于平衡停写时间和整体的迁移时间)。下面描述具体细节：    
   - a. 生成一个 Notification 类型的 _critSecSignal，并标记类型为只读，参考 [ShardingMigrationCriticalSection::enterCriticalSectionCatchUpPhase](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/s/sharding_migration_critical_section.cpp#L42)。需要特别特别注意的是，这个操作需要在表级别的 X 锁保护下进行。这个操作很快结束，但是需要和正在执行读写操作的请求隔离开。
   - b. 然后 A 节点的迁移流程，标记自己进入了 criticalSection 状态。
   - c. 后续读写操作在到达 A 节点之后，会在检查路由阶段获取上述 _critSecSignal 并进行检查。如果是读请求则通过，如果是写请求，则向上层抛出 StaleConfigInfo 异常。（参考 [CollectionShardingState::_getMetadataWithVersionCheckAt](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/s/collection_sharding_state.cpp#L201)）
   - d. A 节点的请求处理的入口会捕获 StaleConfigInfo 异常，并对 _critSecSignal 进行等待（等待通知，或者5分钟后超时），等待结束后会给 mongos 返回可重试的错误码，mongos 会刷新路由之后进行后续重试。（参考 [execCommandDatabase](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/service_entry_point_common.cpp#L645) => ... => [OnShardVersionMismatchNoExcept](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/s/shard_filtering_metadata_refresh.cpp#L128) => ... => [waitForMigrationCriticalSectionSignal](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/s/operation_sharding_state.cpp#L145)）
   - e. 在 A 节点进入 catchup 阶段变成只读之后，B 节点继续处于 steady 阶段，并持续使用 _transferMods 拉取可能存在的增量数据。
6. A->B：A 节点给 B 发送 _recvChunkCommit 命令，期望 B 同步完所有的数据并提交。B 节点收到命令后会执行如下动作：
   - a. 将迁移状态设置为 COMMIT_START.
   - b. 后台线程确认 _transferMods 再也拉取不到增量，而且状态为 COMMIT_START，则确保所有数据都已完成写入，并满足 writeConcern 的要求。这里需要特别特别说明的是，这里的 writeConcern 是既要满足 majority，也要满足 secondaryThrottle。所以只要最终迁移成功了，是不可能有数据在迁移的过程中丢失的！ （参考 [opReplicatedEnough](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/s/migration_destination_manager.cpp#L151)）
   - c. 最后再将状态设置为 DONE，并给 A 节点返回命令结果。
7. A -> ConfigSvr: A 节点此时可以进行路由变更了，会执行如下操作（参考 [MigrationSourceManager::commitChunkMetadataOnConfig](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/s/migration_source_manager.cpp#L386)）：
   - a. 进入 commitPhase 阶段，这个阶段会将第 5 步提到的 _critSecSignal 设置为停读停写，hold 住所有请求。这里提到的停止读写是整个表级别的，而不是精确到 chunk。个人认为原因是，用户读写请求可能并不携带 shardId，所以无法预知是否会命中这个 chunk 进行提前拦截。而如果对请求结果进行过滤，则会非常复杂，特别是 delete/update 这种请求，对代码复杂度和性能的影响都要进行评估。
   - b. A 节点给 configSvr 发送 _configsvrCommitChunkMigration 命令进行路由变更，命令中携带了 chunkRange, fromShard, toShard 等关键信息。configSvr 会将路由变更的动作封装成一个 applyOps 命令再执行，保证原子性。（参考 [commitChunkMigration](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/s/config/sharding_catalog_manager_chunk_operations.cpp#L880) => [makeCommitChunkTransactionCommand](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/s/config/sharding_catalog_manager_chunk_operations.cpp#L180)）
   - c. 刷新自己的路由信息，到 config.changelog 中记录一条迁移成功的日志，给 B 节点发一个路由刷新命令。
   - d. 发起一个删除孤儿文档的任务，并根据 waitForDelete 配置决定是否等删除任务执行完成再返回，默认是 false，这种情况会延迟 [15分钟](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/s/sharding_runtime_d_params.idl#L98) 再删除。

<p align="center">
<img width="1051" height="1174" alt="image" src="https://github.com/user-attachments/assets/9cfbd7d3-b7cf-4332-88df-aa1d6b4e07a0" />
</p>

## 2.2 孤儿文档清理

### 删除动作

在 moveChunk 成功后，就会生成一个删除任务提交给线程池去执行。根据命令中的 waitForDelete 参数，决定是否等待任务执行完毕后再返回。    
**注意，同步方式不等于立即执行。**  


除了 moveChunk 发起的自动删除外，MongoDB 还提供了手动删除的命令 cleanupOrphaned，用于应对迁移过程中出现故障导致的孤儿文档残留的情况。    
         
用户在使用 cleanupOrphaned 命令时可以指定一个 startingFromKey 作为起始扫描位置（如果不指定，则为 minKey ），服务端会根据起始位置进行扫描，寻找一个不在路由表区间，并且不在 moveChunk 接收区间的 range，对这个range 进行清理。示意图如下：    

<p align="center">
<img width="676" height="431" alt="image" src="https://github.com/user-attachments/assets/dd5c8b6d-0acf-45d9-9a41-a707a202e7fe" />
</p>

清理完成之后，会将 range 的 max 作为 stoppedAtKey 参数返回，用户可以将这个参数作为下一轮的 startingFromKey 参数再次调用 cleanupOrphaned 进行后续清理。如果已经没有了可以继续清理的区间，则不会返回 stoppedAtKey，此时整个清理流程全部结束。    


在 [4.4 内核](https://www.mongodb.com/zh-cn/docs/v4.4/release-notes/4.4/#chunk-migration-failover-resiliency-improvements) 版本进行了一些优化，保证 failover 之后， 孤儿文档能够主动被清理：  

<p align="center">
<img width="1242" height="576" alt="image" src="https://github.com/user-attachments/assets/b4fbc881-5c0f-414c-a042-45a1775c8b0d" />
</p>

### 安全性保证
在 chunkRange 被迁移走之后，并不能确定是否能够立即删除。比如：    
1. 可能会被较老的快照读引用。    
2. 可能这个 chunkRange 又在被迁移回来。比如 chunk1 从 shard A 迁移到了B，但是没有马上删除。然后在很短的时间内，又从 B 迁移回 A。而此时 A 上的孤儿文档删除任务刚好调度到了 chunk1 的删除。那么此时能够一边从 B->A 迁移 chunk1，一边又在 A 上清理 chunk1 吗？    

对于来回迁移的问题，MongoDB 做了至少 2 手准备：    
1. 在 B->A 迁移 chunk1 时，要先保证 A 上的 chunk1 已经清理干净。如果 A 上的chunk1 此时还不能清理（比如被快照引用了），则迁移流程需要等待这个清理动作完成，再继续执行。    
2. B -> A 迁移任务发起之后，会在 A 上记录 receiveingChunks。 A 上的孤儿文档删除任务，不能删除和 receiveingChunks 有区间重叠的数据。      

对于快照引用的问题。在删除 chunkRange 之前，会 [遍历](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/s/metadata_manager.cpp#L509) 当前节点上正在使用的路由版本，如果发现有某个路由版本中包含了这个 chunkRange （存在区间重叠），则会将这个 range 放到那个路由版本的 [orphans 队列中](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/s/metadata_manager.cpp#L535)。而如果这个有重叠的路由版本，恰好是当前的最新路由版本，则直接[报错](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/s/metadata_manager.cpp#L511)（正常来说，不会走到这个分支）。    

如果某个路由版本没有再被引用，则会尝试清理自己 orphans 队列中的孤儿文档（参考 [MetadataManager::_retireExpiredMetadata](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/s/metadata_manager.cpp#L382)）。    


路由和孤儿文档之间的关系，可以参考 [源代码](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/s/metadata_manager.cpp#L78) 中的架构图：    

<p align="center">
<img width="1415" height="994" alt="image" src="https://github.com/user-attachments/assets/2c2c3a57-38c6-4b6d-9c67-bff8590d9fd6" />
</p>

---  
**moveChunk 操作和其他 DDL 操作的互斥**    

如果在 moveChunk 过程中，表被删除了，如何处理，是否会有孤儿文档残留？    
MongoDB 通过分布式锁来解决这个问题，具体来说：
1. 在发起 moveChunk 任务时，会先[获取对应表的分布式锁](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/s/balancer/migration_manager.cpp#L490)；    
2. 在执行 dropCollection 操作时，mongos 会将命令转发给 configSvr，而 configSvr 会先尝试[获取对应表的分布式锁](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/s/config/configsvr_drop_collection_command.cpp#L123)， 然后再到各个 shardSvr 上执行删表操作；

分布式锁限制了 DDL 类操作的并发，但是保证了安全性。另外，在分布式锁的保护下，整个 moveChunk 期间，对应表的路由不会被其他 moveChunk 流程更改。    

# 3. MovePrimary 流程
在 MongoDB 的分片集群中，用户也能创建非分片表。相比于分片表而言，非分片表没有关于 shardKey 的一系列限制（必须包含 shardKey（低版本内核必须要有，4.4 版本开始默认是 null）, shardKey 不能更新等），适用于数据量少，又必须充分保证灵活性的场景。每个 DB 的所有非分片表，都只会固定地存储在这个 DB 的主分片上，mongos在处理非分片表的请求时，会直接将请求转发到对应的主分片上。     

非分片表没有 chunk 的概念，因此不能用 balancer+moveChunk 这样的流程进行迁移。因此 MongoDB 提供了 MovePrimary 功能，用于给 DB 切换主分片。      

movePrimary 并不保证线程安全性，即迁移期间最好不要由读写请求，否则不做任何服务保证。参考[官方文档](https://www.mongodb.com/zh-cn/docs/manual/reference/command/moveprimary/)：

>**警告**    
>启动movePrimary后，在该命令完成之前，请勿对该数据库中的任何**未**分片集合执行任何读取或写入操作。 迁移期间对这些集合发出的读取或写入操作可能会导致意外行为，包括迁移操作可能失败或数据丢失。

从 [5.0内核](https://www.mongodb.com/zh-cn/docs/manual/release-notes/5.0/#moveprimary-error-message-for-writes-during-operation) 开始，使用 movePrimary 命令从分片集群中迁移分片时，写入原始分片会生成错误消息。

个人理解其中的原因：对整库的索引非分片表进行迁移，数据量可能会很大，几百 GB 甚至 TB 级别都是有可能的。在这种场景下，如果还像 moveChunk 那样采用全量+持续增量的形式来“在线迁移”，没法保证迁移速度，而且资源的消耗会非常大，极可能出现 OOM 等异常情况。    
因此，MongoDB 对 movePrimary 的整库迁移，设计了一套类似“离线迁移”的流程，如下所示：   

<p align="center">
<img width="1191" height="763" alt="image" src="https://github.com/user-attachments/assets/4fbc0b73-c5c2-4128-9ac8-ed36bf3cff4b" />
</p>

迁移流程涉及 mongos, configSvr，源和目标 shard。核心流程是目标shard-B 逻辑形式拉取数据并插入到本地，然后源 shard-A 到 configSvr 上提交最新的元数据，最后清理自己的非分片表。

# 4.总结

Balancer 给分片集群带来了很大的灵活性，也是 MongoDB 能够进行横向扩缩容的基础。    
本文结合 4.2 内核版本代码，详细阐述了 balancer 模块的核心流程，即chunk 迁移调度算法和 chunk 迁移的执行细节。并重点突出了算法实现中涉及的性能、安全性等问题的解决方案。

# 参考文档
1. https://www.mongodb.com/zh-cn/docs/manual/tutorial/manage-sharded-cluster-balancer/#schedule-the-balancing-window    
2. https://www.mongodb.com/docs/manual/core/sharding-balancer-administration/    
3. https://github.com/mongodb/mongo/tree/r4.2.25/src/mongo/db/s/balancer    
4. https://www.mongodb.com/zh-cn/docs/v4.4/release-notes/4.4/#chunk-migration-failover-resiliency-improvements
5. https://www.mongodb.com/zh-cn/docs/manual/release-notes/5.0/#moveprimary-error-message-for-writes-during-operation
6. https://www.mongodb.com/zh-cn/docs/manual/reference/command/moveprimary/
7. https://www.mongodb.com/zh-cn/docs/manual/release-notes/5.0/#maxcatchuppercentagebeforeblockingwrites-server-parameter




