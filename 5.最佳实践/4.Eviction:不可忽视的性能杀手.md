# 1. 导语
MongoDB 和其他数据库一样，非常依赖内存来弥补硬盘 IO 能力的不足，从而提升整体性能。    
不同数据库对内存的使用方式不尽相同。对于 MongoDB 的 WiredTiger 存储引擎（以下简称 WT）来说，会在内存中为每个表维护一个 btree 结构，读写操作都基于 btree。如果访问到的 page 在内存中没有，则从硬盘读出加载。如果对 btree 中的 page 进行了修改，也需要相应的机制刷回到硬盘。    
本文将对 WT 如何使用内存进行分析，并聚焦 eviction 的相关问题：    
1. WiredTiger 如何使用内存。
2. Eviction 的运行机制，包括何时触发，需要多少线程来完成。
3. WT 的 LRU 实现。
4. 如何避免陷入 eviction 带来的性能陷阱。

# 2. 基本概念
WT 引擎对内存的使用如下图所示：   

<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/85d031f2-d9db-431b-9408-ff3f71418aad" width=300>
</p>
    
首先，我们可以将内存使用的大头，大致划分为 2 部分：     
- WT 自己维护的 cache。这部分内存由 WT 从操作系统申请（一般是 tcmalloc）并自己管理，主要用于维护 btree。[CacheSizeGB](https://www.mongodb.com/docs/v4.2/reference/configuration-options/#storage.wiredTiger.engineConfig.cacheSizeGB) 配置项用于限制这部分内存的大小，通过结合后文要介绍的 eviction 流程，能够将这部分内存的大小限制在指定范围。但是需要注意的是，CacheSizeGB 并不是对 WT 所有内存使用的限制，比如 btree 的 data-handle，block 和 page 转换逻辑等消耗的内存并不受这个参数的限制。参考[官方文档]((http://source.wiredtiger.com/11.1.0/arch-cache.html#arch_cache_size))的描述：
    >The WiredTiger cache is only used for Btree data, including associated in-memory structures such as indexes, insert lists, and update chains. Other WiredTiger data structures, such as dhandles, cursors, and sessions, are not considered part of the cache and do not count against the cache size. Similarly, memory used to read in and write out the on-disk representations of Btree pages is not cached; it is only allocated temporarily during the I/O operation and while the data is converted to or from the on-disk format.

- 操作系统层的 cache。WT 写文件时调用 posix 接口将数据写到操作系统内核的缓冲区。Linux 内核有相应的刷盘策略，并提供 fsync 接口供 WT 将数据从缓冲区中强制持久化到硬盘。

# 3. WiredTiger 的 LRU 实现思路
## 3.1 常见的 LRU

一般来说，磁盘上存储的数据量是远大于内存容量的。因此，WT 引擎中需要制定相关的策略，将不常用的 page 淘汰出去，保证使用的内存总量可控，而且都是访问频次比较高的 page。    

关于如何进行数据淘汰、冷热置换，大家熟知的策略有 LRU、LFU 等。常见的数据库存储引擎，比如 [LevelDB](https://zhuanlan.zhihu.com/p/649896220)、RocksDB、InnoDB 等，基本上都是 LRU 的变种。

先不参考这些开源引擎的 LRU 实现。如果让我们自己从零实现一个 LRU 算法，该怎么去做呢？      
这是一道非常八股的面试题了，基本上都会想到用哈希表和双向链表就能实现出来。为了保证多线程的安全性，还会加一个 mutex。该算法的 C++ 实现可以参考 MongoDB 内核代码 [util/lru_cache.h](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/util/lru_cache.h)。    

如果将上述原始 LRU 算法放到多核 CPU 和超大内存的运行环境，瓶颈凸显：    
1. 多核的并发性。由于抢锁才能操作 LRU List，在高并发的场景下性能受限。
2. 大对象的性能开销：需要维护超大的 map 和 list。

因此，**一般来说存储引擎会基于原始 LRU 算法进行一些优化再落地**。比如 LevelDB/RocksDB 采用 sharded 方式将 block cache 划分为多个桶，既能有效降低多核竞争带来的性能影响，也能减少每个 LRU 单元操作的数据结构大小。InnoDB 引擎也通过 [innodb_buffer_pool_instances](https://dev.mysql.com/doc/refman/8.0/en/innodb-multiple-buffer-pools.html) 实现了类似策略，降低多核竞争。在 [InnoDB](https://zhuanlan.zhihu.com/p/642561326) 中，为了降低全表扫描污染 LRU 排序，会将访问到的数据先插到 LRU 队列的 5/8 处，而不是一次性移动到队头。

## 3.2 WT 的 LRU 实现
### 3.2.1 基本思想
WT 直接摒弃了传统的 LRU 算法，并没有使用 list 去实时维护 cache 中的访问情况，当然也就没有对应的哈希 map。    
WT 中的每个内存 page 有一个 int64 类型的 read_gen 来标记访问情况。笼统来说，这个值越高就表示最近更频繁被访问，就越不会淘汰。可以参考 [__wt_page](https://github.com/mongodb/mongo/blob/r4.2.24/src/third_party/wiredtiger/src/include/btmem.h#L704) 的定义：    
```
struct __wt_page {
    ...

    /*
    * The page's read generation acts as an LRU value for each page in the
    * tree; it is used by the eviction server thread to select pages to be
    * discarded from the in-memory tree.
    *
    * The read generation is a 64-bit value, if incremented frequently, a
    * 32-bit value could overflow.
    *
    * The read generation is a piece of shared memory potentially read
    * by many threads.  We don't want to update page read generations for
    * in-cache workloads and suffer the cache misses, so we don't simply
    * increment the read generation value on every access.  Instead, the
    * read generation is incremented by the eviction server each time it
    * becomes active.  To avoid incrementing a page's read generation too
    * frequently, it is set to a future point.
    *
    * Because low read generation values have special meaning, and there
    * are places where we manipulate the value, use an initial value well
    * outside of the special range.
    */
    uint64_t read_gen;

    ...
}
```

代码看到这里，似乎能猜到 WT 中 LRU 的实现思路了：   
1. ~~每次访问到 page 时（无论读还是写），将 read_gen 设置为当前时间。~~
2. ~~负责淘汰的 eviction 线程扫描所有 page 的 read_gen，做一次排序，然后将末尾的 page 淘汰。~~

但是继续研究代码，会发现上述说法一条都不对。。

首先要说明的是并发修改 read_gen 的问题。    
我们知道，在一个高并发的数据库中，内存 page 是极有可能被**多个线程并发访问**的，特别是 internal page，尤其是 root page。如果多个 CPU 同时读取并修改 root page 的 read_gen，很有可能**由于 cacheline 失效的问题导致性能受限**。   
关于为什么多核场景下，更新同一个变量会有性能下降问题。可以参考[多线程的性能杀手](https://zhuanlan.zhihu.com/p/633269064) 这篇博客中关于 cache 乒乓问题的描述。

再说说全局排序问题。     
对于 list 实现的 LRU 算法来说，每次 page 的访问都会对应调整 list 中的排序，相当于将 list 的排序分摊到了每次的 page 访问请求中。而 WT 的 LRU 算法中并没有 list，如果将排序操作都放在 eviction 后台线程扫描的时候来做，耗时可能会很大。想象一个 cacheSize 有 100GB，如果扫描这 100GB 再排序，将会花费多长时间呢？有没有可能还没完成排序，内存就已经爆了？    

**WT 的解决方案**

WT 通过巧妙的设计来解决并发修改 read_gen 的性能问题。个人认为有 2 个关键点：    
1. 整个 cache 维护了一个全局的 read_gen，在 eviction 流程每次启动 1 轮扫描时，会将这个值[加 1](https://github.com/mongodb/mongo/blob/r4.2.24/src/third_party/wiredtiger/src/include/cache.i#L34)。
2. 用户线程在获取到 page 的 hazard_pointer 进行访问时，将该 page 的 read_gen 提前[设置成一个较大值](https://github.com/mongodb/mongo/blob/r4.2.24/src/third_party/wiredtiger/src/include/cache.i#L44)，即 page->read_gen = cache->read_gen + 100。后续其他线程也获得 hazard_pointer 时，会判断 read_gen 是否已经设置为一个较大值，如果是则不会更新。这种方式将 read_gen 的更新降低了很多个数量级。

WT 的 LRU 算法能够避免多个 CPU 频繁更新共享内存导致 cacheline 失效的问题，提升整体性能，但与此同时也**牺牲了一定的 LRU 准确性**：      
- cache->read_gen 全局变量会随着每轮扫描递增，这个值本身就具有**时效性**，因此可以作为 LRU 的参考。根据实测结果，每轮扫描的间隔时间在空闲时为 200ms 左右，在忙时为几毫秒或者几十毫秒都有可能（上述数据，是在腾讯云2C2G规格的云主机，自建 4.2.24 版本副本集，使用 YCSB 压测的结果，仅做参考，下同）。因此，cache->read_gen 就像逻辑时钟一样持续推进。
- 如果 page 被访问，设置 page->read_gen = cache->read_gen+100。如果 page 没有被访问，则不会增加 page->read_gen。因此，使用 page->read_gen 作为 LRU 打分的依据也是合适的。但是如果在同一个 evict 扫描周期内，先后扫描了 page A 和 B，2 者的 read_gen 相同，因此使用 read_gen 来作为 LRU 打分也会存在一定的误差。

如下图所示，cache->read_gen 推进到 2000，每个 page 的 read_gen 都是 1000（这些 page 都有一段时间没有访问了）。随后， A、B、C、D、E 都被正常访问，因此 read_gen 都设置为 2100。F 没有被访问，read_gen 保持在 1000。 G 被访问并且被删除了非常多的数据，read_gen 直接被设置为 1（优先淘汰）。    
<TODO 加一个示意图>     

另外，为了解决全局排序耗时太长的问题，WT 将全局排序转换为局部排序。不寻求 LRU 的全局最优，而是需求局部最优，这样可以快速扫描完成并立即开展 page 的淘汰工作。不过这种方式也会带来一些问题，比如是否会导致某些值得 evict 的 page 迟迟扫描不到呢？WT 在每轮扫描结果时，需要记录当时的扫描位置，避免重复扫描。      
那么，每次扫描多少个 page 合适呢？如果扫描的多了，server 和 worker 的流水线不是很“顺畅”，影响性能；如果扫描少了，得到的是个非常局部的最优解，影响 cache 命中率，也会影响性能。     

根据实测结果，每轮扫描的 page 从几百到几千不等，dirty 高的场景下可能到几万。具体策略可以参考代码 [__evict_walk_tree](https://github.com/mongodb/mongo/blob/r4.2.24/src/third_party/wiredtiger/src/evict/evict_lru.c#L1656) 的实现。    
扫描后，会将多达 300 个 page 放到 queue 中，然后排序后选取几十到几百个 page 执行 evict 操作。     
>特别说明，上述数据也是自建 4.2.24 版本副本集，使用 YCSB 进行 1:1 测试的结果。数据列出来主要是为了比较量化的理解执行流程。不同版本不同环境下的数据可能会有些差异。    
>测试数据是通过在代码中加日志获取的，可以参考后面附录中的截图。

### 3.2.2 打分和微调
Page 的 read_gen 是 LRU 评分的重要参考，但是结合前面的分析，我们知道为了保证性能，read_gen 并不精确。      
[__evict_entry_priority](https://github.com/mongodb/mongo/blob/r4.2.24/src/third_party/wiredtiger/src/evict/evict_lru.c#L61) 函数实现了 read_gen 转换为 LRU score 的流程：    
```
/*
 * __evict_entry_priority --
 *     Get the adjusted read generation for an eviction entry.
 */
static inline uint64_t
__evict_entry_priority(WT_SESSION_IMPL *session, WT_REF *ref)
{
    WT_BTREE *btree;
    WT_PAGE *page;
    uint64_t read_gen;

    btree = S2BT(session);
    page = ref->page;

    /* Any page set to the oldest generation should be discarded. */
    if (WT_READGEN_EVICT_SOON(page->read_gen))
        return (WT_READGEN_OLDEST);

    /* Any page from a dead tree is a great choice. */
    if (F_ISSET(btree->dhandle, WT_DHANDLE_DEAD))
        return (WT_READGEN_OLDEST);

    /* Any empty page (leaf or internal), is a good choice. */
    if (__wt_page_is_empty(page))
        return (WT_READGEN_OLDEST);

    /* Any large page in memory is likewise a good choice. */
    if (page->memory_footprint > btree->splitmempage)
        return (WT_READGEN_OLDEST);

    /*
     * The base read-generation is skewed by the eviction priority. Internal pages are also
     * adjusted, we prefer to evict leaf pages.
     */
    if (page->modify != NULL && F_ISSET(S2C(session)->cache, WT_CACHE_EVICT_DIRTY) &&
      !F_ISSET(S2C(session)->cache, WT_CACHE_EVICT_CLEAN))
        read_gen = page->modify->update_txn;
    else
        read_gen = page->read_gen;

    read_gen += btree->evict_priority;

#define WT_EVICT_INTL_SKEW 1000
    if (WT_PAGE_IS_INTERNAL(page))
        read_gen += WT_EVICT_INTL_SKEW;

    return (read_gen);
}
```

可以看到代码中依据 read_gen 进行了许多微调，来实现更加精准的淘汰：
- 被其他流程作了 "EVICT_SOON" 特殊标记的，尽快淘汰。这些标记可能是其他正常读写流程设置的，比如在扫描 btree 的时候，如果发现有个 page 的 deleted_count 超过 1000，会[设置标记](https://github.com/mongodb/mongo/blob/r4.2.24/src/third_party/wiredtiger/src/btree/bt_curnext.c#L679)，希望尽快淘汰。
- Dead btree 的 page 尽快淘汰。
- 空 page 尽快淘汰。
- Page 大小超过 splitmempage 阈值的尽快淘汰。
- Internal page 加 1000 分，尽量不要淘汰。优先淘汰 leaf page。
- 有些 btree 要区别对待，比如 metadata 表的 evict_priority 设置的较高，尽量不要淘汰。

### 3.2.3 排序
[__evict_lru_walk](https://github.com/mongodb/mongo/blob/r4.2.24/src/third_party/wiredtiger/src/evict/evict_lru.c#L1125) 函数实现了队列的排序，以及淘汰哪些 page 的逻辑：
```
/*
 * __evict_lru_walk --
 *     Add pages to the LRU queue to be evicted from cache.
 */
static int
__evict_lru_walk(WT_SESSION_IMPL *session) {
    ...
    // 将 page 加入到 queue 中
    /*
     * Get some more pages to consider for eviction.
     *
     * If the walk is interrupted, we still need to sort the queue: the next walk assumes there are
     * no entries beyond WT_EVICT_WALK_BASE.
     */
    if ((ret = __evict_walk(cache->walk_session, queue)) == EBUSY)
        ret = 0;
    WT_ERR_NOTFOUND_OK(ret);
    
    ...

    // 按照 page 的 score 排序
    __wt_qsort(queue->evict_queue, entries, sizeof(WT_EVICT_ENTRY), __evict_lru_cmp);

    ...
    // 选择哪些 page 进行淘汰
    /* Decide how many of the candidates we're going to try and evict. */
    if (__wt_cache_aggressive(session))
        queue->evict_candidates = entries;
    else {
        /*
         * Find the oldest read generation apart that we have in the queue, used to set the initial
         * value for pages read into the system. The queue is sorted, find the first "normal"
         * generation.
         */
        read_gen_oldest = WT_READGEN_START_VALUE;
        for (candidates = 0; candidates < entries; ++candidates) {
            read_gen_oldest = queue->evict_queue[candidates].score;
            if (!WT_READGEN_EVICT_SOON(read_gen_oldest))
                break;
        }

        /*
         * Take all candidates if we only gathered pages with an oldest
         * read generation set.
         *
         * We normally never take more than 50% of the entries but if
         * 50% of the entries were at the oldest read generation, take
         * all of them.
         */
        if (WT_READGEN_EVICT_SOON(read_gen_oldest))
            queue->evict_candidates = entries;
        else if (candidates > entries / 2)
            queue->evict_candidates = candidates;
        else {
            /*
             * Take all of the urgent pages plus a third of ordinary candidates (which could be
             * expressed as WT_EVICT_WALK_INCR / WT_EVICT_WALK_BASE). In the steady state, we want
             * to get as many candidates as the eviction walk adds to the queue.
             *
             * That said, if there is only one entry, which is normal when populating an empty file,
             * don't exclude it.
             */
            queue->evict_candidates = 1 + candidates + ((entries - candidates) - 1) / 3;
            cache->read_gen_oldest = read_gen_oldest;
        }
    }
}
```

# 4. Eviction 架构分析
## 4.1 整体架构
在 WT 初始化时，会创建一组 evict 线程池。所有 evict 线程在运行时会抢锁，并且只有 1 个线程抢锁成功后执行 evict_server 逻辑，其余线程执行 evict_worker 逻辑：    
1. **evict_server** 逻辑：完成 btree 的 page [扫描](https://github.com/mongodb/mongo/blob/r4.2.24/src/third_party/wiredtiger/src/evict/evict_lru.c#L1656)、打分、[排序](https://github.com/mongodb/mongo/blob/r4.2.24/src/third_party/wiredtiger/src/evict/evict_lru.c#L1196)操作，并将[符合 evict 条件](https://github.com/mongodb/mongo/blob/r4.2.24/src/third_party/wiredtiger/src/evict/evict_lru.c#L1199-L1264)的 page 信息保留在 evict_queue 中。    
2. **evict_worker** 逻辑：从 evict_queue 中获取 page 并进行检查。如果没有 page 没有修改过，则直接淘汰。如果修改过，则进行 reconcile 刷脏页逻辑：分裂、序列化、块级压缩、加密等转换流程，并根据 BlockManager 选择合适的 extent 执行写入。另外，在 cache 的内存使用率达到一定阈值后，用户请求处理线程也会执行 evict_worker 逻辑。如果整个存储引擎只配置了 1 个 evict 线程，则该线程会执行 evict_server 和 evict_worker 的全部逻辑。    
3. **evict_queue**：在 WT 初始化时，会[创建](https://github.com/mongodb/mongo/blob/r4.2.24/src/third_party/wiredtiger/src/conn/conn_cache.c#L243-L253) 3 个队列（数组实现），每个队列长度 400。队列中每个元素包含了候选 page 的[元数据信息](https://github.com/mongodb/mongo/blob/r4.2.24/src/third_party/wiredtiger/src/include/cache.h#L27)（ page的指针、打分和所属的 btree）。前 2 个队列是常规队列，组成“双 buffer”架构。最后一个队列是 urgent 紧急队列，对于那些需要紧急淘汰的page（比如page已删除等）会放在这个队列优先处理。    

<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/20caecd7-8524-469a-b71f-8fbb7c5fcb33" width=800>
</p>

Evict_server 和 evict_worker 线程通过 evict_queue 传递数据，并通过[条件变量](https://github.com/mongodb/mongo/blob/r4.2.24/src/third_party/wiredtiger/src/evict/evict_lru.c#L1273)发送通知。    
如果 evict_worker 执行的效率太慢，导致 cache 的使用率或者脏页比例超过了一定阈值，用户请求处理线程（上图中的 app thread）也会执行 evict_worker 的逻辑。此时用户侧可能发现有大量慢请求。

## 4.2 如何触发
触发 eviction 的条件，可以参考官方文档中关于 [wiredtiger_open](http://source.wiredtiger.com/develop/group__wt.html#gacbe8d118f978f5bfc8ccb4c77c9e8813) 配置项的描述，总结如下：    
|参数|说明|默认值（取值范围）|备注|
|:--|:--|:--|:--|
|eviction_dirty_target|cache 脏数据百分比上限（后台线程进行 eviction）|5%（1-100%）||
|eviction_dirty_trigger|cache 脏数据百分比上限（用户线程也参与eviction）|20%（1-100%）||
|eviction_target|cache 使用率百分比上限（后台线程执行 eviction）|80%（10-100%）||
|eviction_trigger|cache使用率百分比上限（用户线程也参与 eviction）|95%（10-100%）||
|eviction_updates_target|cache 中 update 百分比上限（后台线程执行 eviction）|eviction_dirty_target/2<br>（0-100%）|WT 10.0.0 版本引入|
|eviction_updates_trigger|cache 中 update 百分比上限（用户线程也参与 eviction）|eviction_dirty_trigger/2<br>（0-100%）|WT 10.0.0 版本引入|
|eviction_checkpoint_target|每次执行 checkpoint 之前，cache 中脏数据的百分比上限|1%（0-100%）||
|threads_max|后台最大的 evict 线程数|8（1-20）|不同版本的默认值可能不同|
|threads_min|后台最小的 evict 线程数|1（1-20）|不同版本的默认值可能不同|

>注：不同版本的默认值可能有区别。另外这里列出的是 WT 侧的默认值，MongoDB 在使用 WT 时可能会对这些参数进行调整，比如 4.0/4.2 版本中会将 threads_max 和 threads_min 设置为 4，请参考对应版本代码中 WiredTigerKVEngine 构造函数的实现。

每次启动一轮扫描（__evict_paas），都会调用 [__evict_update_work](https://github.com/mongodb/mongo/blob/r4.2.24/src/third_party/wiredtiger/src/evict/evict_lru.c#L538) 制定 evict 的执行策略。__evict_update_work 会根据上述配置以及当前的内存使用情况，决定出本次[淘汰策略](https://github.com/mongodb/mongo/blob/r4.2.24/src/third_party/wiredtiger/src/include/cache.h#L246-L258)，可以是下面标志位的1种或者多种组合：
```
#define WT_CACHE_EVICT_CLEAN 0x001u      /* Evict clean pages */
#define WT_CACHE_EVICT_CLEAN_HARD 0x002u /* Clean % blocking app threads */
#define WT_CACHE_EVICT_DEBUG_MODE 0x004u /* Aggressive debugging mode */
#define WT_CACHE_EVICT_DIRTY 0x008u      /* Evict dirty pages */
#define WT_CACHE_EVICT_DIRTY_HARD 0x010u /* Dirty % blocking app threads */
#define WT_CACHE_EVICT_LOOKASIDE 0x020u  /* Try lookaside eviction */
#define WT_CACHE_EVICT_NOKEEP 0x040u     /* Don't add read pages to cache */
#define WT_CACHE_EVICT_SCRUB 0x080u      /* Scrub dirty pages */
#define WT_CACHE_EVICT_URGENT 0x100u     /* Pages are in the urgent queue */
/* AUTOMATIC FLAG VALUE GENERATION STOP */
#define WT_CACHE_EVICT_ALL (WT_CACHE_EVICT_CLEAN | WT_CACHE_EVICT_DIRTY)
```
第 1 个需要特别说明的是 WT_CACHE_EVICT_SCRUB 状态。这里涉及到一个问题：淘汰的时候是将 page 全部释放，还是只清理 page 中的垃圾值（比如全部事务不可见的修改）？    
WT 制定了如下策略：如何 cache 使用率小于 target+dirty 的平均值（默认是 87.5%）且脏页比例小于 dirty_target+dirty_trigger 的平均值（默认 12.5%），则认为内存使用率还不是太高，则选择 SCRUB 策略，将 page 擦洗之后继续保留在内存中。这样既能提高内存的使用效率，也能充分利用内存提升访问的命中率。

4.2 内核的[判断条件](https://github.com/mongodb/mongo/blob/r4.2.24/src/third_party/wiredtiger/src/evict/evict_lru.c#L598-L606)如下：
```
/*
* Scrub dirty pages and keep them in cache if we are less than half way to the clean or dirty
* trigger.
*/
if (bytes_inuse < (uint64_t)((target + trigger) * bytes_max) / 200) {
    if (dirty_inuse < (uint64_t)((dirty_target + dirty_trigger) * bytes_max) / 200)
        LF_SET(WT_CACHE_EVICT_SCRUB);
} else
    LF_SET(WT_CACHE_EVICT_NOKEEP);
```
>高版本 WT 还有 update 比例小于 eviction_updates_target+eviction_updates_trigger 平均值的约束。

第 2 个需要说明的是 WT_CACHE_EVICT_LOOKASIDE 状态。MongoDB 从 4.0 版本的 WT 引擎开始，引入了 LAS/[History Store](http://source.wiredtiger.com/develop/arch-hs.html#arch_hs_table) （后面简称 HS）来将暂时需要保留的历史版本数据换出到磁盘，缓解内存压力。    
WT 引擎中的所有用户表都共用同一个 HS 表，不同用户表的 key 通过 btree id 前缀进行区分。如下图所示：    

<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/7d094e07-b2d2-4de8-896b-e99c55707978" width=800>
</p>

由于淘汰历史版本到 HS 的代价较大（包括硬盘上数据块的读写 IO，写journal 的开销等），因此执行 [HS 淘汰的条件]((https://github.com/mongodb/mongo/blob/r4.2.24/src/third_party/wiredtiger/src/evict/evict_lru.c#L608-L618))也比较苛刻。


如果需要动态调整配置参数，可以使用 mongosh 连上节点后执行命令：
```
db.adminCommand({setParameter:1, wiredTigerEngineRuntimeConfig:'cache_size=1G,eviction_dirty_target=4,eviction_dirty_trigger=50,eviction_target=80,eviction_trigger=95,eviction_checkpoint_target=5,eviction=(threads_max=16,threads_min=16)'})
```

详细的淘汰过程可以参考淘宝的数据库月报：http://mysql.taobao.org/monthly/2020/07/02/
## 4.3 线程池的自适应调整
WT 初始化时可以指定 evict 线程池大小，比如 eviction=(threads_min=4,threads_max=8) 表示线程池的大小取值为 [4, 8]。    
究竟多少个线程最优呢？在不同软硬件环境和工作负载下，无法给出统一的最优解。因此，evict_server 需要根据 evict 吞吐量变化，对线程数进行自动调整，得到当前场景下的最优值。

如下示意图所示，evict 线程数在某个特定值时，吞吐量（每秒处理的 page数）达到最高：    
- 如果线程数太少，等待 evict 的 pages 可能在队列中积压，无法充分利用 CPU 的能力。    
- 如果线程数太多，可能会有大量的锁争抢和上下文切换，导致性能降低。    

<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/98320ef3-e27a-4744-82bb-67aaa70fed20" width=600>
</p>

另外，实例运行的硬件环境也可能随时出现变化。特别是云环境，增减 CPU 核数或者调整内存大小都是常见操作。因此，evict 线程池也需要具备自我调节能力，来保证运行效率。

evict_server 会周期性调用 [__evict_tune_workers](https://github.com/mongodb/mongo/blob/r4.2.24/src/third_party/wiredtiger/src/evict/evict_lru.c#L944) 找到最优的线程个数，下面以 eviction=(threads_min=1, threads_max=20)的配置为例，说明自适应调整的流程。    

**初次调整**（WT初始化时已经创建 threads_min=1 个线程）：    
1. 每隔 60ms 增加 1 个线程，分为 8 轮，依次统计 [1 , 9] 个线程场景下的吞吐，并记录（最优吞吐，对应的线程数）。    
2. 发现线程数为 9 时达到最优吞吐，则说明还有继续增加线程数的空间。后续继续每隔 60ms 增加 1 个线程，分 8 轮，依次统计[10, 17] 个线程场景下的吞吐，并记录（最优吞吐，对应的线程数）。    
3. 发现线程数为 12 时达到最优吞吐。则说明当前环境下，17 个线程已经越过了最优解，没有必要通过增加线程数继续尝试。此时，将线程数一次性调整为 12，并稳定运行一段时间。    

**后续调整**（稳定运行 25 秒之后）：    
1. 解除稳定态，并减少 1 个线程，即将上述 12 个线程降低到 11 个。    
2. 每隔 60ms 增加 1 个线程，分 8 轮，依次统计[11, 19] 个线程场景下的吞吐，并记录（最优吞吐，对应的线程数）。    
3. 发现线程数为 11 时达到最优吞吐，此时将线程数一次性地调整为 11，并将当前状态置为稳定态。    
4. 以稳定状态持续运行 25 秒。    

运行流程如下图所示：

<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/8c11a58f-b1a9-4b37-80e9-df13b06c37cb" width=1000>
</p>

# 5. 经验总结
MongoDB 在读写请求非常多的场景下，可能会出现 evict 效率太低，导致用户请求执行速度也变慢的情况。    
结合上述分析以及多年的运营经验，建议如下：    
1. 扩容实例的内存，尽量增加 cache 大小。这样可以给 evict 流程预留更多的缓冲空间，从而尽可能减少用户请求 hang 住的可能性。    
2. 根据实际情况进行配置优化。包括调整 eviction 线程池上限，调整 eviction_target 和 eviction_trigger 的配置等。基本的调优思路就是在 cache 压力较大的情况下，尽早触发 evict 操作，并尽量提升操作并发，降低用户请求线程介入的概率。    

在执行配置调整时，常存在以下**误区**：    
1. Evict 线程越多越好。首先，增加 evict 线程只会增加 evict_worker，而很多场景下瓶颈却在 evict_server，需要根据具体场景进行分析；其次，线程数太多也会带来抢锁、上下文切换等开销。    
2. 将 eviction_target 调到很低，尽早触发 evict。此时正常使用的 cache 大小也会变低，从而导致 cache 命中率降低，也会影响性能。    

个人认为，evict_server 的单线程设计，对于超大规模实例（比如几百GB内存，几 TB 的硬盘）来说，会是非常值得关注的性能隐患。    
我曾经在业务运营中碰到这个场景：在业务高峰期，机器上一直有个 CPU 核的使用率为 100%，使用 pstack 查看这个线程的执行堆栈，发现都是 evict_server 线程在扫描 page。    
有没有可能将 evict_server 变成多线程呢？我觉得是能实现的。不过 wiredtiger 的代码比较复杂，没有百分百的把握不要轻易去“优化”。一直比较直接可控的处理方式是使用分片集群，多搞几个实例。比如提到的 “几百 GB内存，几 TB 硬盘” 的硬件配置，如果切分成 10 个逻辑单元，部署 10 个 mongod 进程，那不就对应有 10 个 evict_server 了吗？就不会存在并发受限的问题了。    

个人认为，单个 mongod 的线性扩展能力，其实是存在明显的瓶颈的。比如前面提到的拉 oplog 线程、本文提到的 evict_server 线程、暂未提到的 checkpoint 线程，在架构设计上就存在着单线程并发受限问题。如果这些流程成为了性能瓶颈，加再多的 CPU核和内存，起到的效果也不会明显。    
因此，对于访问量巨大的使用场景，我推荐优先考虑分片集群。

# 附录
补充一个流程图，描述 evict 核心函数的大致调用关系，方便学习代码：     

<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/0807685a-0c88-49e9-9e86-3637af1ea841" width=1000>
</p>

添加日志，观察 evict_server 流程每轮扫描的时间间隔：
空闲时：

<TODO 加一个截图>

YCSB 压测时：

<TODO 加一个截图>

添加日志，观察压测时 evict_server 扫描的 page 数：
<TODO 加一个截图>

添加日志，观察压测时 evict_server 放到队列中的 page 数和最终会执行 evict 的 page 数：
<TODO 加一个截图>

# 参考文档
1. LevelDB源码解读-LRU缓存：https://zhuanlan.zhihu.com/p/649896220
2. 多线程的性能杀手：https://zhuanlan.zhihu.com/p/633269064
3. http://source.wiredtiger.com/11.1.0/arch-cache.html#arch_cache_size
4. https://www.mongodb.com/docs/v4.2/reference/configuration-options/#storage.wiredTiger.engineConfig.cacheSizeGB
5. MySQL 之 InnoDB 存储结构：https://zhuanlan.zhihu.com/p/642561326
6. https://github.com/mongodb/mongo/blob/r4.2.24/src/
7. http://source.wiredtiger.com/develop/arch-eviction.html#eviction_overall
8. http://source.wiredtiger.com/develop/group__wt.html#gacbe8d118f978f5bfc8ccb4c77c9e8813
9. 数据库内核月报：http://mysql.taobao.org/monthly/2020/07/02/
10. https://dev.mysql.com/doc/refman/8.0/en/innodb-multiple-buffer-pools.html
11. https://mp.weixin.qq.com/s/dsOWrHO1MRFJXFYGEsiCng
