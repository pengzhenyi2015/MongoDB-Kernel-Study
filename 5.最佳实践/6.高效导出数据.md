# 1. 导语
在真实的业务场景中，时常会遇到如何将一个大表的数据快速导出的问题。对于这种场景，普通的全表扫描由于存在并发受限，往往无法满足业务需求。     

因此，本文尝试探讨**如何并行化导出数据集**，以及如何避免一些常见的陷阱。

# 2. 切分数据集
要实现并行化导出，首先要完成数据切分。    

以按照 _id 字段切分为例，常见的切分方法：   
1. 业务层自己切分。    
    a. 如果字段的取值非常有规律，比如是从 1 递增的整数，则自己做一个简单的除法即可解决。    
    b. 如果分布不规律，则可以通过索引排序，然后用 skip+limit 查询的方式，遍历索引得到想要的切分点。具体例子可以参考 [mongocheck](https://github.com/pengzhenyi2015/mongocheck#) 中 checkCollection 的采样逻辑。
2. 使用 [splitVector](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/s/split_vector.cpp) 命令，在 MongoDB 服务端进行切分。比如希望 1MB 作为一个切分块，则可以执行下面的命令：
    ```
    db.runCommand({splitVector:"db1.coll2", keyPattern:{_id:1}, maxChunkSize:1})
    ```
    splitVector 也是 MongoDB 分片表进行 chunk 切分的底层命令。其中 `keyPattern` 指定了切分的 key，`maxChunkSize` 表示每个 chunk 的最大大小, 单位是 MB。    
    具体执行的逻辑是，先根据表大小和总条数计算每条文档的平均大小，然后计算出每个 chunk 应该包含的文档数。然后执行索引遍历，找到每个切分点。
    如果在 mongos 上执行 splitVector 切分分片表，会报错。可以考虑其他方式进行切分，比如通过路由表进行切分：
    ```
    use config
    db.chunks.aggregate([{$match: {ns: "db1.shardcollection1"}}, {$project: {_id: 0, min: 1}}, {$sample: {size: 8}}, {$sort: {min: 1}}])
    ```
    **对于分片集群，最高效的方法还是关闭 balancer 之后直接从 shardServer 拉数据，而不是 mongos。**业界比较流行的同步工具，比如官方的 [mongosync](https://www.mongodb.com/zh-cn/docs/mongosync/current/topologies/multiple-mongosyncs/) 和 阿里云的 [mongoshake](https://github.com/alibaba/MongoShake/blob/release-v2.8.6-20250825/conf/collector.conf#L61) 都是采用的这种方式。


# 3. 并行化导出数据集
有了切分点之后，就可以并发地根据切分点执行范围查询，导出数据了。
由于 MongoDB 的索引都是二级索引，所以在底层的执行计划都是 INDEX_SCAN，即扫描索引然后再回表。即使 _id 索引也是二级索引，首先要通过 _id 查到内部的 recordId, 然后再根据 recordId 去回表（表结构的 B+ 树由 recordId 组织，而不是 _id）。

从 MongoDB 5.3 版本开始，用户可以根据 _id 创建 Clustered Collection，即聚集索引。表结构的 B+ 树直接由 _id 组织， 因此使用 _id 进行范围查询要快很多（首先**不需要先查索引再回表**，其次 _id 相邻的文档在表文件中物理上连续使得 **IO 效率更高**）。

# 4. 范围查询的"比较" 陷阱
根据切分点进行范围查询，一定能读到全部数据吗？     

我们需要注意到 MongoDB 的一个设计细节：**使用比较操作的时候，只会返回和比较 key 类型相同的文档。**可以参考 [ComparisonMatchExpression::matchesSingleElement](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/matcher/expression_leaf.cpp#L117) 代码的实现。

比如 coll2 表中有 5 条数据，a 字段的类型有数值、字符串、ObjectId、数组 多种类型，如下所示：
```
mymongo:PRIMARY> db.coll2.find()
{ "_id" : ObjectId("681488df5938079fa6272b80"), "a" : 1 }
{ "_id" : ObjectId("681488df5938079fa6272b81"), "a" : "2" }
{ "_id" : ObjectId("681488df5938079fa6272b82"), "a" : [ "1", "2", "3" ] }
{ "_id" : ObjectId("681488df5938079fa6272b83"), "a" : 4 }
{ "_id" : ObjectId("681488df5938079fa6272b84"), "a" : ObjectId("681488df5938079fa6272b7f") }
```

假设现在的分割点是 a=4, 将表分为两部分。那么通过范围查询只能读取到数值类型的文档，而不是所有文档：
```
// a >= 4 的所有文档
mymongo:PRIMARY> db.coll2.find({a:{$gte:4}})
{ "_id" : ObjectId("681488df5938079fa6272b83"), "a" : 4 }

// a < 4 的所有文档
mymongo:PRIMARY> db.coll2.find({a:{$lt:4}})
{ "_id" : ObjectId("681488df5938079fa6272b80"), "a" : 1 }
```

由于 MongoDB 是一个典型的 schema-free 的数据库，同一字段有多种类型的数据是可能的。     
因此，如果要并行化导出数据集，就必须考虑到这一点。   

以阿里云 mongoshake 为例，内部实现了 [splitVector 分块](https://github.com/alibaba/MongoShake/blob/release-v2.8.6-20250825/collector/docsyncer/doc_reader.go#L110) + [find(\$gte,\$lte)](https://github.com/alibaba/MongoShake/blob/release-v2.8.6-20250825/collector/docsyncer/doc_reader.go#L209) 的并行化导出方案。但是在配置文件中给出了明确的[风险提示](https://github.com/alibaba/MongoShake/blob/release-v2.8.6-20250825/conf/collector.conf#L203)：
>对单个表来说，仅支持索引对应的value是同种类型，如果有不同类型请勿启用该配置项！


# 5. 探讨一些可行的解决方案
## 5.1 做好类型检查
对于我遇到过的很多业务场景来说， 1 个字段从始至终只有一种类型也是很常见的。比如很多业务都不会自己设置 _id，则默认都是 ObjectId 类型。    

因此，我们可以在评估并行导出的方案之前，先对字段的类型进行统计。    

比如利用索引查询 _id 的最大值和最小值，判断类型是否相同。
```
mymongo:PRIMARY> db.t1.find().sort({_id:1})
{ "_id" : 1 }
{ "_id" : 2 }
{ "_id" : 3 }
{ "_id" : "2" }
{ "_id" : ObjectId("68b6f358504f693358471f4b") }
{ "_id" : ObjectId("68b6f47c504f693358471f4c"), "a" : 2 }
{ "_id" : ObjectId("68b6f47d504f693358471f4d"), "a" : 3 }
{ "_id" : ISODate("2025-09-02T13:38:32.559Z") }

# 获取 key 的最小值，是一个数字
mymongo:PRIMARY> db.t1.find().sort({_id:1}).limit(1)
{ "_id" : 1 }

# 获取 key 的最大值，是一个 Date
mymongo:PRIMARY> db.t1.find().sort({_id:-1}).limit(1)
{ "_id" : ISODate("2025-09-02T13:38:32.559Z") }
```
**如果只有一种类型，则可以放心使用范围查询**。更详细的讨论可以参考给 MongoShake 提的 [issue](https://github.com/alibaba/MongoShake/issues/927).      

如果想对字段类型进行精确统计，可以使用聚合命令对 _id 字段的类型进行分类统计：
```
mymongo:PRIMARY> db.coll1.aggregate([{$group:{_id:{$type : "$_id"}, count: {$sum:1}}}])
{ "_id" : "double", "count" : 3 }
{ "_id" : "objectId", "count" : 2 }
```
不过这个命令会进行全表扫描，因此大表慎用。


## 5.2 退化成使用全表扫描

如果 _id 的类型多种多样，则比较稳妥的方式是退化使用全表扫描。即 find + 后续 getmore 的方式。    
不过这里又有一个问题：如果全表扫描的过程中失败退出了，如何进行断点续传？

---

对于 **MongoDB 4.4 及以上版本**，在进行顺序（自然序）扫描时，可以指定服务端返回 requestResumeToken，即返回扫描到的最后一条数据的 recordId.        
如果需要断点续传，则指定 resumeAfter, 从上次扫描到的位置继续读取。    

举例如下，先使用 _requestResumeToken 获取末尾记录的 recordId
```
db.runCommand({find:"coll1", "$_requestResumeToken": true, hint: {$natural: 1}})
```

使用 _resumeAfter ，并指定 recordId, 可以从上一次断开的地方进行重传：
```
db.runCommand({find:"coll1", "$_requestResumeToken": true, "$_resumeAfter":{"$recordId":NumberLong(101)}, hint:{$natural:1}})
```
由于 recordId 是内部递增的 int64 类型的 ID，因此不会出现前面说的类型陷阱。

> MongoDB 在更高版本（5.3）支持了 Clustered Collection（聚集索引）之后， recordId 不再是单纯的 int64. 上述 requestResumeToken 返回的 recordId 类型也从 NumberLong 变成了 Binary, 除此之外命令的使用方式还是相同。

---

对于 **MongoDB 4.4 以下版本**，断点续传似乎不好处理。    
一种变通的方法，是使用 _id 字段进行升序扫描（降序也行，原理类似）。如果需要断点续传，则根据上次扫描到的 _id , 进行 `$gt` 范围查询。    
不过这种方式本质上和全表扫描还是不一样的。首先 INDEX_SCAN 的性能不如真正的 COLL_SCAN, 而且采用 _id 的比较操作进行断点续传，也会遇到前面说的类型陷阱。

## 5.3 使用 Clustered Collection

MongoDB 高版本支持使用 Clustered Index(聚簇索引) 建表。
举例如下：
```
test> db.createCollection("coll1", {clusteredIndex: {key : {"_id":1}, unique: true, name: "clustered index"}})
{ ok: 1 }
```

使用上述方式建表之后，对于 _id 的范围查询，会从 INDEX_SCAN 变成 CLUSTERED_IXSCAN，性能会高很多。
但是，使用 _id 进行比较操作，还是会存在前面说的类型陷阱。

## 5.4 对不同的类型，分别执行并行导出
综合前面的讨论，并行化导出最大的问题是比较超过遇到的多类型问题。        
那么是否能执行这样一种策略：首先统计有多少种类型，然后对每个类型进行并行化导出？    
从理论上来讲，应该是可行的。

## 5.5 魔改内核代码，支持比较操作导出所有类型的数据
既然 MongoDB 是开源的，理论上我们可以“魔改”内核代码来实现我们的需求 ： 比较操作导出所有类型。    

以 r4.2.5 版本为例，我们可以进行如下改动，增加一个系统参数 compareReturnAllTypes(默认关闭)，当开启时，比较操作不再区分类型。 不同 BSON 类型的比较顺序遵循[官方定义](https://www.mongodb.com/docs/manual/reference/bson-type-comparison-order/)：

```
diff --git a/src/mongo/db/matcher/SConscript b/src/mongo/db/matcher/SConscript
index f8ecefcdc8d..92dcbdc96a3 100644
--- a/src/mongo/db/matcher/SConscript
+++ b/src/mongo/db/matcher/SConscript
@@ -67,6 +67,7 @@ env.Library(
         'schema/json_pointer.cpp',
         'schema/json_schema_parser.cpp',
         env.Idlc('schema/encrypt_schema.idl')[0],
+        env.Idlc("expression_leaf.idl")[0],
     ],
     LIBDEPS=[
         '$BUILD_DIR/mongo/base',
diff --git a/src/mongo/db/matcher/expression_leaf.cpp b/src/mongo/db/matcher/expression_leaf.cpp
index a20d9d53860..ea365261c39 100644
--- a/src/mongo/db/matcher/expression_leaf.cpp
+++ b/src/mongo/db/matcher/expression_leaf.cpp
@@ -30,6 +30,7 @@
 #include "mongo/platform/basic.h"
 
 #include "mongo/db/matcher/expression_leaf.h"
+#include "mongo/db/matcher/expression_leaf_gen.h"
 
 #include <cmath>
 #include <pcrecpp.h>
@@ -146,6 +147,24 @@ bool ComparisonMatchExpression::matchesSingleElement(const BSONElement& e,
                     MONGO_UNREACHABLE;
             }
         }
+
+        if (compareReturnAllTypes.load()) {
+            switch (matchType()) {
+                case LT:
+                case LTE:
+                    return e.canonicalType() < _rhs.canonicalType();
+                case GT:
+                case GTE:
+                    return e.canonicalType() > _rhs.canonicalType();
+                case EQ:
+                    return false;
+                default:
+                    // This is a comparison match expression, so it must be either
+                    // a $lt, $lte, $gt, $gte, or equality expression.
+                    fassertFailed(16829);
+            }
+        }
+
         return false;
     }
 
diff --git a/src/mongo/db/matcher/expression_leaf.idl b/src/mongo/db/matcher/expression_leaf.idl
new file mode 100644
index 00000000000..824b3df4f08
--- /dev/null
+++ b/src/mongo/db/matcher/expression_leaf.idl
@@ -0,0 +1,10 @@
+global:
+    cpp_namespace: mongo
+
+server_parameters:
+    compareReturnAllTypes:
+        description: "Return all bson types when compare (used for testing)."
+        set_at: [ startup, runtime ]
+        cpp_vartype: AtomicWord<bool>
+        cpp_varname: compareReturnAllTypes
+        default: false
```

然后在开启这个参数的场景下，就能通过比较操作导出所有类型的数据了：
```
# 开启 compareReturnAllTypes 参数
mymongo:PRIMARY> db.adminCommand({setParameter:1, compareReturnAllTypes: true})
{
        "was" : false,
        "ok" : 1,
        "$clusterTime" : {
                "clusterTime" : Timestamp(1748047060, 1),
                "signature" : {
                        "hash" : BinData(0,"AAAAAAAAAAAAAAAAAAAAAAAAAAA="),
                        "keyId" : NumberLong(0)
                }
        },
        "operationTime" : Timestamp(1748047060, 1)
}

# 执行比较操作返回不同类型的数据
mymongo:PRIMARY> db.coll11.find({a:{$gte:"2"}})
{ "_id" : ObjectId("68305e17735d3d31a5483c49"), "a" : ISODate("2025-05-23T11:37:59.691Z") }
{ "_id" : ObjectId("68311570407d855025e010fc"), "a" : "2" }
mymongo:PRIMARY> db.coll11.find({a:{$lte:"2"}})
{ "_id" : ObjectId("68305e17735d3d31a5483c47"), "a" : "1" }
{ "_id" : ObjectId("68305e17735d3d31a5483c48"), "a" : 2 }
{ "_id" : ObjectId("68311570407d855025e010fc"), "a" : "2" }
{ "_id" : ObjectId("68311574407d855025e010fd"), "a" : "11" }
```

>说明：上述方法只是演示思路， 不代表建议在实际生产环境下这么做。

# 6. 横向对比 PostgreSQL 数据库
MongoDB 执行引擎有一个很大的缺点就是不支持并行化（这里不讨论分片场景），无论查询大小都是一个线程走到底。    
对于很多 TP 类场景，比如小范围的查询甚至是 KV 查询表现不错，但是涉及到超大规模的数据处理，MongoDB 天然存在比较大的缺陷。    

有些数据库天生支持并行化执行，比如 PostgreSQL 等。       
下面就以 PostgreSQL 为例，看看并行化执行的实现方式。通过和 MongoDB 对比寻找一些启发。      

## PostgreSQL 的并行 Table Scan

在介绍 PostgreSQL 的并行化执行之前，先总结一下常见的表组织方式：    
- 按主键： 聚簇索引，使用主键进行 B+ 树组织。比如 MySQL 和 MongoDB 的 Clustered Collection.
- 按recordId: 对于 MongoDB 这种 schema-free 数据库，没有明确的主键，因此使用内部生成的 recordId 进行 B+ 树组织.
- 堆表（Heap）：以 block 为单元，直接将数据平铺在表文件中。比如 PostgreSQL 的默认表组织方式。

堆表场景下，由于每个 block 地位都是平等的，不像 B+ 树那样有层级关系，因此并行化执行会非常简单。    
举例如下，使用 2 个 worker 进程并行遍历一个表（黄色部分是 worker1 扫描的 block, 蓝色部分是 worker2 扫描的 block）：

<p align="center">
  <img src="https://github.com/user-attachments/assets/3a6db111-597d-4887-ba75-ebf3b7dbbde9" width=500>
</p>

并行遍历的核心在于数据块的分配算法：
1. 为了充分利用磁盘 IO 的 readahead 能力，分配的单元为chunk，即多个 block(每个 block 8KB, 在内存也被称为 page)。
2. 在并行化开始时，首先尝试将一个大表划分为 [2048](https://github.com/postgres/postgres/blob/REL_15_STABLE/src/backend/access/table/tableam.c#L41) 个 chunk，并计算 chunksize. 如果这样分出来的 chunksize 大于 [8192](https://github.com/postgres/postgres/blob/REL_15_STABLE/src/backend/access/table/tableam.c#L45) 个 block，则按照 8192 个 block 作为初始的 chunksize。
3. 每个 worker 在执行时，会通过原子操作分配一个 chunk，完成这个 chunk 的扫描之后再分配下一个。
4. 不同 worker 属于不同的进程，执行有快有慢。因此，每个 worker 要扫描哪些 chunk 也不是在最开始就分配好的，而是在执行过程中动态分配, 完成一个 chunk，在去申请下一个 chunk。
5. 一个大表的 block 数量，很可能不能刚好被 chunk 大小整除。因此，在遍历到最后的一些 block 时，chunk 大小会[指数递减](https://github.com/postgres/postgres/blob/REL_15_STABLE/src/backend/access/table/tableam.c#L577)，比如从 8192 降到 4096 再到 2048，直到 1.

理想中的并行化，会随着 worker 进程数增多，性能线性提升。    
真实情况下的并行化，会存在其他一些因素的影响。比如并行化流程的启动开销，分配算法的开销，元组在 worker 进程到 gather 进程中的传递开销等。    

对于并行全表扫描，PostgreSQL 有如下参数和策略方面的优化：    
1. 在生成执行计划时，会根据下面的关键参数进行估算。    

    |参数|含义|备注|    
    |:--|:--|:--|    
    |min_parallel_table_scan_size|能执行并行扫描的最小的表大小|默认 8MB|
    |reltuples|要处理的行数||
    |relpages|要处理的 page 数||
    |cpu_tuple_cost|处理一行（从 page 中读出来）的开销|默认 0.01|
    |cpu_operator_cost|操作（运算）一行的开销|默认 0.0025|
    |seq_page_cost|顺序扫描一个 page 的开销|默认 1.0|
    |**parallel_setup_cost**|并行扫描的启动开销|默认 1000.0, 这也意味着，较小的表启动并行不划算|
    |**parallel_tuple_cost**|并行扫描时，将一行数据从 worker 进程传递到 gather 进程的开销|默认 0.1，这意味着，如果输出的结果集很大，也不适合并行查询。<br>比如对一个大表 bigtable， select * from bigtable where c1=50000 会触发并行查询，但是 select * from bigtable where c1>50000 和 select * from bigtable 不会|

2. 对于全表扫描和并行全表扫描，有 [syncscan](https://github.com/postgres/postgres/blob/REL_15_STABLE/src/backend/access/common/syncscan.c) 机制来同步多个扫描任务。比如有一个 sql1 正在进行全表扫描，而且扫描到了表中间的位置；此时 sql2 也触发了全表扫描，则 sql2 会获取 sql1 扫描的位置开始向后扫描，扫描到末尾之后，再扫描表头开始的部分。这种机制可以充分利用硬盘的 IO 能力。     

## PostgreSQL 的并行 Index Scan

对于大多数数据库使用场景来说，走索引才是常态。因此，并行化索引扫描也显得尤为重要。    

PostgreSQL 中 btree 索引支持了并行扫描， 通过[共享内存](https://github.com/postgres/postgres/blob/REL_15_STABLE/src/backend/access/nbtree/nbtree.c#L66)来同步多个 worker 进程扫描的位置。举例来说：    
1. worker1 扫描到一个索引的 page1 时，会将[下一个 page 的位置写入共享内存](https://github.com/postgres/postgres/blob/REL_15_STABLE/src/backend/access/nbtree/nbtsearch.c#L2036)（如果是顺序扫描，则保存 page 的 next 指针， 如果是逆序扫描则是 prev 指针）， 假设是 page2。
2. worker2 在工作时，会从[共享内存获取](https://github.com/postgres/postgres/blob/REL_15_STABLE/src/backend/access/nbtree/nbtree.c#L636) page2，同时也将自己的下一个 page 位置（假设是 page3）写入共享内存。然后自己执行 page2 的扫描。
3. worker1 在处理完 page1 之后，再去共享内存中获取当前要处理的 page3，同时将写一个 page4 写入共享内存，然后执行 page3 的扫描。
4. 如此往复，直到扫描完成。

从原理上来看，有点像我们在 5.4 章节介绍的 MongoDB 的 evict 扫描流程。不过这里没有专门的 evict_server 去扫描，而是多个 worker 进程自己通过共享内存进行同步。    

# 7. 总结
1. MongoDB 可以通过 skip+limit 或者 splitVector 等多种方式对大表进行数据切片，然后进行并行化导出。    
2. MongoDB 的比较操作符只支持返回同类型的数据，因此传统的分片后走 indexScan 导出的方式可能存在“类型陷阱”。    
3. 在 MongoDB 4.4 版本支持了使用 recordId 进行全表扫描和续传，在 MongoDB 5.3 版本支持了 _id 的 Clustered Collection（聚簇索引）。这些高级特性都可以给我们提供有价值的参考。    
4. MongoDB 当前不支持并行化的执行计划，因此对于大规模数据处理，性能瓶颈会比较明显。 PostgreSQL 等数据库具备并行化执行能力，在处理大数据是具备天生优势。通过对 PostgreSQL 并行化机制的介绍，能够给我们提供一些启发，也许在未来，MongoDB 也能支持并行化执行 :)    


# 参考文档
1. https://github.com/pengzhenyi2015/mongocheck#
2. https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/s/split_vector.cpp
3. https://www.mongodb.com/docs/v6.0/core/clustered-collections/
4. https://github.com/postgres/postgres/tree/REL_15_STABLE
5. https://www.mongodb.com/docs/manual/reference/bson-type-comparison-order/
6. https://www.mongodb.com/zh-cn/docs/mongosync/current/topologies/multiple-mongosyncs/
7. https://github.com/alibaba/MongoShake/tree/release-v2.8.6-20250825
8. https://github.com/alibaba/MongoShake/issues/927
