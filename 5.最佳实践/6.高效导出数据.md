# 1. 导语
在真实的业务场景中，时常会遇到如何将一个大表的数据快速导出的问题。    
对于这种场景，普通的全表扫描由于存在并发受限，往往无法满足业务需求。     
因此，本文尝试探讨**如何并行化导出数据集**，以及如何避免一些常见的陷阱。

# 2. 切分数据集
要实现并行化导出，首先要完成数据切分。    

以按照 _id 字段切分为例，常见的切分方法：   
1. 业务层自己切分。    
    a. 如果字段的取值非常有规律，比如是从 1 递增的整数，则自己做一个简单的除法即可解决。    
    b. 如果分布不规律，则可以通过索引排序，然后用 skip+limit 查询的方式，遍历索引得到想要的切分点。具体例子可以参考 [mongocheck](https://github.com/pengzhenyi2015/mongocheck#) 中 checkCollection 的采样逻辑。
2. 使用 [splitVector](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/s/split_vector.cpp) 命令，在 MongoDB 服务端进行切分。比如希望 1MB 作为一个切分块，则可以执行下面的命令：
    ```
    db.runCommand({splitVector:"db1.coll2", keyPattern:{_id:1}, maxChunkSize:1})
    ```
    splitVector 也是 MongoDB 分片表进行 chunk 切分的底层命令。    
    具体执行的逻辑是，先根据表大小和总条数计算每条文档的平均大小，然后计算出每个 chunk 应该包含的文档数。然后执行索引遍历，找到每个切分点。

# 3. 并行化导出数据集
有了切分点之后，就可以并发地根据切分点执行范围查询，导出数据了。
由于 MongoDB 的索引都是二级索引，所以在底层的执行计划都是 INDEX_SCAN，即扫描索引然后再回表。即使 _id 索引也是二级索引，首先要通过 _id 查到内部的 recordId, 然后再根据 recordId 去回表（表结构的 B+ 树由 recordId 组织，而不是 _id）。

从 MongoDB 5.3 版本开始，用户可以根据 _id 创建 Clustered Collection，即聚集索引。表结构的 B+ 树直接由 _id 组织， 因此使用 _id 进行范围查询要快很多（首先**不需要先查索引再回表**，其次 _id 相邻的文档在表文件中物理上连续使得 **IO 效率更高**）。

# 4. 范围查询的"比较" 陷阱
根据切分点进行范围查询，一定能读到全部数据吗？     

我们需要注意到 MongoDB 的一个设计细节：**使用比较操作的时候，只会返回和比较 key 类型相同的文档。**

比如 coll2 表中有 5 条数据，a 字段的类型有数值、字符串、ObjectId、数组 多种类型，如下所示：
```
mymongo:PRIMARY> db.coll2.find()
{ "_id" : ObjectId("681488df5938079fa6272b80"), "a" : 1 }
{ "_id" : ObjectId("681488df5938079fa6272b81"), "a" : "2" }
{ "_id" : ObjectId("681488df5938079fa6272b82"), "a" : [ "1", "2", "3" ] }
{ "_id" : ObjectId("681488df5938079fa6272b83"), "a" : 4 }
{ "_id" : ObjectId("681488df5938079fa6272b84"), "a" : ObjectId("681488df5938079fa6272b7f") }
```

假设现在的分割点是 a=4, 将表分为两部分。那么通过范围查询只能读取到数值类型的文档，而不是所有文档：
```
// a >= 4 的所有文档
mymongo:PRIMARY> db.coll2.find({a:{$gte:4}})
{ "_id" : ObjectId("681488df5938079fa6272b83"), "a" : 4 }

// a < 4 的所有文档
mymongo:PRIMARY> db.coll2.find({a:{$lt:4}})
{ "_id" : ObjectId("681488df5938079fa6272b80"), "a" : 1 }
```

由于 MongoDB 是一个典型的 schema-free 的数据库，同一字段有多种类型的数据是可能的。     
因此，如果要并行化导出数据集，就必须考虑到这一点。

# 5. 探讨一些可行的解决方案
## 5.1 做好类型检查
对于我遇到过的很多业务场景来说， 1 个字段从始至终只有一种类型也是很常见的。比如很多业务都不会自己设置 _id，则默认都是 ObjectId 类型。    

因此，我们可以在评估并行导出的方案之前，先对字段的类型进行统计。    

比如使用聚合命令对 _id 字段的类型进行分类统计：    
```
mymongo:PRIMARY> db.coll1.aggregate([{$group:{_id:{$type : "$_id"}, count: {$sum:1}}}])
{ "_id" : "double", "count" : 3 }
{ "_id" : "objectId", "count" : 2 }
```

如果只有一种类型，则可以放心使用范围查询。    

## 5.2 退化成使用全表扫描

如果 _id 的类型多种多样，则比较稳妥的方式是退化使用全表扫描。即 find + 后续 getmore 的方式。    
不过这里又有一个问题：如果全表扫描的过程中失败退出了，如何进行断点续传？

---

对于 **MongoDB 4.4 及以上版本**，在进行顺序（自然序）扫描时，可以指定服务端返回 requestResumeToken，即返回扫描到的最后一条数据的 recordId.        
如果需要断点续传，则指定 resumeAfter, 从上次扫描到的位置继续读取。    

举例如下，先使用 _requestResumeToken 获取末尾记录的 recordId
```
db.runCommand({find:"coll1", "$_requestResumeToken": true, hint: {$natural: 1}})
```

使用 _resumeAfter ，并指定 recordId, 可以从上一次断开的地方进行重传：
```
db.runCommand({find:"coll1", "$_requestResumeToken": true, "$_resumeAfter":{"$recordId":NumberLong(101)}, hint:{$natural:1}})
```
由于 recordId 是内部递增的 int64 类型的 ID，因此不会出现前面说的类型陷阱。

> MongoDB 在更高版本（5.3）支持了 Clustered Collection（聚集索引）之后， recordId 不再是单纯的 int64. 上述 requestResumeToken 返回的 recordId 类型也从 NumberLong 变成了 Binary, 除此之外命令的使用方式还是相同。

---

对于 **MongoDB 4.4 以下版本**，断点续传似乎不好处理。    
一种变通的方法，是使用 _id 字段进行升序扫描（降序也行，原理类似）。如果需要断点续传，则根据上次扫描到的 _id , 进行 `$gt` 范围查询。    
不过这种方式本质上和全表扫描还是不一样的。首先 INDEX_SCAN 的性能不如真正的 COLL_SCAN, 而且采用 _id 的比较操作进行断点续传，也会遇到前面说的类型陷阱。

## 5.3 使用 Clustered Collection

MongoDB 高版本支持使用 Clustered Index(聚簇索引) 建表。
举例如下：
```
test> db.createCollection("coll1", {clusteredIndex: {key : {"_id":1}, unique: true, name: "clustered index"}})
{ ok: 1 }
```

使用上述方式建表之后，对于 _id 的范围查询，会从 INDEX_SCAN 变成 CLUSTERED_IXSCAN，性能会高很多。
但是，使用 _id 进行比较操作，还是会存在前面说的类型陷阱。

## 5.4 分类型并行导出
综合前面的讨论，并行化导出最大的问题是比较超过遇到的多类型问题。        
那么是否能执行这样一种策略：首先统计有多少种类型，然后对每个类型进行并行化导出？    
从理论上来讲，应该是可行的。

# 6. 横向对比其他数据库
MongoDB 执行引擎有一个很大的缺点就是不支持并行化（这里不讨论分片场景），无论查询大小都是一个线程走到底。    
对于很多 TP 类场景，比如小范围的查询甚至是 KV 查询表现不错，但是涉及到超大规模的数据处理，MongoDB 天然存在比较大的缺陷。    

有些数据库天生支持并行化执行，比如 PostgreSQL 等。       
下面就以 PostgreSQL 为例，看看并行化执行的实现方式。通过和 MongoDB 对比寻找一些启发。      

## 并行 Table Scan

在介绍 PostgreSQL 的并行化执行之前，先总结一下常见的表组织方式：    
- 按主键： 聚簇索引，使用主键进行 B+ 树组织。比如 MySQL 和 MongoDB 的 Clustered Collection.
- 按recordId: 对于 MongoDB 这种 schema-free 数据库，没有明确的主键，因此使用内部生成的 recordId 进行 B+ 树组织.
- 堆表（Heap）：以 block 为单元，直接将数据平铺在表文件中。比如 PostgreSQL 的默认表组织方式。

堆表场景下，由于每个 block 地位都是平等的，不像 B+ 树那样有层级关系，因此并行化执行会非常简单。    
举例如下，使用 2 个 worker 进程并行遍历一个表：

<TODO 加一张示意图>

并行遍历的核心在于数据块的分配算法：
1. 为了充分利用磁盘 IO 的 readahead 能力，分配的单元为chunk，即多个 block(每个 block 8KB, 在内存也被称为 page)。
2. 在并行化开始时，首先尝试将一个大表划分为 [2048](https://github.com/postgres/postgres/blob/REL_15_STABLE/src/backend/access/table/tableam.c#L41) 个 chunk，并计算 chunksize. 如果这样分出来的 chunksize 大于 [8192](https://github.com/postgres/postgres/blob/REL_15_STABLE/src/backend/access/table/tableam.c#L45) 个 block，则按照 8192 个 block 作为初始的 chunksize。
3. 每个 worker 在执行时，会通过原子操作分配一个 chunk，完成这个 chunk 的扫描之后再分配下一个。
4. 不同 worker 属于不同的进程，执行有快有慢。因此，每个 worker 要扫描哪些 chunk 也不是在最开始就分配好的，而是在执行过程中动态分配, 完成一个 chunk，在去申请下一个 chunk。
5. 一个大表的 block 数量，很可能不能刚好被 chunk 大小整除。因此，在遍历到最后的一些 block 时，chunk 大小会[指数递减](https://github.com/postgres/postgres/blob/REL_15_STABLE/src/backend/access/table/tableam.c#L577)，比如从 8192 降到 4096 再到 2048，直到 1.

理想中的并行化，会随着 worker 进程数增多，性能线性提升。    
真实情况下的并行化，会存在其他一些因素的影响。比如并行化流程的启动开销，分配算法的开销，元组在 worker 进程到 gather 进程中的传递开销等。    

对于并行全表扫描，PostgreSQL 有如下参数和策略方面的优化：    
1. 在生成执行计划时，会根据下面的关键参数进行估算。    

    |参数|含义|备注|    
    |:--|:--|:--|    
    |min_parallel_table_scan_size|能执行并行扫描的最小的表大小|默认 8MB|
    |reltuples|要处理的行数||
    |relpages|要处理的 page 数||
    |cpu_tuple_cost|处理一行（从 page 中读出来）的开销|默认 0.01|
    |cpu_operator_cost|操作（运算）一行的开销|默认 0.0025|
    |seq_page_cost|顺序扫描一个 page 的开销|默认 1.0|
    |**parallel_setup_cost**|并行扫描的启动开销|默认 1000.0, 这也意味着，较小的表启动并行不划算|
    |**parallel_tuple_cost**|并行扫描时，将一行数据从 worker 进程传递到 gather 进程的开销|默认 0.1，这意味着，如果输出的结果集很大，也不适合并行查询。<br>比如对一个大表 bigtable， select * from bigtable where c1=50000 会触发并行查询，但是 select * from bigtable where c1>50000 和 select * from bigtable 不会|

2. 对于全表扫描和并行全表扫描，有 syncscan 机制来同步多个扫描任务。比如有一个 sql1 正在进行全表扫描，而且扫描到了表中间的位置；此时 sql2 也触发了全表扫描，则 sql2 会获取 sql1 扫描的位置开始向后扫描，扫描到末尾之后，再扫描表头开始的部分。这种机制可以充分利用硬盘的 IO 能力。   

## 并行 Index Scan



# 7. 总结


# 参考文档
1. https://github.com/pengzhenyi2015/mongocheck#
2. https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/db/s/split_vector.cpp
3. https://www.mongodb.com/docs/v6.0/core/clustered-collections/
4. https://github.com/postgres/postgres/tree/REL_15_STABLE
